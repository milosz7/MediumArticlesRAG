{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-26T20:09:07.561213Z",
     "start_time": "2024-04-26T20:09:07.559411Z"
    }
   },
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 199
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:09:07.610358Z",
     "start_time": "2024-04-26T20:09:07.608302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# retrieval - find relevant info - ask question abous an article -> retrieve passage of texts related to the query\n",
    "\n",
    "# augmentation - take relevant info and augment input to LLM with relevant info\n",
    "\n",
    "# generation - Take the first two steps and pass to LLM for GEN outs\n",
    "\n",
    "## WHY RAG?\n",
    "\n",
    "# the goal is to improve output of an LLM\n",
    "\n",
    "# prevent hallucinations - the text is more factual not only good looking RAG can help LLMs generate information based on relevant passages that are factual\n",
    "# work with custom data - not trained only with internet scale data like LLMs. However it also does a lot of the responses in a general nature. RAG helps to create specific responses based on specific documents\n",
    "\n",
    "# use cases?\n",
    "\n",
    "# customer support Q&A chat. Existing customer support docs as a resource, retrieval system based on that data, LLM crafts the answer based on those, basically a \"chatbot for your docs\".\n",
    "\n",
    "# email chain analysis - chains of emails of claims. you can use rag pipeline to find relevant information from the chains and use LLM to process that info\n",
    "\n",
    "# company internal documentation chat \n",
    "\n",
    "# textbook Q&A - build a rag pipeline to go through the textbook to find relevant passages to answer the questions\n",
    "\n",
    "# Common theme - take documents to a query and process it with LLM, LLM -> kind of like a calculator for language"
   ],
   "id": "ed3902d09da4ed54",
   "outputs": [],
   "execution_count": 200
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:09:07.612878Z",
     "start_time": "2024-04-26T20:09:07.611512Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "53279448507845ef",
   "outputs": [],
   "execution_count": 200
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Text preprocessing",
   "id": "3a3f0d23d628665c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:09:07.692401Z",
     "start_time": "2024-04-26T20:09:07.623354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(\"../data/medium.csv\")\n",
    "data.head()"
   ],
   "id": "f865993fea6e49f6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                     Title  \\\n",
       "0          A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model   \n",
       "1          Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric   \n",
       "2                                             How to Use ggplot2 in Python   \n",
       "3  Databricks: How to Save Data Frames as CSV Files on Your Local Computer   \n",
       "4    A Step-by-Step Implementation of Gradient Descent and Backpropagation   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Text  \n",
       "0  1. Introduction of Word2vec\\n\\nWord2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another.\\n\\nThere are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit. For more details about the word2vec algorithm, please check here.\\n\\n2. Gensim Python Library Introduction\\n\\nGensim is an open source python library for natural language processing and it was developed and is maintained by the Czech natural language processing researcher Radim Řehůřek. Gensim library will enable us to develop word embeddings by training our own word2vec models on a custom corpus either with CBOW of skip-grams algorithms.\\n\\nAt first, we need to install the genism package. Gensim runs on Linux, Windows and Mac OS X, and should run on any other platform that supports Python 2.7+ and NumPy. Gensim depends on the following software:\\n\\nPython >= 2.7 (tested with versions 2.7, 3.5 and 3.6)\\n\\n>= 2.7 (tested with versions 2.7, 3.5 and 3.6) NumPy >= 1.11.3\\n\\n>= 1.11.3 SciPy >= 0.18.1\\n\\n>= 0.18.1 Six >= 1.5.0\\n\\n>= 1.5.0 smart_open >= 1.2.1\\n\\nThere are two ways for installation. We could run the following code in our terminal to install genism package.\\n\\npip install --upgrade gensim\\n\\nOr, alternatively for Conda environments:\\n\\nconda install -c conda-forge gensim\\n\\n3. Implementation of word Embedding with Gensim Word2Vec Model\\n\\nIn this tutorial, I will show how to generate word embedding with genism using a concrete example. The dataset I used for this tutorial is from Kaggle Dataset.\\n\\nThis vehicle dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The full python tutorial can be found here.\\n\\n>>> df = pd.read_csv('data.csv')\\n\\n>>> df.head()\\n\\n3.1 Data Preprocessing:\\n\\nSince the purpose of this tutorial is to learn how to generate word embeddings using genism library, we will not do the EDA and feature selection for the word2vec model for the sake of simplicity.\\n\\nGenism word2vec requires that a format of ‘list of lists’ for training where every document is contained in a list and every list contains lists of tokens of that document. At first, we need to generate a format of ‘list of lists’ for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains lists of features of that make model.\\n\\nTo achieve this, we need to do the following things :\\n\\na. Create a new column for Make Model\\n\\n>>> df['Maker_Model']= df['Make']+ \" \" + df['Model']\\n\\nb. Generate a format of ‘ list of lists’ for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size, Vehicle Style.\\n\\n# Select features from original dataset to form a new dataframe\\n\\n>>> df1 = df[['Engine Fuel Type','Transmission Type','Driven_Wheels','Market Category','Vehicle Size', 'Vehicle Style', 'Maker_Model']] # For each row, combine all the columns into one column\\n\\n>>> df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1) # Store them in a pandas dataframe\\n\\n>>> df_clean = pd.DataFrame({'clean': df2}) # Create the list of list format of the custom corpus for gensim modeling\\n\\n>>> sent = [row.split(',') for row in df_clean['clean']] # show the example of list of list format of the custom corpus for gensim modeling\\n\\n>>> sent[:2]\\n\\n[['premium unleaded (required)',\\n\\n'MANUAL',\\n\\n'rear wheel drive',\\n\\n'Factory Tuner',\\n\\n'Luxury',\\n\\n'High-Performance',\\n\\n'Compact',\\n\\n'Coupe',\\n\\n'BMW 1 Series M'],\\n\\n['premium unleaded (required)',\\n\\n'MANUAL',\\n\\n'rear wheel drive',\\n\\n'Luxury',\\n\\n'Performance',\\n\\n'Compact',\\n\\n'Convertible',\\n\\n'BMW 1 Series']]\\n\\n3.2. Genism word2vec Model Training\\n\\nWe can train the genism word2vec model with our own custom corpus as following:\\n\\n>>> model = Word2Vec(sent, min_count=1,size= 50,workers=3, window =3, sg = 1)\\n\\nLet’s try to understand the hyperparameters of this model.\\n\\nsize: The number of dimensions of the embeddings and the default is 100.\\n\\nwindow: The maximum distance between a target word and words around the target word. The default window is 5.\\n\\nmin_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.\\n\\nworkers: The number of partitions during training and the default workers is 3.\\n\\nsg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.\\n\\nAfter training the word2vec model, we can obtain the word embedding directly from the training model as following.\\n\\n>>> model['Toyota Camry'] array([-0.11884457, 0.03035539, -0.0248678 , -0.06297892, -0.01703234,\\n\\n-0.03832747, -0.0825972 , -0.00268112, -0.09192555, -0.08458661,\\n\\n-0.07199778, 0.05235871, 0.21303181, 0.15767808, -0.1883737 ,\\n\\n0.01938575, -0.24431638, 0.04261152, 0.11865819, 0.09881561,\\n\\n-0.04580643, -0.08342388, -0.01355413, -0.07892415, -0.08467747,\\n\\n-0.0040625 , 0.16796461, 0.14578669, 0.04187112, -0.01436194,\\n\\n-0.25554284, 0.25494182, 0.05522631, 0.19295982, 0.14461821,\\n\\n0.14022525, -0.2065216 , -0.05020927, -0.08133671, 0.18031682,\\n\\n0.35042757, 0.0245426 , 0.15938364, -0.05617865, 0.00297452,\\n\\n0.15442047, -0.01286271, 0.13923576, 0.085941 , 0.18811756],\\n\\ndtype=float32)\\n\\n4. Compute Similarities\\n\\nNow we could even use Word2vec to compute the similarity between two Make Models in the vocabulary by invoking the model.similarity( ) and passing in the relevant words. For instance, model.similarity(‘Porsche 718 Cayman’, ‘Nissan Van’) This will give us the Euclidian similarity between Porsche 718 Cayman and Nissan Van.\\n\\n>>> model.similarity('Porsche 718 Cayman', 'Nissan Van')\\n\\n0.822824584626184 >>> model.similarity('Porsche 718 Cayman', 'Mercedes-Benz SLK-Class')\\n\\n0.961089779453727\\n\\nFrom the above examples, we can tell that Porsche 718 Cayman is more similar to Mercedes-Benz SLK-Class than Nissan Van. We also can use the built-in function model.most_similar() to get a set of the most similar make models for a given make model based on the Euclidean distance.\\n\\n>>> model1.most_similar('Mercedes-Benz SLK-Class')[:5] [('BMW M4', 0.9959905743598938),\\n\\n('Maserati Coupe', 0.9949707984924316),\\n\\n('Porsche Cayman', 0.9945154190063477),\\n\\n('Mercedes-Benz SLS AMG GT', 0.9944609999656677),\\n\\n('Maserati Spyder', 0.9942780137062073)]\\n\\nHowever, Euclidian similarity cannot work well for the high-dimensional word vectors. This is because Euclidian similarity will increase as the number of dimensions increases, even if the word embedding stands for different meanings. Alternatively, we can use cosine similarity to measure the similarity between two vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at a 0-degree angle.\\n\\nThe following function shows how can we generate the most similar make model based on cosine similarity.\\n\\ndef cosine_distance (model, word,target_list , num) :\\n\\ncosine_dict ={}\\n\\nword_list = []\\n\\na = model[word]\\n\\nfor item in target_list :\\n\\nif item != word :\\n\\nb = model [item]\\n\\ncos_sim = dot(a, b)/(norm(a)*norm(b))\\n\\ncosine_dict[item] = cos_sim\\n\\ndist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order\\n\\nfor item in dist_sort:\\n\\nword_list.append((item[0], item[1]))\\n\\nreturn word_list[0:num] # only get the unique Maker_Model\\n\\n>>> Maker_Model = list(df.Maker_Model.unique()) # Show the most similar Mercedes-Benz SLK-Class by cosine distance\\n\\n>>> cosine_distance (model,'Mercedes-Benz SLK-Class',Maker_Model,5) [('Mercedes-Benz CLK-Class', 0.99737006),\\n\\n('Aston Martin DB9', 0.99593246),\\n\\n('Maserati Spyder', 0.99571854),\\n\\n('Ferrari 458 Italia', 0.9952333),\\n\\n('Maserati GranTurismo Convertible', 0.994994)]\\n\\n5. T-SNE Visualizations\\n\\nIt’s hard to visualize the word embedding directly, for they usually have more than 3 dimensions. T-SNE is a useful tool to visualize high-dimensional data by dimension reduction while keeping relative pairwise distance between points. It can be said that T-SNE looking for a new data representation where the neighborhood relations are preserved. The following code shows how to plot the word embedding with T-SNE plot.\\n\\ndef display_closestwords_tsnescatterplot(model, word, size):\\n\\n\\n\\narr = np.empty((0,size), dtype='f')\\n\\nword_labels = [word] close_words = model.similar_by_word(word) arr = np.append(arr, np.array([model[word]]), axis=0)\\n\\nfor wrd_score in close_words:\\n\\nwrd_vector = model[wrd_score[0]]\\n\\nword_labels.append(wrd_score[0])\\n\\narr = np.append(arr, np.array([wrd_vector]), axis=0)\\n\\n\\n\\ntsne = TSNE(n_components=2, random_state=0)\\n\\nnp.set_printoptions(suppress=True)\\n\\nY = tsne.fit_transform(arr) x_coords = Y[:, 0]\\n\\ny_coords = Y[:, 1]\\n\\nplt.scatter(x_coords, y_coords) for label, x, y in zip(word_labels, x_coords, y_coords):\\n\\nplt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\\n\\nplt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\\n\\nplt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\\n\\nplt.show() >>> display_closestwords_tsnescatterplot(model, 'Porsche 718 Cayman', 50)\\n\\nThis T-SNE plot shows the top 10 similar vehicles to the Porsche 718 Cayman in two-dimensional space.\\n\\nAbout Me\\n\\nI am a master student in Data Science at the University of San Francisco. I am passionate about using Machine Learning to solve business challenges. You can also find me through Linkedin.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          In my last article, I introduced the concept of Graph Neural Network (GNN) and some recent advancements of it. Since this topic is getting seriously hyped up, I decided to make this tutorial on how to easily implement your Graph Neural Network in your project. You will learn how to construct your own GNN with PyTorch Geometric, and how to use GNN to solve a real-world problem (Recsys Challenge 2015).\\n\\nIn this blog post, we will be using PyTorch and PyTorch Geometric (PyG), a Graph Neural Network framework built on top of PyTorch that runs blazingly fast. It is several times faster than the most well-known GNN framework, DGL.\\n\\nAside from its remarkable speed, PyG comes with a collection of well-implemented GNN models illustrated in various papers. Therefore, it would be very handy to reproduce the experiments with PyG.  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Introduction\\n\\nThanks to its strict implementation of the grammar of graphics, ggplot2 provides an extremely intuitive and consistent way of plotting your data. Not only does ggplot2’s approach to plotting ensure that each plot comprises certain basic elements but it also simplifies the readability of your code to a great extent.\\n\\nHowever, if you are a frequent user of Python, then implementing the grammar of graphics can be extremely challenging due to the lack of standardized syntax in popular plotting libraries, such as matplotlib or seaborn. Should you still want to make use of the grammar of graphics, then the Python package plotnine provides an alternative for you.\\n\\nThe Grammar of Graphics\\n\\nIn case you should be unfamiliar with the grammar of graphics, here is a quick overview:\\n\\nMain Components of the Grammar of Graphics\\n\\nAs you can see, there are several components that make up the grammar of graphics, starting with your data. After identifying the data you would like to visualize, you have to specify the variables you are interested in. For instance, you might want to display one variable on the x-axis and another on the y-axis. Third, you have to define what type of geometric object (geom for short) you would like to utilize. This could be anything from a bar plot to a scatter plot or any of the other existing plot types.\\n\\nThese first three components are compulsory. Without data, there is nothing to plot. Without axis definitions, there is nothing to plot either. And finally, without defining a geometric object, you will only see an empty coordinate system. The remaining components making up the grammar of graphics are optional and can be implemented to improve visualizations. Facets refer to specifications of subplots, that is, plotting several variables within your data next to one another in separate plots. Statistical transformations mainly refer to the inclusion of summary statistics in your plot, such as the median or percentiles. Coordinates describe the different coordinate systems available to you. The most used and default coordinate system is the Cartesian coordinate system. Depending on the structure of the data you would like to plot, lesser used coordinate systems, such as the Polar coordinate system, might provide a better way of visualizing your data. Finally, themes provide a variety of options to design all non-data elements of your plot, such as the legend, background, or annotations.\\n\\nWhile there are many ways of visualizing the grammar of graphics, I particularly like the one I created above because it implies the additivity of these layers as well as the fact that they are building upon one another. If you have ever used ggplot2, you are familiar with the ‘+’ in its syntax that symbolizes the same idea described above.\\n\\nplotnine\\n\\nplotnine is a Python package allowing you to use ggplot2-like code that is implementing the grammar of graphics. By doing so, just as in ggplot2, you are able to specifically map data to visual objects that make up the visualization. This enables you to improve both the readability as well as the structure of your code. While you could set matplotlib’s style to ggplot, you cannot implement the grammar of graphics in matplotlib the same way you can in ggplot2.\\n\\nInstallation\\n\\nBefore getting started, you have to install plotnine. As always, there are two main options for doing so: pip and conda.\\n\\nPlotting\\n\\nHaving installed plotnine, you can get started plotting using the grammar of graphics. Let us begin by building a very simple plot only using the three requisite components: data, aesthetics, and geometric objects.\\n\\nBuilding a plot using the grammar of graphics\\n\\nAs you can see, the syntax is very similar to ggplot2. First, we specify the data source. In our case, the data we are using is the classic mpg data set. Next, we define that the variable ‘class’ is going to be displayed on the x-axis. Lastly, we say that we would like to use a bar plot with bars of size 20 to visualize our data. Let us look at the complete code and the resulting plot:\\n\\nThe code above will yield the following output:\\n\\nWhile this is a good start, it is not very nice to look at yet. Let us use other components of the grammar of graphics to beautify our plot.\\n\\nFor instance, we could flip the axes using coord_flip() and customize the plot and axes titles with labs() to improve our plot. Using the code chunk above, our plot would look like this:\\n\\nPlotting Multidimensional Data\\n\\nBesides basic plots, you can do almost everything you could otherwise do in ggplot2, such as plotting multidimensional data. If you would like to visualize the relationships between three variables you could add aesthetics to an otherwise two-dimensional plot:\\n\\nAdding color to the aesthetics will prompt plotnine to display a two-dimensional plot using displ (engine displacement, in liters) on its x- and hwy (highway miles per gallon) on its y-axis and color the data according to the variable class. We have also switched the geometric object to geom_point(), which will give us a scatter instead of a bar plot. Let us take a look at what that would look like:\\n\\nConclusion\\n\\nAs you can see, plotnine provides you with the ability to utilize the grammar of graphics within Python. This increases the readability of your code and allows you to specifically map parts of your data to visual objects. If you are already familiar with ggplot2, then you won’t have to learn anything new to master plotnine. If not, here is a link to the ggplot2 website on which you can find out plenty more about the grammar of graphics and all types of geometric objects available to you.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Photo credit to Mika Baumeister from Unsplash\\n\\nWhen I work on Python projects dealing with large datasets, I usually use Spyder. The environment of Spyder is very simple; I can browse through working directories, maintain large code bases and review data frames I create. However, if I don’t subset the large data, I constantly face memory issues and struggle with very long computational time. For this reason, I occasionally use Databricks. Databricks is a Microsoft Azure platform where you can easily parse large amounts of data into “notebooks” and perform Apache Spark-based analytics.\\n\\nIf you want to work with data frames and run models using pyspark, you can easily refer to Databricks’ website for more information. However, while working on Databricks, I noticed that saving files in CSV, which is supposed to be quite easy, is not very straightforward. In the following section, I would like to share how you can save data frames from Databricks into CSV format on your local computer with no hassles.\\n\\n1. Explore the Databricks File System (DBFS)\\n\\nFrom Azure Databricks home, you can go to “Upload Data” (under Common Tasks)→ “DBFS” → “FileStore”.\\n\\nDBFS FileStore is where you create folders and save your data frames into CSV format. By default, FileStore has three folders: import-stage, plots, and tables.\\n\\n2. Save a data frame into CSV in FileStore\\n\\nSample.coalesce(1).write.format(“com.databricks.spark.csv”).option(“header”, “true”).save(“dbfs:/FileStore/df/Sample.csv”)\\n\\nUsing the above code on the notebook, I created a folder “df” and saved a data frame “Sample” into CSV. It is important to use coalesce(1) since it saves the data frame as a whole. At the end of this article, I will also demonstrate what happens when you don’t include coalesce(1) in the code.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          A Step-by-Step Implementation of Gradient Descent and Backpropagation\\n\\nThe original intention behind this post was merely me brushing upon mathematics in neural network, as I like to be well versed in the inner workings of algorithms and get to the essence of things. I then think I might as well put together a story rather than just revisiting the formulas on my notepad over and over. Though you might find a number of tutorials for building a simple neural network from scratch. Different people have varied angles of seeing things as well as the emphasis of study. Another way of thinking might in some sense enhance understanding. So let’s dive in.\\n\\nPhoto from Unsplash\\n\\nNeural network in a nutshell\\n\\nThe core of neural network is a big function that maps some input to the desired target value, in the intermediate step does the operation to produce the network, which is by multiplying weights and add bias in a pipeline scenario that does this over and over again. The process of training a neural network is to determine a set of parameters that minimize the difference between expected value and model output. This is done using gradient descent (aka backpropagation), which by definition comprises two steps: calculating gradients of the loss/error function, then updating existing parameters in response to the gradients, which is how the descent is done. This cycle is repeated until reaching the minima of the loss function. This learning process can be described by the simple equation: W(t+1) = W(t) — dJ(W)/dW(t).\\n\\nThe mathematical intuition\\n\\nPhoto from https://blog.goodaudience.com/artificial-neural-networks-explained-436fcf36e75\\n\\nFor my own practice purpose, I like to use a small network with a single hidden layer as in the diagram. In this layout, X represents input, subscripts i, j, k denote the number of units in the input, hidden and output layers respectively; w_ij represents the weights connecting input to hidden layer, and w_jk is the weights connecting hidden to output layer.\\n\\nThe model output calculation, in this case, would be:\\n\\nOften the choice of the loss function is the sum of squared error. Here I use sigmoid activation function and assume bias b is 0 for simplicity, meaning weights are the only variables that affect model output. Let’s derive the formula for calculating gradients of hidden to output weights w_jk.\\n\\nThe complexity of determining input to hidden weights is that it affects output error indirectly. Each hidden unit output affects model output, thus input to hidden weights w_ij depend on the errors at all of the units it is connected to. The derivation starts the same, just to expand the chain rule at z_k to the subfunction.\\n\\nMore thoughts:\\n\\nNotice that the gradients of the two weights have a similar form. The error is backpropagated via the derivative of activation function, then weighted by the input (the activation output) from the previous layer. In the second formula, the backpropagated error from the output layer is further projected onto w_jk, then repeat the same way of backpropagation and weighted by the input. This backpropagating process is iterated all the way back to the very first layer in an arbitrary-layer neural network. “The gradients with respect to each parameter are thus considered to be the contribution of the parameter to the error and should be negated during learning.”\\n\\nPutting the above process into code:\\n\\nBelow is the complete example:\\n\\nimport numpy as np class NeuralNetwork:\\n\\ndef __init__(self):\\n\\nnp.random.seed(10) # for generating the same results\\n\\nself.wij = np.random.rand(3,4) # input to hidden layer weights\\n\\nself.wjk = np.random.rand(4,1) # hidden layer to output weights\\n\\n\\n\\ndef sigmoid(self, x, w):\\n\\nz = np.dot(x, w)\\n\\nreturn 1/(1 + np.exp(-z))\\n\\n\\n\\ndef sigmoid_derivative(self, x, w):\\n\\nreturn self.sigmoid(x, w) * (1 - self.sigmoid(x, w))\\n\\n\\n\\ndef gradient_descent(self, x, y, iterations):\\n\\nfor i in range(iterations):\\n\\nXi = x\\n\\nXj = self.sigmoid(Xi, self.wij)\\n\\nyhat = self.sigmoid(Xj, self.wjk)\\n\\n# gradients for hidden to output weights\\n\\ng_wjk = np.dot(Xj.T, (y - yhat) * self.sigmoid_derivative(Xj, self.wjk))\\n\\n# gradients for input to hidden weights\\n\\ng_wij = np.dot(Xi.T, np.dot((y - yhat) * self.sigmoid_derivative(Xj, self.wjk), self.wjk.T) * self.sigmoid_derivative(Xi, self.wij))\\n\\n# update weights\\n\\nself.wij += g_wij\\n\\nself.wjk += g_wjk\\n\\nprint('The final prediction from neural network are: ')\\n\\nprint(yhat) if __name__ == '__main__':\\n\\nneural_network = NeuralNetwork()\\n\\nprint('Random starting input to hidden weights: ')\\n\\nprint(neural_network.wij)\\n\\nprint('Random starting hidden to output weights: ')\\n\\nprint(neural_network.wjk)\\n\\nX = np.array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\\n\\ny = np.array([[0, 1, 1, 0]]).T\\n\\nneural_network.gradient_descent(X, y, 10000)\\n\\nReferences:  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model</td>\n",
       "      <td>1. Introduction of Word2vec\\n\\nWord2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another.\\n\\nThere are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit. For more details about the word2vec algorithm, please check here.\\n\\n2. Gensim Python Library Introduction\\n\\nGensim is an open source python library for natural language processing and it was developed and is maintained by the Czech natural language processing researcher Radim Řehůřek. Gensim library will enable us to develop word embeddings by training our own word2vec models on a custom corpus either with CBOW of skip-grams algorithms.\\n\\nAt first, we need to install the genism package. Gensim runs on Linux, Windows and Mac OS X, and should run on any other platform that supports Python 2.7+ and NumPy. Gensim depends on the following software:\\n\\nPython &gt;= 2.7 (tested with versions 2.7, 3.5 and 3.6)\\n\\n&gt;= 2.7 (tested with versions 2.7, 3.5 and 3.6) NumPy &gt;= 1.11.3\\n\\n&gt;= 1.11.3 SciPy &gt;= 0.18.1\\n\\n&gt;= 0.18.1 Six &gt;= 1.5.0\\n\\n&gt;= 1.5.0 smart_open &gt;= 1.2.1\\n\\nThere are two ways for installation. We could run the following code in our terminal to install genism package.\\n\\npip install --upgrade gensim\\n\\nOr, alternatively for Conda environments:\\n\\nconda install -c conda-forge gensim\\n\\n3. Implementation of word Embedding with Gensim Word2Vec Model\\n\\nIn this tutorial, I will show how to generate word embedding with genism using a concrete example. The dataset I used for this tutorial is from Kaggle Dataset.\\n\\nThis vehicle dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The full python tutorial can be found here.\\n\\n&gt;&gt;&gt; df = pd.read_csv('data.csv')\\n\\n&gt;&gt;&gt; df.head()\\n\\n3.1 Data Preprocessing:\\n\\nSince the purpose of this tutorial is to learn how to generate word embeddings using genism library, we will not do the EDA and feature selection for the word2vec model for the sake of simplicity.\\n\\nGenism word2vec requires that a format of ‘list of lists’ for training where every document is contained in a list and every list contains lists of tokens of that document. At first, we need to generate a format of ‘list of lists’ for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains lists of features of that make model.\\n\\nTo achieve this, we need to do the following things :\\n\\na. Create a new column for Make Model\\n\\n&gt;&gt;&gt; df['Maker_Model']= df['Make']+ \" \" + df['Model']\\n\\nb. Generate a format of ‘ list of lists’ for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size, Vehicle Style.\\n\\n# Select features from original dataset to form a new dataframe\\n\\n&gt;&gt;&gt; df1 = df[['Engine Fuel Type','Transmission Type','Driven_Wheels','Market Category','Vehicle Size', 'Vehicle Style', 'Maker_Model']] # For each row, combine all the columns into one column\\n\\n&gt;&gt;&gt; df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1) # Store them in a pandas dataframe\\n\\n&gt;&gt;&gt; df_clean = pd.DataFrame({'clean': df2}) # Create the list of list format of the custom corpus for gensim modeling\\n\\n&gt;&gt;&gt; sent = [row.split(',') for row in df_clean['clean']] # show the example of list of list format of the custom corpus for gensim modeling\\n\\n&gt;&gt;&gt; sent[:2]\\n\\n[['premium unleaded (required)',\\n\\n'MANUAL',\\n\\n'rear wheel drive',\\n\\n'Factory Tuner',\\n\\n'Luxury',\\n\\n'High-Performance',\\n\\n'Compact',\\n\\n'Coupe',\\n\\n'BMW 1 Series M'],\\n\\n['premium unleaded (required)',\\n\\n'MANUAL',\\n\\n'rear wheel drive',\\n\\n'Luxury',\\n\\n'Performance',\\n\\n'Compact',\\n\\n'Convertible',\\n\\n'BMW 1 Series']]\\n\\n3.2. Genism word2vec Model Training\\n\\nWe can train the genism word2vec model with our own custom corpus as following:\\n\\n&gt;&gt;&gt; model = Word2Vec(sent, min_count=1,size= 50,workers=3, window =3, sg = 1)\\n\\nLet’s try to understand the hyperparameters of this model.\\n\\nsize: The number of dimensions of the embeddings and the default is 100.\\n\\nwindow: The maximum distance between a target word and words around the target word. The default window is 5.\\n\\nmin_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.\\n\\nworkers: The number of partitions during training and the default workers is 3.\\n\\nsg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.\\n\\nAfter training the word2vec model, we can obtain the word embedding directly from the training model as following.\\n\\n&gt;&gt;&gt; model['Toyota Camry'] array([-0.11884457, 0.03035539, -0.0248678 , -0.06297892, -0.01703234,\\n\\n-0.03832747, -0.0825972 , -0.00268112, -0.09192555, -0.08458661,\\n\\n-0.07199778, 0.05235871, 0.21303181, 0.15767808, -0.1883737 ,\\n\\n0.01938575, -0.24431638, 0.04261152, 0.11865819, 0.09881561,\\n\\n-0.04580643, -0.08342388, -0.01355413, -0.07892415, -0.08467747,\\n\\n-0.0040625 , 0.16796461, 0.14578669, 0.04187112, -0.01436194,\\n\\n-0.25554284, 0.25494182, 0.05522631, 0.19295982, 0.14461821,\\n\\n0.14022525, -0.2065216 , -0.05020927, -0.08133671, 0.18031682,\\n\\n0.35042757, 0.0245426 , 0.15938364, -0.05617865, 0.00297452,\\n\\n0.15442047, -0.01286271, 0.13923576, 0.085941 , 0.18811756],\\n\\ndtype=float32)\\n\\n4. Compute Similarities\\n\\nNow we could even use Word2vec to compute the similarity between two Make Models in the vocabulary by invoking the model.similarity( ) and passing in the relevant words. For instance, model.similarity(‘Porsche 718 Cayman’, ‘Nissan Van’) This will give us the Euclidian similarity between Porsche 718 Cayman and Nissan Van.\\n\\n&gt;&gt;&gt; model.similarity('Porsche 718 Cayman', 'Nissan Van')\\n\\n0.822824584626184 &gt;&gt;&gt; model.similarity('Porsche 718 Cayman', 'Mercedes-Benz SLK-Class')\\n\\n0.961089779453727\\n\\nFrom the above examples, we can tell that Porsche 718 Cayman is more similar to Mercedes-Benz SLK-Class than Nissan Van. We also can use the built-in function model.most_similar() to get a set of the most similar make models for a given make model based on the Euclidean distance.\\n\\n&gt;&gt;&gt; model1.most_similar('Mercedes-Benz SLK-Class')[:5] [('BMW M4', 0.9959905743598938),\\n\\n('Maserati Coupe', 0.9949707984924316),\\n\\n('Porsche Cayman', 0.9945154190063477),\\n\\n('Mercedes-Benz SLS AMG GT', 0.9944609999656677),\\n\\n('Maserati Spyder', 0.9942780137062073)]\\n\\nHowever, Euclidian similarity cannot work well for the high-dimensional word vectors. This is because Euclidian similarity will increase as the number of dimensions increases, even if the word embedding stands for different meanings. Alternatively, we can use cosine similarity to measure the similarity between two vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at a 0-degree angle.\\n\\nThe following function shows how can we generate the most similar make model based on cosine similarity.\\n\\ndef cosine_distance (model, word,target_list , num) :\\n\\ncosine_dict ={}\\n\\nword_list = []\\n\\na = model[word]\\n\\nfor item in target_list :\\n\\nif item != word :\\n\\nb = model [item]\\n\\ncos_sim = dot(a, b)/(norm(a)*norm(b))\\n\\ncosine_dict[item] = cos_sim\\n\\ndist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order\\n\\nfor item in dist_sort:\\n\\nword_list.append((item[0], item[1]))\\n\\nreturn word_list[0:num] # only get the unique Maker_Model\\n\\n&gt;&gt;&gt; Maker_Model = list(df.Maker_Model.unique()) # Show the most similar Mercedes-Benz SLK-Class by cosine distance\\n\\n&gt;&gt;&gt; cosine_distance (model,'Mercedes-Benz SLK-Class',Maker_Model,5) [('Mercedes-Benz CLK-Class', 0.99737006),\\n\\n('Aston Martin DB9', 0.99593246),\\n\\n('Maserati Spyder', 0.99571854),\\n\\n('Ferrari 458 Italia', 0.9952333),\\n\\n('Maserati GranTurismo Convertible', 0.994994)]\\n\\n5. T-SNE Visualizations\\n\\nIt’s hard to visualize the word embedding directly, for they usually have more than 3 dimensions. T-SNE is a useful tool to visualize high-dimensional data by dimension reduction while keeping relative pairwise distance between points. It can be said that T-SNE looking for a new data representation where the neighborhood relations are preserved. The following code shows how to plot the word embedding with T-SNE plot.\\n\\ndef display_closestwords_tsnescatterplot(model, word, size):\\n\\n\\n\\narr = np.empty((0,size), dtype='f')\\n\\nword_labels = [word] close_words = model.similar_by_word(word) arr = np.append(arr, np.array([model[word]]), axis=0)\\n\\nfor wrd_score in close_words:\\n\\nwrd_vector = model[wrd_score[0]]\\n\\nword_labels.append(wrd_score[0])\\n\\narr = np.append(arr, np.array([wrd_vector]), axis=0)\\n\\n\\n\\ntsne = TSNE(n_components=2, random_state=0)\\n\\nnp.set_printoptions(suppress=True)\\n\\nY = tsne.fit_transform(arr) x_coords = Y[:, 0]\\n\\ny_coords = Y[:, 1]\\n\\nplt.scatter(x_coords, y_coords) for label, x, y in zip(word_labels, x_coords, y_coords):\\n\\nplt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\\n\\nplt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\\n\\nplt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\\n\\nplt.show() &gt;&gt;&gt; display_closestwords_tsnescatterplot(model, 'Porsche 718 Cayman', 50)\\n\\nThis T-SNE plot shows the top 10 similar vehicles to the Porsche 718 Cayman in two-dimensional space.\\n\\nAbout Me\\n\\nI am a master student in Data Science at the University of San Francisco. I am passionate about using Machine Learning to solve business challenges. You can also find me through Linkedin.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hands-on Graph Neural Networks with PyTorch &amp; PyTorch Geometric</td>\n",
       "      <td>In my last article, I introduced the concept of Graph Neural Network (GNN) and some recent advancements of it. Since this topic is getting seriously hyped up, I decided to make this tutorial on how to easily implement your Graph Neural Network in your project. You will learn how to construct your own GNN with PyTorch Geometric, and how to use GNN to solve a real-world problem (Recsys Challenge 2015).\\n\\nIn this blog post, we will be using PyTorch and PyTorch Geometric (PyG), a Graph Neural Network framework built on top of PyTorch that runs blazingly fast. It is several times faster than the most well-known GNN framework, DGL.\\n\\nAside from its remarkable speed, PyG comes with a collection of well-implemented GNN models illustrated in various papers. Therefore, it would be very handy to reproduce the experiments with PyG.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to Use ggplot2 in Python</td>\n",
       "      <td>Introduction\\n\\nThanks to its strict implementation of the grammar of graphics, ggplot2 provides an extremely intuitive and consistent way of plotting your data. Not only does ggplot2’s approach to plotting ensure that each plot comprises certain basic elements but it also simplifies the readability of your code to a great extent.\\n\\nHowever, if you are a frequent user of Python, then implementing the grammar of graphics can be extremely challenging due to the lack of standardized syntax in popular plotting libraries, such as matplotlib or seaborn. Should you still want to make use of the grammar of graphics, then the Python package plotnine provides an alternative for you.\\n\\nThe Grammar of Graphics\\n\\nIn case you should be unfamiliar with the grammar of graphics, here is a quick overview:\\n\\nMain Components of the Grammar of Graphics\\n\\nAs you can see, there are several components that make up the grammar of graphics, starting with your data. After identifying the data you would like to visualize, you have to specify the variables you are interested in. For instance, you might want to display one variable on the x-axis and another on the y-axis. Third, you have to define what type of geometric object (geom for short) you would like to utilize. This could be anything from a bar plot to a scatter plot or any of the other existing plot types.\\n\\nThese first three components are compulsory. Without data, there is nothing to plot. Without axis definitions, there is nothing to plot either. And finally, without defining a geometric object, you will only see an empty coordinate system. The remaining components making up the grammar of graphics are optional and can be implemented to improve visualizations. Facets refer to specifications of subplots, that is, plotting several variables within your data next to one another in separate plots. Statistical transformations mainly refer to the inclusion of summary statistics in your plot, such as the median or percentiles. Coordinates describe the different coordinate systems available to you. The most used and default coordinate system is the Cartesian coordinate system. Depending on the structure of the data you would like to plot, lesser used coordinate systems, such as the Polar coordinate system, might provide a better way of visualizing your data. Finally, themes provide a variety of options to design all non-data elements of your plot, such as the legend, background, or annotations.\\n\\nWhile there are many ways of visualizing the grammar of graphics, I particularly like the one I created above because it implies the additivity of these layers as well as the fact that they are building upon one another. If you have ever used ggplot2, you are familiar with the ‘+’ in its syntax that symbolizes the same idea described above.\\n\\nplotnine\\n\\nplotnine is a Python package allowing you to use ggplot2-like code that is implementing the grammar of graphics. By doing so, just as in ggplot2, you are able to specifically map data to visual objects that make up the visualization. This enables you to improve both the readability as well as the structure of your code. While you could set matplotlib’s style to ggplot, you cannot implement the grammar of graphics in matplotlib the same way you can in ggplot2.\\n\\nInstallation\\n\\nBefore getting started, you have to install plotnine. As always, there are two main options for doing so: pip and conda.\\n\\nPlotting\\n\\nHaving installed plotnine, you can get started plotting using the grammar of graphics. Let us begin by building a very simple plot only using the three requisite components: data, aesthetics, and geometric objects.\\n\\nBuilding a plot using the grammar of graphics\\n\\nAs you can see, the syntax is very similar to ggplot2. First, we specify the data source. In our case, the data we are using is the classic mpg data set. Next, we define that the variable ‘class’ is going to be displayed on the x-axis. Lastly, we say that we would like to use a bar plot with bars of size 20 to visualize our data. Let us look at the complete code and the resulting plot:\\n\\nThe code above will yield the following output:\\n\\nWhile this is a good start, it is not very nice to look at yet. Let us use other components of the grammar of graphics to beautify our plot.\\n\\nFor instance, we could flip the axes using coord_flip() and customize the plot and axes titles with labs() to improve our plot. Using the code chunk above, our plot would look like this:\\n\\nPlotting Multidimensional Data\\n\\nBesides basic plots, you can do almost everything you could otherwise do in ggplot2, such as plotting multidimensional data. If you would like to visualize the relationships between three variables you could add aesthetics to an otherwise two-dimensional plot:\\n\\nAdding color to the aesthetics will prompt plotnine to display a two-dimensional plot using displ (engine displacement, in liters) on its x- and hwy (highway miles per gallon) on its y-axis and color the data according to the variable class. We have also switched the geometric object to geom_point(), which will give us a scatter instead of a bar plot. Let us take a look at what that would look like:\\n\\nConclusion\\n\\nAs you can see, plotnine provides you with the ability to utilize the grammar of graphics within Python. This increases the readability of your code and allows you to specifically map parts of your data to visual objects. If you are already familiar with ggplot2, then you won’t have to learn anything new to master plotnine. If not, here is a link to the ggplot2 website on which you can find out plenty more about the grammar of graphics and all types of geometric objects available to you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Databricks: How to Save Data Frames as CSV Files on Your Local Computer</td>\n",
       "      <td>Photo credit to Mika Baumeister from Unsplash\\n\\nWhen I work on Python projects dealing with large datasets, I usually use Spyder. The environment of Spyder is very simple; I can browse through working directories, maintain large code bases and review data frames I create. However, if I don’t subset the large data, I constantly face memory issues and struggle with very long computational time. For this reason, I occasionally use Databricks. Databricks is a Microsoft Azure platform where you can easily parse large amounts of data into “notebooks” and perform Apache Spark-based analytics.\\n\\nIf you want to work with data frames and run models using pyspark, you can easily refer to Databricks’ website for more information. However, while working on Databricks, I noticed that saving files in CSV, which is supposed to be quite easy, is not very straightforward. In the following section, I would like to share how you can save data frames from Databricks into CSV format on your local computer with no hassles.\\n\\n1. Explore the Databricks File System (DBFS)\\n\\nFrom Azure Databricks home, you can go to “Upload Data” (under Common Tasks)→ “DBFS” → “FileStore”.\\n\\nDBFS FileStore is where you create folders and save your data frames into CSV format. By default, FileStore has three folders: import-stage, plots, and tables.\\n\\n2. Save a data frame into CSV in FileStore\\n\\nSample.coalesce(1).write.format(“com.databricks.spark.csv”).option(“header”, “true”).save(“dbfs:/FileStore/df/Sample.csv”)\\n\\nUsing the above code on the notebook, I created a folder “df” and saved a data frame “Sample” into CSV. It is important to use coalesce(1) since it saves the data frame as a whole. At the end of this article, I will also demonstrate what happens when you don’t include coalesce(1) in the code.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Step-by-Step Implementation of Gradient Descent and Backpropagation</td>\n",
       "      <td>A Step-by-Step Implementation of Gradient Descent and Backpropagation\\n\\nThe original intention behind this post was merely me brushing upon mathematics in neural network, as I like to be well versed in the inner workings of algorithms and get to the essence of things. I then think I might as well put together a story rather than just revisiting the formulas on my notepad over and over. Though you might find a number of tutorials for building a simple neural network from scratch. Different people have varied angles of seeing things as well as the emphasis of study. Another way of thinking might in some sense enhance understanding. So let’s dive in.\\n\\nPhoto from Unsplash\\n\\nNeural network in a nutshell\\n\\nThe core of neural network is a big function that maps some input to the desired target value, in the intermediate step does the operation to produce the network, which is by multiplying weights and add bias in a pipeline scenario that does this over and over again. The process of training a neural network is to determine a set of parameters that minimize the difference between expected value and model output. This is done using gradient descent (aka backpropagation), which by definition comprises two steps: calculating gradients of the loss/error function, then updating existing parameters in response to the gradients, which is how the descent is done. This cycle is repeated until reaching the minima of the loss function. This learning process can be described by the simple equation: W(t+1) = W(t) — dJ(W)/dW(t).\\n\\nThe mathematical intuition\\n\\nPhoto from https://blog.goodaudience.com/artificial-neural-networks-explained-436fcf36e75\\n\\nFor my own practice purpose, I like to use a small network with a single hidden layer as in the diagram. In this layout, X represents input, subscripts i, j, k denote the number of units in the input, hidden and output layers respectively; w_ij represents the weights connecting input to hidden layer, and w_jk is the weights connecting hidden to output layer.\\n\\nThe model output calculation, in this case, would be:\\n\\nOften the choice of the loss function is the sum of squared error. Here I use sigmoid activation function and assume bias b is 0 for simplicity, meaning weights are the only variables that affect model output. Let’s derive the formula for calculating gradients of hidden to output weights w_jk.\\n\\nThe complexity of determining input to hidden weights is that it affects output error indirectly. Each hidden unit output affects model output, thus input to hidden weights w_ij depend on the errors at all of the units it is connected to. The derivation starts the same, just to expand the chain rule at z_k to the subfunction.\\n\\nMore thoughts:\\n\\nNotice that the gradients of the two weights have a similar form. The error is backpropagated via the derivative of activation function, then weighted by the input (the activation output) from the previous layer. In the second formula, the backpropagated error from the output layer is further projected onto w_jk, then repeat the same way of backpropagation and weighted by the input. This backpropagating process is iterated all the way back to the very first layer in an arbitrary-layer neural network. “The gradients with respect to each parameter are thus considered to be the contribution of the parameter to the error and should be negated during learning.”\\n\\nPutting the above process into code:\\n\\nBelow is the complete example:\\n\\nimport numpy as np class NeuralNetwork:\\n\\ndef __init__(self):\\n\\nnp.random.seed(10) # for generating the same results\\n\\nself.wij = np.random.rand(3,4) # input to hidden layer weights\\n\\nself.wjk = np.random.rand(4,1) # hidden layer to output weights\\n\\n\\n\\ndef sigmoid(self, x, w):\\n\\nz = np.dot(x, w)\\n\\nreturn 1/(1 + np.exp(-z))\\n\\n\\n\\ndef sigmoid_derivative(self, x, w):\\n\\nreturn self.sigmoid(x, w) * (1 - self.sigmoid(x, w))\\n\\n\\n\\ndef gradient_descent(self, x, y, iterations):\\n\\nfor i in range(iterations):\\n\\nXi = x\\n\\nXj = self.sigmoid(Xi, self.wij)\\n\\nyhat = self.sigmoid(Xj, self.wjk)\\n\\n# gradients for hidden to output weights\\n\\ng_wjk = np.dot(Xj.T, (y - yhat) * self.sigmoid_derivative(Xj, self.wjk))\\n\\n# gradients for input to hidden weights\\n\\ng_wij = np.dot(Xi.T, np.dot((y - yhat) * self.sigmoid_derivative(Xj, self.wjk), self.wjk.T) * self.sigmoid_derivative(Xi, self.wij))\\n\\n# update weights\\n\\nself.wij += g_wij\\n\\nself.wjk += g_wjk\\n\\nprint('The final prediction from neural network are: ')\\n\\nprint(yhat) if __name__ == '__main__':\\n\\nneural_network = NeuralNetwork()\\n\\nprint('Random starting input to hidden weights: ')\\n\\nprint(neural_network.wij)\\n\\nprint('Random starting hidden to output weights: ')\\n\\nprint(neural_network.wjk)\\n\\nX = np.array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\\n\\ny = np.array([[0, 1, 1, 0]]).T\\n\\nneural_network.gradient_descent(X, y, 10000)\\n\\nReferences:</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 201
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:09:07.695896Z",
     "start_time": "2024-04-26T20:09:07.693478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_text(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    data[\"Text\"] = data[\"Text\"].str.replace(\"\\n\", \" \")\n",
    "    data[\"Text\"] = data[\"Text\"].str.replace(\"\\t\", \" \")\n",
    "    data[\"Text\"] = data[\"Text\"].str.replace(\"\\r\", \" \")\n",
    "    data[\"Text\"] = data[\"Text\"].str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    data[\"Text\"] = data[\"Text\"].str.strip()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def sample_random_texts(data: pd.DataFrame, n=5) -> None:\n",
    "    for text in data.sample(n)[\"Text\"].values:\n",
    "        print(text)\n",
    "        print(\"\\n\\n\")\n",
    "        \n",
    "        "
   ],
   "id": "8c40e3043864d4c1",
   "outputs": [],
   "execution_count": 202
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:09:07.905036Z",
     "start_time": "2024-04-26T20:09:07.696666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = preprocess_text(data)\n",
    "data.head()"
   ],
   "id": "b24eff34bcbc7220",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                     Title  \\\n",
       "0          A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model   \n",
       "1          Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric   \n",
       "2                                             How to Use ggplot2 in Python   \n",
       "3  Databricks: How to Save Data Frames as CSV Files on Your Local Computer   \n",
       "4    A Step-by-Step Implementation of Gradient Descent and Backpropagation   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Text  \n",
       "0  1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit. For more details about the word2vec algorithm, please check here. 2. Gensim Python Library Introduction Gensim is an open source python library for natural language processing and it was developed and is maintained by the Czech natural language processing researcher Radim Řehůřek. Gensim library will enable us to develop word embeddings by training our own word2vec models on a custom corpus either with CBOW of skip-grams algorithms. At first, we need to install the genism package. Gensim runs on Linux, Windows and Mac OS X, and should run on any other platform that supports Python 2.7+ and NumPy. Gensim depends on the following software: Python >= 2.7 (tested with versions 2.7, 3.5 and 3.6) >= 2.7 (tested with versions 2.7, 3.5 and 3.6) NumPy >= 1.11.3 >= 1.11.3 SciPy >= 0.18.1 >= 0.18.1 Six >= 1.5.0 >= 1.5.0 smart_open >= 1.2.1 There are two ways for installation. We could run the following code in our terminal to install genism package. pip install --upgrade gensim Or, alternatively for Conda environments: conda install -c conda-forge gensim 3. Implementation of word Embedding with Gensim Word2Vec Model In this tutorial, I will show how to generate word embedding with genism using a concrete example. The dataset I used for this tutorial is from Kaggle Dataset. This vehicle dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The full python tutorial can be found here. >>> df = pd.read_csv('data.csv') >>> df.head() 3.1 Data Preprocessing: Since the purpose of this tutorial is to learn how to generate word embeddings using genism library, we will not do the EDA and feature selection for the word2vec model for the sake of simplicity. Genism word2vec requires that a format of ‘list of lists’ for training where every document is contained in a list and every list contains lists of tokens of that document. At first, we need to generate a format of ‘list of lists’ for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains lists of features of that make model. To achieve this, we need to do the following things : a. Create a new column for Make Model >>> df['Maker_Model']= df['Make']+ \" \" + df['Model'] b. Generate a format of ‘ list of lists’ for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size, Vehicle Style. # Select features from original dataset to form a new dataframe >>> df1 = df[['Engine Fuel Type','Transmission Type','Driven_Wheels','Market Category','Vehicle Size', 'Vehicle Style', 'Maker_Model']] # For each row, combine all the columns into one column >>> df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1) # Store them in a pandas dataframe >>> df_clean = pd.DataFrame({'clean': df2}) # Create the list of list format of the custom corpus for gensim modeling >>> sent = [row.split(',') for row in df_clean['clean']] # show the example of list of list format of the custom corpus for gensim modeling >>> sent[:2] [['premium unleaded (required)', 'MANUAL', 'rear wheel drive', 'Factory Tuner', 'Luxury', 'High-Performance', 'Compact', 'Coupe', 'BMW 1 Series M'], ['premium unleaded (required)', 'MANUAL', 'rear wheel drive', 'Luxury', 'Performance', 'Compact', 'Convertible', 'BMW 1 Series']] 3.2. Genism word2vec Model Training We can train the genism word2vec model with our own custom corpus as following: >>> model = Word2Vec(sent, min_count=1,size= 50,workers=3, window =3, sg = 1) Let’s try to understand the hyperparameters of this model. size: The number of dimensions of the embeddings and the default is 100. window: The maximum distance between a target word and words around the target word. The default window is 5. min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5. workers: The number of partitions during training and the default workers is 3. sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW. After training the word2vec model, we can obtain the word embedding directly from the training model as following. >>> model['Toyota Camry'] array([-0.11884457, 0.03035539, -0.0248678 , -0.06297892, -0.01703234, -0.03832747, -0.0825972 , -0.00268112, -0.09192555, -0.08458661, -0.07199778, 0.05235871, 0.21303181, 0.15767808, -0.1883737 , 0.01938575, -0.24431638, 0.04261152, 0.11865819, 0.09881561, -0.04580643, -0.08342388, -0.01355413, -0.07892415, -0.08467747, -0.0040625 , 0.16796461, 0.14578669, 0.04187112, -0.01436194, -0.25554284, 0.25494182, 0.05522631, 0.19295982, 0.14461821, 0.14022525, -0.2065216 , -0.05020927, -0.08133671, 0.18031682, 0.35042757, 0.0245426 , 0.15938364, -0.05617865, 0.00297452, 0.15442047, -0.01286271, 0.13923576, 0.085941 , 0.18811756], dtype=float32) 4. Compute Similarities Now we could even use Word2vec to compute the similarity between two Make Models in the vocabulary by invoking the model.similarity( ) and passing in the relevant words. For instance, model.similarity(‘Porsche 718 Cayman’, ‘Nissan Van’) This will give us the Euclidian similarity between Porsche 718 Cayman and Nissan Van. >>> model.similarity('Porsche 718 Cayman', 'Nissan Van') 0.822824584626184 >>> model.similarity('Porsche 718 Cayman', 'Mercedes-Benz SLK-Class') 0.961089779453727 From the above examples, we can tell that Porsche 718 Cayman is more similar to Mercedes-Benz SLK-Class than Nissan Van. We also can use the built-in function model.most_similar() to get a set of the most similar make models for a given make model based on the Euclidean distance. >>> model1.most_similar('Mercedes-Benz SLK-Class')[:5] [('BMW M4', 0.9959905743598938), ('Maserati Coupe', 0.9949707984924316), ('Porsche Cayman', 0.9945154190063477), ('Mercedes-Benz SLS AMG GT', 0.9944609999656677), ('Maserati Spyder', 0.9942780137062073)] However, Euclidian similarity cannot work well for the high-dimensional word vectors. This is because Euclidian similarity will increase as the number of dimensions increases, even if the word embedding stands for different meanings. Alternatively, we can use cosine similarity to measure the similarity between two vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at a 0-degree angle. The following function shows how can we generate the most similar make model based on cosine similarity. def cosine_distance (model, word,target_list , num) : cosine_dict ={} word_list = [] a = model[word] for item in target_list : if item != word : b = model [item] cos_sim = dot(a, b)/(norm(a)*norm(b)) cosine_dict[item] = cos_sim dist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order for item in dist_sort: word_list.append((item[0], item[1])) return word_list[0:num] # only get the unique Maker_Model >>> Maker_Model = list(df.Maker_Model.unique()) # Show the most similar Mercedes-Benz SLK-Class by cosine distance >>> cosine_distance (model,'Mercedes-Benz SLK-Class',Maker_Model,5) [('Mercedes-Benz CLK-Class', 0.99737006), ('Aston Martin DB9', 0.99593246), ('Maserati Spyder', 0.99571854), ('Ferrari 458 Italia', 0.9952333), ('Maserati GranTurismo Convertible', 0.994994)] 5. T-SNE Visualizations It’s hard to visualize the word embedding directly, for they usually have more than 3 dimensions. T-SNE is a useful tool to visualize high-dimensional data by dimension reduction while keeping relative pairwise distance between points. It can be said that T-SNE looking for a new data representation where the neighborhood relations are preserved. The following code shows how to plot the word embedding with T-SNE plot. def display_closestwords_tsnescatterplot(model, word, size): arr = np.empty((0,size), dtype='f') word_labels = [word] close_words = model.similar_by_word(word) arr = np.append(arr, np.array([model[word]]), axis=0) for wrd_score in close_words: wrd_vector = model[wrd_score[0]] word_labels.append(wrd_score[0]) arr = np.append(arr, np.array([wrd_vector]), axis=0) tsne = TSNE(n_components=2, random_state=0) np.set_printoptions(suppress=True) Y = tsne.fit_transform(arr) x_coords = Y[:, 0] y_coords = Y[:, 1] plt.scatter(x_coords, y_coords) for label, x, y in zip(word_labels, x_coords, y_coords): plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points') plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005) plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005) plt.show() >>> display_closestwords_tsnescatterplot(model, 'Porsche 718 Cayman', 50) This T-SNE plot shows the top 10 similar vehicles to the Porsche 718 Cayman in two-dimensional space. About Me I am a master student in Data Science at the University of San Francisco. I am passionate about using Machine Learning to solve business challenges. You can also find me through Linkedin.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In my last article, I introduced the concept of Graph Neural Network (GNN) and some recent advancements of it. Since this topic is getting seriously hyped up, I decided to make this tutorial on how to easily implement your Graph Neural Network in your project. You will learn how to construct your own GNN with PyTorch Geometric, and how to use GNN to solve a real-world problem (Recsys Challenge 2015). In this blog post, we will be using PyTorch and PyTorch Geometric (PyG), a Graph Neural Network framework built on top of PyTorch that runs blazingly fast. It is several times faster than the most well-known GNN framework, DGL. Aside from its remarkable speed, PyG comes with a collection of well-implemented GNN models illustrated in various papers. Therefore, it would be very handy to reproduce the experiments with PyG.  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Introduction Thanks to its strict implementation of the grammar of graphics, ggplot2 provides an extremely intuitive and consistent way of plotting your data. Not only does ggplot2’s approach to plotting ensure that each plot comprises certain basic elements but it also simplifies the readability of your code to a great extent. However, if you are a frequent user of Python, then implementing the grammar of graphics can be extremely challenging due to the lack of standardized syntax in popular plotting libraries, such as matplotlib or seaborn. Should you still want to make use of the grammar of graphics, then the Python package plotnine provides an alternative for you. The Grammar of Graphics In case you should be unfamiliar with the grammar of graphics, here is a quick overview: Main Components of the Grammar of Graphics As you can see, there are several components that make up the grammar of graphics, starting with your data. After identifying the data you would like to visualize, you have to specify the variables you are interested in. For instance, you might want to display one variable on the x-axis and another on the y-axis. Third, you have to define what type of geometric object (geom for short) you would like to utilize. This could be anything from a bar plot to a scatter plot or any of the other existing plot types. These first three components are compulsory. Without data, there is nothing to plot. Without axis definitions, there is nothing to plot either. And finally, without defining a geometric object, you will only see an empty coordinate system. The remaining components making up the grammar of graphics are optional and can be implemented to improve visualizations. Facets refer to specifications of subplots, that is, plotting several variables within your data next to one another in separate plots. Statistical transformations mainly refer to the inclusion of summary statistics in your plot, such as the median or percentiles. Coordinates describe the different coordinate systems available to you. The most used and default coordinate system is the Cartesian coordinate system. Depending on the structure of the data you would like to plot, lesser used coordinate systems, such as the Polar coordinate system, might provide a better way of visualizing your data. Finally, themes provide a variety of options to design all non-data elements of your plot, such as the legend, background, or annotations. While there are many ways of visualizing the grammar of graphics, I particularly like the one I created above because it implies the additivity of these layers as well as the fact that they are building upon one another. If you have ever used ggplot2, you are familiar with the ‘+’ in its syntax that symbolizes the same idea described above. plotnine plotnine is a Python package allowing you to use ggplot2-like code that is implementing the grammar of graphics. By doing so, just as in ggplot2, you are able to specifically map data to visual objects that make up the visualization. This enables you to improve both the readability as well as the structure of your code. While you could set matplotlib’s style to ggplot, you cannot implement the grammar of graphics in matplotlib the same way you can in ggplot2. Installation Before getting started, you have to install plotnine. As always, there are two main options for doing so: pip and conda. Plotting Having installed plotnine, you can get started plotting using the grammar of graphics. Let us begin by building a very simple plot only using the three requisite components: data, aesthetics, and geometric objects. Building a plot using the grammar of graphics As you can see, the syntax is very similar to ggplot2. First, we specify the data source. In our case, the data we are using is the classic mpg data set. Next, we define that the variable ‘class’ is going to be displayed on the x-axis. Lastly, we say that we would like to use a bar plot with bars of size 20 to visualize our data. Let us look at the complete code and the resulting plot: The code above will yield the following output: While this is a good start, it is not very nice to look at yet. Let us use other components of the grammar of graphics to beautify our plot. For instance, we could flip the axes using coord_flip() and customize the plot and axes titles with labs() to improve our plot. Using the code chunk above, our plot would look like this: Plotting Multidimensional Data Besides basic plots, you can do almost everything you could otherwise do in ggplot2, such as plotting multidimensional data. If you would like to visualize the relationships between three variables you could add aesthetics to an otherwise two-dimensional plot: Adding color to the aesthetics will prompt plotnine to display a two-dimensional plot using displ (engine displacement, in liters) on its x- and hwy (highway miles per gallon) on its y-axis and color the data according to the variable class. We have also switched the geometric object to geom_point(), which will give us a scatter instead of a bar plot. Let us take a look at what that would look like: Conclusion As you can see, plotnine provides you with the ability to utilize the grammar of graphics within Python. This increases the readability of your code and allows you to specifically map parts of your data to visual objects. If you are already familiar with ggplot2, then you won’t have to learn anything new to master plotnine. If not, here is a link to the ggplot2 website on which you can find out plenty more about the grammar of graphics and all types of geometric objects available to you.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Photo credit to Mika Baumeister from Unsplash When I work on Python projects dealing with large datasets, I usually use Spyder. The environment of Spyder is very simple; I can browse through working directories, maintain large code bases and review data frames I create. However, if I don’t subset the large data, I constantly face memory issues and struggle with very long computational time. For this reason, I occasionally use Databricks. Databricks is a Microsoft Azure platform where you can easily parse large amounts of data into “notebooks” and perform Apache Spark-based analytics. If you want to work with data frames and run models using pyspark, you can easily refer to Databricks’ website for more information. However, while working on Databricks, I noticed that saving files in CSV, which is supposed to be quite easy, is not very straightforward. In the following section, I would like to share how you can save data frames from Databricks into CSV format on your local computer with no hassles. 1. Explore the Databricks File System (DBFS) From Azure Databricks home, you can go to “Upload Data” (under Common Tasks)→ “DBFS” → “FileStore”. DBFS FileStore is where you create folders and save your data frames into CSV format. By default, FileStore has three folders: import-stage, plots, and tables. 2. Save a data frame into CSV in FileStore Sample.coalesce(1).write.format(“com.databricks.spark.csv”).option(“header”, “true”).save(“dbfs:/FileStore/df/Sample.csv”) Using the above code on the notebook, I created a folder “df” and saved a data frame “Sample” into CSV. It is important to use coalesce(1) since it saves the data frame as a whole. At the end of this article, I will also demonstrate what happens when you don’t include coalesce(1) in the code.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          A Step-by-Step Implementation of Gradient Descent and Backpropagation The original intention behind this post was merely me brushing upon mathematics in neural network, as I like to be well versed in the inner workings of algorithms and get to the essence of things. I then think I might as well put together a story rather than just revisiting the formulas on my notepad over and over. Though you might find a number of tutorials for building a simple neural network from scratch. Different people have varied angles of seeing things as well as the emphasis of study. Another way of thinking might in some sense enhance understanding. So let’s dive in. Photo from Unsplash Neural network in a nutshell The core of neural network is a big function that maps some input to the desired target value, in the intermediate step does the operation to produce the network, which is by multiplying weights and add bias in a pipeline scenario that does this over and over again. The process of training a neural network is to determine a set of parameters that minimize the difference between expected value and model output. This is done using gradient descent (aka backpropagation), which by definition comprises two steps: calculating gradients of the loss/error function, then updating existing parameters in response to the gradients, which is how the descent is done. This cycle is repeated until reaching the minima of the loss function. This learning process can be described by the simple equation: W(t+1) = W(t) — dJ(W)/dW(t). The mathematical intuition Photo from https://blog.goodaudience.com/artificial-neural-networks-explained-436fcf36e75 For my own practice purpose, I like to use a small network with a single hidden layer as in the diagram. In this layout, X represents input, subscripts i, j, k denote the number of units in the input, hidden and output layers respectively; w_ij represents the weights connecting input to hidden layer, and w_jk is the weights connecting hidden to output layer. The model output calculation, in this case, would be: Often the choice of the loss function is the sum of squared error. Here I use sigmoid activation function and assume bias b is 0 for simplicity, meaning weights are the only variables that affect model output. Let’s derive the formula for calculating gradients of hidden to output weights w_jk. The complexity of determining input to hidden weights is that it affects output error indirectly. Each hidden unit output affects model output, thus input to hidden weights w_ij depend on the errors at all of the units it is connected to. The derivation starts the same, just to expand the chain rule at z_k to the subfunction. More thoughts: Notice that the gradients of the two weights have a similar form. The error is backpropagated via the derivative of activation function, then weighted by the input (the activation output) from the previous layer. In the second formula, the backpropagated error from the output layer is further projected onto w_jk, then repeat the same way of backpropagation and weighted by the input. This backpropagating process is iterated all the way back to the very first layer in an arbitrary-layer neural network. “The gradients with respect to each parameter are thus considered to be the contribution of the parameter to the error and should be negated during learning.” Putting the above process into code: Below is the complete example: import numpy as np class NeuralNetwork: def __init__(self): np.random.seed(10) # for generating the same results self.wij = np.random.rand(3,4) # input to hidden layer weights self.wjk = np.random.rand(4,1) # hidden layer to output weights def sigmoid(self, x, w): z = np.dot(x, w) return 1/(1 + np.exp(-z)) def sigmoid_derivative(self, x, w): return self.sigmoid(x, w) * (1 - self.sigmoid(x, w)) def gradient_descent(self, x, y, iterations): for i in range(iterations): Xi = x Xj = self.sigmoid(Xi, self.wij) yhat = self.sigmoid(Xj, self.wjk) # gradients for hidden to output weights g_wjk = np.dot(Xj.T, (y - yhat) * self.sigmoid_derivative(Xj, self.wjk)) # gradients for input to hidden weights g_wij = np.dot(Xi.T, np.dot((y - yhat) * self.sigmoid_derivative(Xj, self.wjk), self.wjk.T) * self.sigmoid_derivative(Xi, self.wij)) # update weights self.wij += g_wij self.wjk += g_wjk print('The final prediction from neural network are: ') print(yhat) if __name__ == '__main__': neural_network = NeuralNetwork() print('Random starting input to hidden weights: ') print(neural_network.wij) print('Random starting hidden to output weights: ') print(neural_network.wjk) X = np.array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]]) y = np.array([[0, 1, 1, 0]]).T neural_network.gradient_descent(X, y, 10000) References:  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model</td>\n",
       "      <td>1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit. For more details about the word2vec algorithm, please check here. 2. Gensim Python Library Introduction Gensim is an open source python library for natural language processing and it was developed and is maintained by the Czech natural language processing researcher Radim Řehůřek. Gensim library will enable us to develop word embeddings by training our own word2vec models on a custom corpus either with CBOW of skip-grams algorithms. At first, we need to install the genism package. Gensim runs on Linux, Windows and Mac OS X, and should run on any other platform that supports Python 2.7+ and NumPy. Gensim depends on the following software: Python &gt;= 2.7 (tested with versions 2.7, 3.5 and 3.6) &gt;= 2.7 (tested with versions 2.7, 3.5 and 3.6) NumPy &gt;= 1.11.3 &gt;= 1.11.3 SciPy &gt;= 0.18.1 &gt;= 0.18.1 Six &gt;= 1.5.0 &gt;= 1.5.0 smart_open &gt;= 1.2.1 There are two ways for installation. We could run the following code in our terminal to install genism package. pip install --upgrade gensim Or, alternatively for Conda environments: conda install -c conda-forge gensim 3. Implementation of word Embedding with Gensim Word2Vec Model In this tutorial, I will show how to generate word embedding with genism using a concrete example. The dataset I used for this tutorial is from Kaggle Dataset. This vehicle dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The full python tutorial can be found here. &gt;&gt;&gt; df = pd.read_csv('data.csv') &gt;&gt;&gt; df.head() 3.1 Data Preprocessing: Since the purpose of this tutorial is to learn how to generate word embeddings using genism library, we will not do the EDA and feature selection for the word2vec model for the sake of simplicity. Genism word2vec requires that a format of ‘list of lists’ for training where every document is contained in a list and every list contains lists of tokens of that document. At first, we need to generate a format of ‘list of lists’ for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains lists of features of that make model. To achieve this, we need to do the following things : a. Create a new column for Make Model &gt;&gt;&gt; df['Maker_Model']= df['Make']+ \" \" + df['Model'] b. Generate a format of ‘ list of lists’ for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size, Vehicle Style. # Select features from original dataset to form a new dataframe &gt;&gt;&gt; df1 = df[['Engine Fuel Type','Transmission Type','Driven_Wheels','Market Category','Vehicle Size', 'Vehicle Style', 'Maker_Model']] # For each row, combine all the columns into one column &gt;&gt;&gt; df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1) # Store them in a pandas dataframe &gt;&gt;&gt; df_clean = pd.DataFrame({'clean': df2}) # Create the list of list format of the custom corpus for gensim modeling &gt;&gt;&gt; sent = [row.split(',') for row in df_clean['clean']] # show the example of list of list format of the custom corpus for gensim modeling &gt;&gt;&gt; sent[:2] [['premium unleaded (required)', 'MANUAL', 'rear wheel drive', 'Factory Tuner', 'Luxury', 'High-Performance', 'Compact', 'Coupe', 'BMW 1 Series M'], ['premium unleaded (required)', 'MANUAL', 'rear wheel drive', 'Luxury', 'Performance', 'Compact', 'Convertible', 'BMW 1 Series']] 3.2. Genism word2vec Model Training We can train the genism word2vec model with our own custom corpus as following: &gt;&gt;&gt; model = Word2Vec(sent, min_count=1,size= 50,workers=3, window =3, sg = 1) Let’s try to understand the hyperparameters of this model. size: The number of dimensions of the embeddings and the default is 100. window: The maximum distance between a target word and words around the target word. The default window is 5. min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5. workers: The number of partitions during training and the default workers is 3. sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW. After training the word2vec model, we can obtain the word embedding directly from the training model as following. &gt;&gt;&gt; model['Toyota Camry'] array([-0.11884457, 0.03035539, -0.0248678 , -0.06297892, -0.01703234, -0.03832747, -0.0825972 , -0.00268112, -0.09192555, -0.08458661, -0.07199778, 0.05235871, 0.21303181, 0.15767808, -0.1883737 , 0.01938575, -0.24431638, 0.04261152, 0.11865819, 0.09881561, -0.04580643, -0.08342388, -0.01355413, -0.07892415, -0.08467747, -0.0040625 , 0.16796461, 0.14578669, 0.04187112, -0.01436194, -0.25554284, 0.25494182, 0.05522631, 0.19295982, 0.14461821, 0.14022525, -0.2065216 , -0.05020927, -0.08133671, 0.18031682, 0.35042757, 0.0245426 , 0.15938364, -0.05617865, 0.00297452, 0.15442047, -0.01286271, 0.13923576, 0.085941 , 0.18811756], dtype=float32) 4. Compute Similarities Now we could even use Word2vec to compute the similarity between two Make Models in the vocabulary by invoking the model.similarity( ) and passing in the relevant words. For instance, model.similarity(‘Porsche 718 Cayman’, ‘Nissan Van’) This will give us the Euclidian similarity between Porsche 718 Cayman and Nissan Van. &gt;&gt;&gt; model.similarity('Porsche 718 Cayman', 'Nissan Van') 0.822824584626184 &gt;&gt;&gt; model.similarity('Porsche 718 Cayman', 'Mercedes-Benz SLK-Class') 0.961089779453727 From the above examples, we can tell that Porsche 718 Cayman is more similar to Mercedes-Benz SLK-Class than Nissan Van. We also can use the built-in function model.most_similar() to get a set of the most similar make models for a given make model based on the Euclidean distance. &gt;&gt;&gt; model1.most_similar('Mercedes-Benz SLK-Class')[:5] [('BMW M4', 0.9959905743598938), ('Maserati Coupe', 0.9949707984924316), ('Porsche Cayman', 0.9945154190063477), ('Mercedes-Benz SLS AMG GT', 0.9944609999656677), ('Maserati Spyder', 0.9942780137062073)] However, Euclidian similarity cannot work well for the high-dimensional word vectors. This is because Euclidian similarity will increase as the number of dimensions increases, even if the word embedding stands for different meanings. Alternatively, we can use cosine similarity to measure the similarity between two vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at a 0-degree angle. The following function shows how can we generate the most similar make model based on cosine similarity. def cosine_distance (model, word,target_list , num) : cosine_dict ={} word_list = [] a = model[word] for item in target_list : if item != word : b = model [item] cos_sim = dot(a, b)/(norm(a)*norm(b)) cosine_dict[item] = cos_sim dist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order for item in dist_sort: word_list.append((item[0], item[1])) return word_list[0:num] # only get the unique Maker_Model &gt;&gt;&gt; Maker_Model = list(df.Maker_Model.unique()) # Show the most similar Mercedes-Benz SLK-Class by cosine distance &gt;&gt;&gt; cosine_distance (model,'Mercedes-Benz SLK-Class',Maker_Model,5) [('Mercedes-Benz CLK-Class', 0.99737006), ('Aston Martin DB9', 0.99593246), ('Maserati Spyder', 0.99571854), ('Ferrari 458 Italia', 0.9952333), ('Maserati GranTurismo Convertible', 0.994994)] 5. T-SNE Visualizations It’s hard to visualize the word embedding directly, for they usually have more than 3 dimensions. T-SNE is a useful tool to visualize high-dimensional data by dimension reduction while keeping relative pairwise distance between points. It can be said that T-SNE looking for a new data representation where the neighborhood relations are preserved. The following code shows how to plot the word embedding with T-SNE plot. def display_closestwords_tsnescatterplot(model, word, size): arr = np.empty((0,size), dtype='f') word_labels = [word] close_words = model.similar_by_word(word) arr = np.append(arr, np.array([model[word]]), axis=0) for wrd_score in close_words: wrd_vector = model[wrd_score[0]] word_labels.append(wrd_score[0]) arr = np.append(arr, np.array([wrd_vector]), axis=0) tsne = TSNE(n_components=2, random_state=0) np.set_printoptions(suppress=True) Y = tsne.fit_transform(arr) x_coords = Y[:, 0] y_coords = Y[:, 1] plt.scatter(x_coords, y_coords) for label, x, y in zip(word_labels, x_coords, y_coords): plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points') plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005) plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005) plt.show() &gt;&gt;&gt; display_closestwords_tsnescatterplot(model, 'Porsche 718 Cayman', 50) This T-SNE plot shows the top 10 similar vehicles to the Porsche 718 Cayman in two-dimensional space. About Me I am a master student in Data Science at the University of San Francisco. I am passionate about using Machine Learning to solve business challenges. You can also find me through Linkedin.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hands-on Graph Neural Networks with PyTorch &amp; PyTorch Geometric</td>\n",
       "      <td>In my last article, I introduced the concept of Graph Neural Network (GNN) and some recent advancements of it. Since this topic is getting seriously hyped up, I decided to make this tutorial on how to easily implement your Graph Neural Network in your project. You will learn how to construct your own GNN with PyTorch Geometric, and how to use GNN to solve a real-world problem (Recsys Challenge 2015). In this blog post, we will be using PyTorch and PyTorch Geometric (PyG), a Graph Neural Network framework built on top of PyTorch that runs blazingly fast. It is several times faster than the most well-known GNN framework, DGL. Aside from its remarkable speed, PyG comes with a collection of well-implemented GNN models illustrated in various papers. Therefore, it would be very handy to reproduce the experiments with PyG.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to Use ggplot2 in Python</td>\n",
       "      <td>Introduction Thanks to its strict implementation of the grammar of graphics, ggplot2 provides an extremely intuitive and consistent way of plotting your data. Not only does ggplot2’s approach to plotting ensure that each plot comprises certain basic elements but it also simplifies the readability of your code to a great extent. However, if you are a frequent user of Python, then implementing the grammar of graphics can be extremely challenging due to the lack of standardized syntax in popular plotting libraries, such as matplotlib or seaborn. Should you still want to make use of the grammar of graphics, then the Python package plotnine provides an alternative for you. The Grammar of Graphics In case you should be unfamiliar with the grammar of graphics, here is a quick overview: Main Components of the Grammar of Graphics As you can see, there are several components that make up the grammar of graphics, starting with your data. After identifying the data you would like to visualize, you have to specify the variables you are interested in. For instance, you might want to display one variable on the x-axis and another on the y-axis. Third, you have to define what type of geometric object (geom for short) you would like to utilize. This could be anything from a bar plot to a scatter plot or any of the other existing plot types. These first three components are compulsory. Without data, there is nothing to plot. Without axis definitions, there is nothing to plot either. And finally, without defining a geometric object, you will only see an empty coordinate system. The remaining components making up the grammar of graphics are optional and can be implemented to improve visualizations. Facets refer to specifications of subplots, that is, plotting several variables within your data next to one another in separate plots. Statistical transformations mainly refer to the inclusion of summary statistics in your plot, such as the median or percentiles. Coordinates describe the different coordinate systems available to you. The most used and default coordinate system is the Cartesian coordinate system. Depending on the structure of the data you would like to plot, lesser used coordinate systems, such as the Polar coordinate system, might provide a better way of visualizing your data. Finally, themes provide a variety of options to design all non-data elements of your plot, such as the legend, background, or annotations. While there are many ways of visualizing the grammar of graphics, I particularly like the one I created above because it implies the additivity of these layers as well as the fact that they are building upon one another. If you have ever used ggplot2, you are familiar with the ‘+’ in its syntax that symbolizes the same idea described above. plotnine plotnine is a Python package allowing you to use ggplot2-like code that is implementing the grammar of graphics. By doing so, just as in ggplot2, you are able to specifically map data to visual objects that make up the visualization. This enables you to improve both the readability as well as the structure of your code. While you could set matplotlib’s style to ggplot, you cannot implement the grammar of graphics in matplotlib the same way you can in ggplot2. Installation Before getting started, you have to install plotnine. As always, there are two main options for doing so: pip and conda. Plotting Having installed plotnine, you can get started plotting using the grammar of graphics. Let us begin by building a very simple plot only using the three requisite components: data, aesthetics, and geometric objects. Building a plot using the grammar of graphics As you can see, the syntax is very similar to ggplot2. First, we specify the data source. In our case, the data we are using is the classic mpg data set. Next, we define that the variable ‘class’ is going to be displayed on the x-axis. Lastly, we say that we would like to use a bar plot with bars of size 20 to visualize our data. Let us look at the complete code and the resulting plot: The code above will yield the following output: While this is a good start, it is not very nice to look at yet. Let us use other components of the grammar of graphics to beautify our plot. For instance, we could flip the axes using coord_flip() and customize the plot and axes titles with labs() to improve our plot. Using the code chunk above, our plot would look like this: Plotting Multidimensional Data Besides basic plots, you can do almost everything you could otherwise do in ggplot2, such as plotting multidimensional data. If you would like to visualize the relationships between three variables you could add aesthetics to an otherwise two-dimensional plot: Adding color to the aesthetics will prompt plotnine to display a two-dimensional plot using displ (engine displacement, in liters) on its x- and hwy (highway miles per gallon) on its y-axis and color the data according to the variable class. We have also switched the geometric object to geom_point(), which will give us a scatter instead of a bar plot. Let us take a look at what that would look like: Conclusion As you can see, plotnine provides you with the ability to utilize the grammar of graphics within Python. This increases the readability of your code and allows you to specifically map parts of your data to visual objects. If you are already familiar with ggplot2, then you won’t have to learn anything new to master plotnine. If not, here is a link to the ggplot2 website on which you can find out plenty more about the grammar of graphics and all types of geometric objects available to you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Databricks: How to Save Data Frames as CSV Files on Your Local Computer</td>\n",
       "      <td>Photo credit to Mika Baumeister from Unsplash When I work on Python projects dealing with large datasets, I usually use Spyder. The environment of Spyder is very simple; I can browse through working directories, maintain large code bases and review data frames I create. However, if I don’t subset the large data, I constantly face memory issues and struggle with very long computational time. For this reason, I occasionally use Databricks. Databricks is a Microsoft Azure platform where you can easily parse large amounts of data into “notebooks” and perform Apache Spark-based analytics. If you want to work with data frames and run models using pyspark, you can easily refer to Databricks’ website for more information. However, while working on Databricks, I noticed that saving files in CSV, which is supposed to be quite easy, is not very straightforward. In the following section, I would like to share how you can save data frames from Databricks into CSV format on your local computer with no hassles. 1. Explore the Databricks File System (DBFS) From Azure Databricks home, you can go to “Upload Data” (under Common Tasks)→ “DBFS” → “FileStore”. DBFS FileStore is where you create folders and save your data frames into CSV format. By default, FileStore has three folders: import-stage, plots, and tables. 2. Save a data frame into CSV in FileStore Sample.coalesce(1).write.format(“com.databricks.spark.csv”).option(“header”, “true”).save(“dbfs:/FileStore/df/Sample.csv”) Using the above code on the notebook, I created a folder “df” and saved a data frame “Sample” into CSV. It is important to use coalesce(1) since it saves the data frame as a whole. At the end of this article, I will also demonstrate what happens when you don’t include coalesce(1) in the code.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Step-by-Step Implementation of Gradient Descent and Backpropagation</td>\n",
       "      <td>A Step-by-Step Implementation of Gradient Descent and Backpropagation The original intention behind this post was merely me brushing upon mathematics in neural network, as I like to be well versed in the inner workings of algorithms and get to the essence of things. I then think I might as well put together a story rather than just revisiting the formulas on my notepad over and over. Though you might find a number of tutorials for building a simple neural network from scratch. Different people have varied angles of seeing things as well as the emphasis of study. Another way of thinking might in some sense enhance understanding. So let’s dive in. Photo from Unsplash Neural network in a nutshell The core of neural network is a big function that maps some input to the desired target value, in the intermediate step does the operation to produce the network, which is by multiplying weights and add bias in a pipeline scenario that does this over and over again. The process of training a neural network is to determine a set of parameters that minimize the difference between expected value and model output. This is done using gradient descent (aka backpropagation), which by definition comprises two steps: calculating gradients of the loss/error function, then updating existing parameters in response to the gradients, which is how the descent is done. This cycle is repeated until reaching the minima of the loss function. This learning process can be described by the simple equation: W(t+1) = W(t) — dJ(W)/dW(t). The mathematical intuition Photo from https://blog.goodaudience.com/artificial-neural-networks-explained-436fcf36e75 For my own practice purpose, I like to use a small network with a single hidden layer as in the diagram. In this layout, X represents input, subscripts i, j, k denote the number of units in the input, hidden and output layers respectively; w_ij represents the weights connecting input to hidden layer, and w_jk is the weights connecting hidden to output layer. The model output calculation, in this case, would be: Often the choice of the loss function is the sum of squared error. Here I use sigmoid activation function and assume bias b is 0 for simplicity, meaning weights are the only variables that affect model output. Let’s derive the formula for calculating gradients of hidden to output weights w_jk. The complexity of determining input to hidden weights is that it affects output error indirectly. Each hidden unit output affects model output, thus input to hidden weights w_ij depend on the errors at all of the units it is connected to. The derivation starts the same, just to expand the chain rule at z_k to the subfunction. More thoughts: Notice that the gradients of the two weights have a similar form. The error is backpropagated via the derivative of activation function, then weighted by the input (the activation output) from the previous layer. In the second formula, the backpropagated error from the output layer is further projected onto w_jk, then repeat the same way of backpropagation and weighted by the input. This backpropagating process is iterated all the way back to the very first layer in an arbitrary-layer neural network. “The gradients with respect to each parameter are thus considered to be the contribution of the parameter to the error and should be negated during learning.” Putting the above process into code: Below is the complete example: import numpy as np class NeuralNetwork: def __init__(self): np.random.seed(10) # for generating the same results self.wij = np.random.rand(3,4) # input to hidden layer weights self.wjk = np.random.rand(4,1) # hidden layer to output weights def sigmoid(self, x, w): z = np.dot(x, w) return 1/(1 + np.exp(-z)) def sigmoid_derivative(self, x, w): return self.sigmoid(x, w) * (1 - self.sigmoid(x, w)) def gradient_descent(self, x, y, iterations): for i in range(iterations): Xi = x Xj = self.sigmoid(Xi, self.wij) yhat = self.sigmoid(Xj, self.wjk) # gradients for hidden to output weights g_wjk = np.dot(Xj.T, (y - yhat) * self.sigmoid_derivative(Xj, self.wjk)) # gradients for input to hidden weights g_wij = np.dot(Xi.T, np.dot((y - yhat) * self.sigmoid_derivative(Xj, self.wjk), self.wjk.T) * self.sigmoid_derivative(Xi, self.wij)) # update weights self.wij += g_wij self.wjk += g_wjk print('The final prediction from neural network are: ') print(yhat) if __name__ == '__main__': neural_network = NeuralNetwork() print('Random starting input to hidden weights: ') print(neural_network.wij) print('Random starting hidden to output weights: ') print(neural_network.wjk) X = np.array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]]) y = np.array([[0, 1, 1, 0]]).T neural_network.gradient_descent(X, y, 10000) References:</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 203
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:09:11.693173Z",
     "start_time": "2024-04-26T20:09:07.906844Z"
    }
   },
   "cell_type": "code",
   "source": "!python -m spacy download en_core_web_sm",
   "id": "9d71c1c765d6f7a4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.8/12.8 MB\u001B[0m \u001B[31m33.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\r\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\r\n",
      "Requirement already satisfied: setuptools in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.5.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\r\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\r\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\r\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\r\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/miloszbargiel/PycharmProjects/MediumRAG/.venv/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_sm')\r\n"
     ]
    }
   ],
   "execution_count": 204
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Feature engineering for text data EDA",
   "id": "5cd572873b2395ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:09:16.098441Z",
     "start_time": "2024-04-26T20:09:11.694696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from spacy.lang.en import English\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "nlp = English()\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "data[\"text_words_num\"] = data[\"Text\"].apply(lambda x: len(x.split()))\n",
    "data[\"text_length\"] = data[\"Text\"].apply(lambda x: len(x))\n",
    "data[\"text_sentences_num\"] = data[\"Text\"].progress_apply(lambda x: len(list(nlp(x).sents)))\n",
    "data[\"token_count\"] = data[\"Text\"].apply(lambda x: len(x) / 4)\n",
    "data.head()"
   ],
   "id": "c11a3100d66138be",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1391/1391 [00:04<00:00, 326.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                                     Title  \\\n",
       "0          A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model   \n",
       "1          Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric   \n",
       "2                                             How to Use ggplot2 in Python   \n",
       "3  Databricks: How to Save Data Frames as CSV Files on Your Local Computer   \n",
       "4    A Step-by-Step Implementation of Gradient Descent and Backpropagation   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Text  \\\n",
       "0  1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit. For more details about the word2vec algorithm, please check here. 2. Gensim Python Library Introduction Gensim is an open source python library for natural language processing and it was developed and is maintained by the Czech natural language processing researcher Radim Řehůřek. Gensim library will enable us to develop word embeddings by training our own word2vec models on a custom corpus either with CBOW of skip-grams algorithms. At first, we need to install the genism package. Gensim runs on Linux, Windows and Mac OS X, and should run on any other platform that supports Python 2.7+ and NumPy. Gensim depends on the following software: Python >= 2.7 (tested with versions 2.7, 3.5 and 3.6) >= 2.7 (tested with versions 2.7, 3.5 and 3.6) NumPy >= 1.11.3 >= 1.11.3 SciPy >= 0.18.1 >= 0.18.1 Six >= 1.5.0 >= 1.5.0 smart_open >= 1.2.1 There are two ways for installation. We could run the following code in our terminal to install genism package. pip install --upgrade gensim Or, alternatively for Conda environments: conda install -c conda-forge gensim 3. Implementation of word Embedding with Gensim Word2Vec Model In this tutorial, I will show how to generate word embedding with genism using a concrete example. The dataset I used for this tutorial is from Kaggle Dataset. This vehicle dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The full python tutorial can be found here. >>> df = pd.read_csv('data.csv') >>> df.head() 3.1 Data Preprocessing: Since the purpose of this tutorial is to learn how to generate word embeddings using genism library, we will not do the EDA and feature selection for the word2vec model for the sake of simplicity. Genism word2vec requires that a format of ‘list of lists’ for training where every document is contained in a list and every list contains lists of tokens of that document. At first, we need to generate a format of ‘list of lists’ for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains lists of features of that make model. To achieve this, we need to do the following things : a. Create a new column for Make Model >>> df['Maker_Model']= df['Make']+ \" \" + df['Model'] b. Generate a format of ‘ list of lists’ for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size, Vehicle Style. # Select features from original dataset to form a new dataframe >>> df1 = df[['Engine Fuel Type','Transmission Type','Driven_Wheels','Market Category','Vehicle Size', 'Vehicle Style', 'Maker_Model']] # For each row, combine all the columns into one column >>> df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1) # Store them in a pandas dataframe >>> df_clean = pd.DataFrame({'clean': df2}) # Create the list of list format of the custom corpus for gensim modeling >>> sent = [row.split(',') for row in df_clean['clean']] # show the example of list of list format of the custom corpus for gensim modeling >>> sent[:2] [['premium unleaded (required)', 'MANUAL', 'rear wheel drive', 'Factory Tuner', 'Luxury', 'High-Performance', 'Compact', 'Coupe', 'BMW 1 Series M'], ['premium unleaded (required)', 'MANUAL', 'rear wheel drive', 'Luxury', 'Performance', 'Compact', 'Convertible', 'BMW 1 Series']] 3.2. Genism word2vec Model Training We can train the genism word2vec model with our own custom corpus as following: >>> model = Word2Vec(sent, min_count=1,size= 50,workers=3, window =3, sg = 1) Let’s try to understand the hyperparameters of this model. size: The number of dimensions of the embeddings and the default is 100. window: The maximum distance between a target word and words around the target word. The default window is 5. min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5. workers: The number of partitions during training and the default workers is 3. sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW. After training the word2vec model, we can obtain the word embedding directly from the training model as following. >>> model['Toyota Camry'] array([-0.11884457, 0.03035539, -0.0248678 , -0.06297892, -0.01703234, -0.03832747, -0.0825972 , -0.00268112, -0.09192555, -0.08458661, -0.07199778, 0.05235871, 0.21303181, 0.15767808, -0.1883737 , 0.01938575, -0.24431638, 0.04261152, 0.11865819, 0.09881561, -0.04580643, -0.08342388, -0.01355413, -0.07892415, -0.08467747, -0.0040625 , 0.16796461, 0.14578669, 0.04187112, -0.01436194, -0.25554284, 0.25494182, 0.05522631, 0.19295982, 0.14461821, 0.14022525, -0.2065216 , -0.05020927, -0.08133671, 0.18031682, 0.35042757, 0.0245426 , 0.15938364, -0.05617865, 0.00297452, 0.15442047, -0.01286271, 0.13923576, 0.085941 , 0.18811756], dtype=float32) 4. Compute Similarities Now we could even use Word2vec to compute the similarity between two Make Models in the vocabulary by invoking the model.similarity( ) and passing in the relevant words. For instance, model.similarity(‘Porsche 718 Cayman’, ‘Nissan Van’) This will give us the Euclidian similarity between Porsche 718 Cayman and Nissan Van. >>> model.similarity('Porsche 718 Cayman', 'Nissan Van') 0.822824584626184 >>> model.similarity('Porsche 718 Cayman', 'Mercedes-Benz SLK-Class') 0.961089779453727 From the above examples, we can tell that Porsche 718 Cayman is more similar to Mercedes-Benz SLK-Class than Nissan Van. We also can use the built-in function model.most_similar() to get a set of the most similar make models for a given make model based on the Euclidean distance. >>> model1.most_similar('Mercedes-Benz SLK-Class')[:5] [('BMW M4', 0.9959905743598938), ('Maserati Coupe', 0.9949707984924316), ('Porsche Cayman', 0.9945154190063477), ('Mercedes-Benz SLS AMG GT', 0.9944609999656677), ('Maserati Spyder', 0.9942780137062073)] However, Euclidian similarity cannot work well for the high-dimensional word vectors. This is because Euclidian similarity will increase as the number of dimensions increases, even if the word embedding stands for different meanings. Alternatively, we can use cosine similarity to measure the similarity between two vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at a 0-degree angle. The following function shows how can we generate the most similar make model based on cosine similarity. def cosine_distance (model, word,target_list , num) : cosine_dict ={} word_list = [] a = model[word] for item in target_list : if item != word : b = model [item] cos_sim = dot(a, b)/(norm(a)*norm(b)) cosine_dict[item] = cos_sim dist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order for item in dist_sort: word_list.append((item[0], item[1])) return word_list[0:num] # only get the unique Maker_Model >>> Maker_Model = list(df.Maker_Model.unique()) # Show the most similar Mercedes-Benz SLK-Class by cosine distance >>> cosine_distance (model,'Mercedes-Benz SLK-Class',Maker_Model,5) [('Mercedes-Benz CLK-Class', 0.99737006), ('Aston Martin DB9', 0.99593246), ('Maserati Spyder', 0.99571854), ('Ferrari 458 Italia', 0.9952333), ('Maserati GranTurismo Convertible', 0.994994)] 5. T-SNE Visualizations It’s hard to visualize the word embedding directly, for they usually have more than 3 dimensions. T-SNE is a useful tool to visualize high-dimensional data by dimension reduction while keeping relative pairwise distance between points. It can be said that T-SNE looking for a new data representation where the neighborhood relations are preserved. The following code shows how to plot the word embedding with T-SNE plot. def display_closestwords_tsnescatterplot(model, word, size): arr = np.empty((0,size), dtype='f') word_labels = [word] close_words = model.similar_by_word(word) arr = np.append(arr, np.array([model[word]]), axis=0) for wrd_score in close_words: wrd_vector = model[wrd_score[0]] word_labels.append(wrd_score[0]) arr = np.append(arr, np.array([wrd_vector]), axis=0) tsne = TSNE(n_components=2, random_state=0) np.set_printoptions(suppress=True) Y = tsne.fit_transform(arr) x_coords = Y[:, 0] y_coords = Y[:, 1] plt.scatter(x_coords, y_coords) for label, x, y in zip(word_labels, x_coords, y_coords): plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points') plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005) plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005) plt.show() >>> display_closestwords_tsnescatterplot(model, 'Porsche 718 Cayman', 50) This T-SNE plot shows the top 10 similar vehicles to the Porsche 718 Cayman in two-dimensional space. About Me I am a master student in Data Science at the University of San Francisco. I am passionate about using Machine Learning to solve business challenges. You can also find me through Linkedin.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In my last article, I introduced the concept of Graph Neural Network (GNN) and some recent advancements of it. Since this topic is getting seriously hyped up, I decided to make this tutorial on how to easily implement your Graph Neural Network in your project. You will learn how to construct your own GNN with PyTorch Geometric, and how to use GNN to solve a real-world problem (Recsys Challenge 2015). In this blog post, we will be using PyTorch and PyTorch Geometric (PyG), a Graph Neural Network framework built on top of PyTorch that runs blazingly fast. It is several times faster than the most well-known GNN framework, DGL. Aside from its remarkable speed, PyG comes with a collection of well-implemented GNN models illustrated in various papers. Therefore, it would be very handy to reproduce the experiments with PyG.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Introduction Thanks to its strict implementation of the grammar of graphics, ggplot2 provides an extremely intuitive and consistent way of plotting your data. Not only does ggplot2’s approach to plotting ensure that each plot comprises certain basic elements but it also simplifies the readability of your code to a great extent. However, if you are a frequent user of Python, then implementing the grammar of graphics can be extremely challenging due to the lack of standardized syntax in popular plotting libraries, such as matplotlib or seaborn. Should you still want to make use of the grammar of graphics, then the Python package plotnine provides an alternative for you. The Grammar of Graphics In case you should be unfamiliar with the grammar of graphics, here is a quick overview: Main Components of the Grammar of Graphics As you can see, there are several components that make up the grammar of graphics, starting with your data. After identifying the data you would like to visualize, you have to specify the variables you are interested in. For instance, you might want to display one variable on the x-axis and another on the y-axis. Third, you have to define what type of geometric object (geom for short) you would like to utilize. This could be anything from a bar plot to a scatter plot or any of the other existing plot types. These first three components are compulsory. Without data, there is nothing to plot. Without axis definitions, there is nothing to plot either. And finally, without defining a geometric object, you will only see an empty coordinate system. The remaining components making up the grammar of graphics are optional and can be implemented to improve visualizations. Facets refer to specifications of subplots, that is, plotting several variables within your data next to one another in separate plots. Statistical transformations mainly refer to the inclusion of summary statistics in your plot, such as the median or percentiles. Coordinates describe the different coordinate systems available to you. The most used and default coordinate system is the Cartesian coordinate system. Depending on the structure of the data you would like to plot, lesser used coordinate systems, such as the Polar coordinate system, might provide a better way of visualizing your data. Finally, themes provide a variety of options to design all non-data elements of your plot, such as the legend, background, or annotations. While there are many ways of visualizing the grammar of graphics, I particularly like the one I created above because it implies the additivity of these layers as well as the fact that they are building upon one another. If you have ever used ggplot2, you are familiar with the ‘+’ in its syntax that symbolizes the same idea described above. plotnine plotnine is a Python package allowing you to use ggplot2-like code that is implementing the grammar of graphics. By doing so, just as in ggplot2, you are able to specifically map data to visual objects that make up the visualization. This enables you to improve both the readability as well as the structure of your code. While you could set matplotlib’s style to ggplot, you cannot implement the grammar of graphics in matplotlib the same way you can in ggplot2. Installation Before getting started, you have to install plotnine. As always, there are two main options for doing so: pip and conda. Plotting Having installed plotnine, you can get started plotting using the grammar of graphics. Let us begin by building a very simple plot only using the three requisite components: data, aesthetics, and geometric objects. Building a plot using the grammar of graphics As you can see, the syntax is very similar to ggplot2. First, we specify the data source. In our case, the data we are using is the classic mpg data set. Next, we define that the variable ‘class’ is going to be displayed on the x-axis. Lastly, we say that we would like to use a bar plot with bars of size 20 to visualize our data. Let us look at the complete code and the resulting plot: The code above will yield the following output: While this is a good start, it is not very nice to look at yet. Let us use other components of the grammar of graphics to beautify our plot. For instance, we could flip the axes using coord_flip() and customize the plot and axes titles with labs() to improve our plot. Using the code chunk above, our plot would look like this: Plotting Multidimensional Data Besides basic plots, you can do almost everything you could otherwise do in ggplot2, such as plotting multidimensional data. If you would like to visualize the relationships between three variables you could add aesthetics to an otherwise two-dimensional plot: Adding color to the aesthetics will prompt plotnine to display a two-dimensional plot using displ (engine displacement, in liters) on its x- and hwy (highway miles per gallon) on its y-axis and color the data according to the variable class. We have also switched the geometric object to geom_point(), which will give us a scatter instead of a bar plot. Let us take a look at what that would look like: Conclusion As you can see, plotnine provides you with the ability to utilize the grammar of graphics within Python. This increases the readability of your code and allows you to specifically map parts of your data to visual objects. If you are already familiar with ggplot2, then you won’t have to learn anything new to master plotnine. If not, here is a link to the ggplot2 website on which you can find out plenty more about the grammar of graphics and all types of geometric objects available to you.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Photo credit to Mika Baumeister from Unsplash When I work on Python projects dealing with large datasets, I usually use Spyder. The environment of Spyder is very simple; I can browse through working directories, maintain large code bases and review data frames I create. However, if I don’t subset the large data, I constantly face memory issues and struggle with very long computational time. For this reason, I occasionally use Databricks. Databricks is a Microsoft Azure platform where you can easily parse large amounts of data into “notebooks” and perform Apache Spark-based analytics. If you want to work with data frames and run models using pyspark, you can easily refer to Databricks’ website for more information. However, while working on Databricks, I noticed that saving files in CSV, which is supposed to be quite easy, is not very straightforward. In the following section, I would like to share how you can save data frames from Databricks into CSV format on your local computer with no hassles. 1. Explore the Databricks File System (DBFS) From Azure Databricks home, you can go to “Upload Data” (under Common Tasks)→ “DBFS” → “FileStore”. DBFS FileStore is where you create folders and save your data frames into CSV format. By default, FileStore has three folders: import-stage, plots, and tables. 2. Save a data frame into CSV in FileStore Sample.coalesce(1).write.format(“com.databricks.spark.csv”).option(“header”, “true”).save(“dbfs:/FileStore/df/Sample.csv”) Using the above code on the notebook, I created a folder “df” and saved a data frame “Sample” into CSV. It is important to use coalesce(1) since it saves the data frame as a whole. At the end of this article, I will also demonstrate what happens when you don’t include coalesce(1) in the code.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          A Step-by-Step Implementation of Gradient Descent and Backpropagation The original intention behind this post was merely me brushing upon mathematics in neural network, as I like to be well versed in the inner workings of algorithms and get to the essence of things. I then think I might as well put together a story rather than just revisiting the formulas on my notepad over and over. Though you might find a number of tutorials for building a simple neural network from scratch. Different people have varied angles of seeing things as well as the emphasis of study. Another way of thinking might in some sense enhance understanding. So let’s dive in. Photo from Unsplash Neural network in a nutshell The core of neural network is a big function that maps some input to the desired target value, in the intermediate step does the operation to produce the network, which is by multiplying weights and add bias in a pipeline scenario that does this over and over again. The process of training a neural network is to determine a set of parameters that minimize the difference between expected value and model output. This is done using gradient descent (aka backpropagation), which by definition comprises two steps: calculating gradients of the loss/error function, then updating existing parameters in response to the gradients, which is how the descent is done. This cycle is repeated until reaching the minima of the loss function. This learning process can be described by the simple equation: W(t+1) = W(t) — dJ(W)/dW(t). The mathematical intuition Photo from https://blog.goodaudience.com/artificial-neural-networks-explained-436fcf36e75 For my own practice purpose, I like to use a small network with a single hidden layer as in the diagram. In this layout, X represents input, subscripts i, j, k denote the number of units in the input, hidden and output layers respectively; w_ij represents the weights connecting input to hidden layer, and w_jk is the weights connecting hidden to output layer. The model output calculation, in this case, would be: Often the choice of the loss function is the sum of squared error. Here I use sigmoid activation function and assume bias b is 0 for simplicity, meaning weights are the only variables that affect model output. Let’s derive the formula for calculating gradients of hidden to output weights w_jk. The complexity of determining input to hidden weights is that it affects output error indirectly. Each hidden unit output affects model output, thus input to hidden weights w_ij depend on the errors at all of the units it is connected to. The derivation starts the same, just to expand the chain rule at z_k to the subfunction. More thoughts: Notice that the gradients of the two weights have a similar form. The error is backpropagated via the derivative of activation function, then weighted by the input (the activation output) from the previous layer. In the second formula, the backpropagated error from the output layer is further projected onto w_jk, then repeat the same way of backpropagation and weighted by the input. This backpropagating process is iterated all the way back to the very first layer in an arbitrary-layer neural network. “The gradients with respect to each parameter are thus considered to be the contribution of the parameter to the error and should be negated during learning.” Putting the above process into code: Below is the complete example: import numpy as np class NeuralNetwork: def __init__(self): np.random.seed(10) # for generating the same results self.wij = np.random.rand(3,4) # input to hidden layer weights self.wjk = np.random.rand(4,1) # hidden layer to output weights def sigmoid(self, x, w): z = np.dot(x, w) return 1/(1 + np.exp(-z)) def sigmoid_derivative(self, x, w): return self.sigmoid(x, w) * (1 - self.sigmoid(x, w)) def gradient_descent(self, x, y, iterations): for i in range(iterations): Xi = x Xj = self.sigmoid(Xi, self.wij) yhat = self.sigmoid(Xj, self.wjk) # gradients for hidden to output weights g_wjk = np.dot(Xj.T, (y - yhat) * self.sigmoid_derivative(Xj, self.wjk)) # gradients for input to hidden weights g_wij = np.dot(Xi.T, np.dot((y - yhat) * self.sigmoid_derivative(Xj, self.wjk), self.wjk.T) * self.sigmoid_derivative(Xi, self.wij)) # update weights self.wij += g_wij self.wjk += g_wjk print('The final prediction from neural network are: ') print(yhat) if __name__ == '__main__': neural_network = NeuralNetwork() print('Random starting input to hidden weights: ') print(neural_network.wij) print('Random starting hidden to output weights: ') print(neural_network.wjk) X = np.array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]]) y = np.array([[0, 1, 1, 0]]).T neural_network.gradient_descent(X, y, 10000) References:   \n",
       "\n",
       "   text_words_num  text_length  text_sentences_num  token_count  \n",
       "0            1489        10432                  64      2608.00  \n",
       "1             139          827                   7       206.75  \n",
       "2             953         5632                  45      1408.00  \n",
       "3             280         1776                  16       444.00  \n",
       "4             737         4744                  28      1186.00  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>text_words_num</th>\n",
       "      <th>text_length</th>\n",
       "      <th>text_sentences_num</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model</td>\n",
       "      <td>1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit. For more details about the word2vec algorithm, please check here. 2. Gensim Python Library Introduction Gensim is an open source python library for natural language processing and it was developed and is maintained by the Czech natural language processing researcher Radim Řehůřek. Gensim library will enable us to develop word embeddings by training our own word2vec models on a custom corpus either with CBOW of skip-grams algorithms. At first, we need to install the genism package. Gensim runs on Linux, Windows and Mac OS X, and should run on any other platform that supports Python 2.7+ and NumPy. Gensim depends on the following software: Python &gt;= 2.7 (tested with versions 2.7, 3.5 and 3.6) &gt;= 2.7 (tested with versions 2.7, 3.5 and 3.6) NumPy &gt;= 1.11.3 &gt;= 1.11.3 SciPy &gt;= 0.18.1 &gt;= 0.18.1 Six &gt;= 1.5.0 &gt;= 1.5.0 smart_open &gt;= 1.2.1 There are two ways for installation. We could run the following code in our terminal to install genism package. pip install --upgrade gensim Or, alternatively for Conda environments: conda install -c conda-forge gensim 3. Implementation of word Embedding with Gensim Word2Vec Model In this tutorial, I will show how to generate word embedding with genism using a concrete example. The dataset I used for this tutorial is from Kaggle Dataset. This vehicle dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The full python tutorial can be found here. &gt;&gt;&gt; df = pd.read_csv('data.csv') &gt;&gt;&gt; df.head() 3.1 Data Preprocessing: Since the purpose of this tutorial is to learn how to generate word embeddings using genism library, we will not do the EDA and feature selection for the word2vec model for the sake of simplicity. Genism word2vec requires that a format of ‘list of lists’ for training where every document is contained in a list and every list contains lists of tokens of that document. At first, we need to generate a format of ‘list of lists’ for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains lists of features of that make model. To achieve this, we need to do the following things : a. Create a new column for Make Model &gt;&gt;&gt; df['Maker_Model']= df['Make']+ \" \" + df['Model'] b. Generate a format of ‘ list of lists’ for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size, Vehicle Style. # Select features from original dataset to form a new dataframe &gt;&gt;&gt; df1 = df[['Engine Fuel Type','Transmission Type','Driven_Wheels','Market Category','Vehicle Size', 'Vehicle Style', 'Maker_Model']] # For each row, combine all the columns into one column &gt;&gt;&gt; df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1) # Store them in a pandas dataframe &gt;&gt;&gt; df_clean = pd.DataFrame({'clean': df2}) # Create the list of list format of the custom corpus for gensim modeling &gt;&gt;&gt; sent = [row.split(',') for row in df_clean['clean']] # show the example of list of list format of the custom corpus for gensim modeling &gt;&gt;&gt; sent[:2] [['premium unleaded (required)', 'MANUAL', 'rear wheel drive', 'Factory Tuner', 'Luxury', 'High-Performance', 'Compact', 'Coupe', 'BMW 1 Series M'], ['premium unleaded (required)', 'MANUAL', 'rear wheel drive', 'Luxury', 'Performance', 'Compact', 'Convertible', 'BMW 1 Series']] 3.2. Genism word2vec Model Training We can train the genism word2vec model with our own custom corpus as following: &gt;&gt;&gt; model = Word2Vec(sent, min_count=1,size= 50,workers=3, window =3, sg = 1) Let’s try to understand the hyperparameters of this model. size: The number of dimensions of the embeddings and the default is 100. window: The maximum distance between a target word and words around the target word. The default window is 5. min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5. workers: The number of partitions during training and the default workers is 3. sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW. After training the word2vec model, we can obtain the word embedding directly from the training model as following. &gt;&gt;&gt; model['Toyota Camry'] array([-0.11884457, 0.03035539, -0.0248678 , -0.06297892, -0.01703234, -0.03832747, -0.0825972 , -0.00268112, -0.09192555, -0.08458661, -0.07199778, 0.05235871, 0.21303181, 0.15767808, -0.1883737 , 0.01938575, -0.24431638, 0.04261152, 0.11865819, 0.09881561, -0.04580643, -0.08342388, -0.01355413, -0.07892415, -0.08467747, -0.0040625 , 0.16796461, 0.14578669, 0.04187112, -0.01436194, -0.25554284, 0.25494182, 0.05522631, 0.19295982, 0.14461821, 0.14022525, -0.2065216 , -0.05020927, -0.08133671, 0.18031682, 0.35042757, 0.0245426 , 0.15938364, -0.05617865, 0.00297452, 0.15442047, -0.01286271, 0.13923576, 0.085941 , 0.18811756], dtype=float32) 4. Compute Similarities Now we could even use Word2vec to compute the similarity between two Make Models in the vocabulary by invoking the model.similarity( ) and passing in the relevant words. For instance, model.similarity(‘Porsche 718 Cayman’, ‘Nissan Van’) This will give us the Euclidian similarity between Porsche 718 Cayman and Nissan Van. &gt;&gt;&gt; model.similarity('Porsche 718 Cayman', 'Nissan Van') 0.822824584626184 &gt;&gt;&gt; model.similarity('Porsche 718 Cayman', 'Mercedes-Benz SLK-Class') 0.961089779453727 From the above examples, we can tell that Porsche 718 Cayman is more similar to Mercedes-Benz SLK-Class than Nissan Van. We also can use the built-in function model.most_similar() to get a set of the most similar make models for a given make model based on the Euclidean distance. &gt;&gt;&gt; model1.most_similar('Mercedes-Benz SLK-Class')[:5] [('BMW M4', 0.9959905743598938), ('Maserati Coupe', 0.9949707984924316), ('Porsche Cayman', 0.9945154190063477), ('Mercedes-Benz SLS AMG GT', 0.9944609999656677), ('Maserati Spyder', 0.9942780137062073)] However, Euclidian similarity cannot work well for the high-dimensional word vectors. This is because Euclidian similarity will increase as the number of dimensions increases, even if the word embedding stands for different meanings. Alternatively, we can use cosine similarity to measure the similarity between two vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at a 0-degree angle. The following function shows how can we generate the most similar make model based on cosine similarity. def cosine_distance (model, word,target_list , num) : cosine_dict ={} word_list = [] a = model[word] for item in target_list : if item != word : b = model [item] cos_sim = dot(a, b)/(norm(a)*norm(b)) cosine_dict[item] = cos_sim dist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order for item in dist_sort: word_list.append((item[0], item[1])) return word_list[0:num] # only get the unique Maker_Model &gt;&gt;&gt; Maker_Model = list(df.Maker_Model.unique()) # Show the most similar Mercedes-Benz SLK-Class by cosine distance &gt;&gt;&gt; cosine_distance (model,'Mercedes-Benz SLK-Class',Maker_Model,5) [('Mercedes-Benz CLK-Class', 0.99737006), ('Aston Martin DB9', 0.99593246), ('Maserati Spyder', 0.99571854), ('Ferrari 458 Italia', 0.9952333), ('Maserati GranTurismo Convertible', 0.994994)] 5. T-SNE Visualizations It’s hard to visualize the word embedding directly, for they usually have more than 3 dimensions. T-SNE is a useful tool to visualize high-dimensional data by dimension reduction while keeping relative pairwise distance between points. It can be said that T-SNE looking for a new data representation where the neighborhood relations are preserved. The following code shows how to plot the word embedding with T-SNE plot. def display_closestwords_tsnescatterplot(model, word, size): arr = np.empty((0,size), dtype='f') word_labels = [word] close_words = model.similar_by_word(word) arr = np.append(arr, np.array([model[word]]), axis=0) for wrd_score in close_words: wrd_vector = model[wrd_score[0]] word_labels.append(wrd_score[0]) arr = np.append(arr, np.array([wrd_vector]), axis=0) tsne = TSNE(n_components=2, random_state=0) np.set_printoptions(suppress=True) Y = tsne.fit_transform(arr) x_coords = Y[:, 0] y_coords = Y[:, 1] plt.scatter(x_coords, y_coords) for label, x, y in zip(word_labels, x_coords, y_coords): plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points') plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005) plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005) plt.show() &gt;&gt;&gt; display_closestwords_tsnescatterplot(model, 'Porsche 718 Cayman', 50) This T-SNE plot shows the top 10 similar vehicles to the Porsche 718 Cayman in two-dimensional space. About Me I am a master student in Data Science at the University of San Francisco. I am passionate about using Machine Learning to solve business challenges. You can also find me through Linkedin.</td>\n",
       "      <td>1489</td>\n",
       "      <td>10432</td>\n",
       "      <td>64</td>\n",
       "      <td>2608.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hands-on Graph Neural Networks with PyTorch &amp; PyTorch Geometric</td>\n",
       "      <td>In my last article, I introduced the concept of Graph Neural Network (GNN) and some recent advancements of it. Since this topic is getting seriously hyped up, I decided to make this tutorial on how to easily implement your Graph Neural Network in your project. You will learn how to construct your own GNN with PyTorch Geometric, and how to use GNN to solve a real-world problem (Recsys Challenge 2015). In this blog post, we will be using PyTorch and PyTorch Geometric (PyG), a Graph Neural Network framework built on top of PyTorch that runs blazingly fast. It is several times faster than the most well-known GNN framework, DGL. Aside from its remarkable speed, PyG comes with a collection of well-implemented GNN models illustrated in various papers. Therefore, it would be very handy to reproduce the experiments with PyG.</td>\n",
       "      <td>139</td>\n",
       "      <td>827</td>\n",
       "      <td>7</td>\n",
       "      <td>206.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to Use ggplot2 in Python</td>\n",
       "      <td>Introduction Thanks to its strict implementation of the grammar of graphics, ggplot2 provides an extremely intuitive and consistent way of plotting your data. Not only does ggplot2’s approach to plotting ensure that each plot comprises certain basic elements but it also simplifies the readability of your code to a great extent. However, if you are a frequent user of Python, then implementing the grammar of graphics can be extremely challenging due to the lack of standardized syntax in popular plotting libraries, such as matplotlib or seaborn. Should you still want to make use of the grammar of graphics, then the Python package plotnine provides an alternative for you. The Grammar of Graphics In case you should be unfamiliar with the grammar of graphics, here is a quick overview: Main Components of the Grammar of Graphics As you can see, there are several components that make up the grammar of graphics, starting with your data. After identifying the data you would like to visualize, you have to specify the variables you are interested in. For instance, you might want to display one variable on the x-axis and another on the y-axis. Third, you have to define what type of geometric object (geom for short) you would like to utilize. This could be anything from a bar plot to a scatter plot or any of the other existing plot types. These first three components are compulsory. Without data, there is nothing to plot. Without axis definitions, there is nothing to plot either. And finally, without defining a geometric object, you will only see an empty coordinate system. The remaining components making up the grammar of graphics are optional and can be implemented to improve visualizations. Facets refer to specifications of subplots, that is, plotting several variables within your data next to one another in separate plots. Statistical transformations mainly refer to the inclusion of summary statistics in your plot, such as the median or percentiles. Coordinates describe the different coordinate systems available to you. The most used and default coordinate system is the Cartesian coordinate system. Depending on the structure of the data you would like to plot, lesser used coordinate systems, such as the Polar coordinate system, might provide a better way of visualizing your data. Finally, themes provide a variety of options to design all non-data elements of your plot, such as the legend, background, or annotations. While there are many ways of visualizing the grammar of graphics, I particularly like the one I created above because it implies the additivity of these layers as well as the fact that they are building upon one another. If you have ever used ggplot2, you are familiar with the ‘+’ in its syntax that symbolizes the same idea described above. plotnine plotnine is a Python package allowing you to use ggplot2-like code that is implementing the grammar of graphics. By doing so, just as in ggplot2, you are able to specifically map data to visual objects that make up the visualization. This enables you to improve both the readability as well as the structure of your code. While you could set matplotlib’s style to ggplot, you cannot implement the grammar of graphics in matplotlib the same way you can in ggplot2. Installation Before getting started, you have to install plotnine. As always, there are two main options for doing so: pip and conda. Plotting Having installed plotnine, you can get started plotting using the grammar of graphics. Let us begin by building a very simple plot only using the three requisite components: data, aesthetics, and geometric objects. Building a plot using the grammar of graphics As you can see, the syntax is very similar to ggplot2. First, we specify the data source. In our case, the data we are using is the classic mpg data set. Next, we define that the variable ‘class’ is going to be displayed on the x-axis. Lastly, we say that we would like to use a bar plot with bars of size 20 to visualize our data. Let us look at the complete code and the resulting plot: The code above will yield the following output: While this is a good start, it is not very nice to look at yet. Let us use other components of the grammar of graphics to beautify our plot. For instance, we could flip the axes using coord_flip() and customize the plot and axes titles with labs() to improve our plot. Using the code chunk above, our plot would look like this: Plotting Multidimensional Data Besides basic plots, you can do almost everything you could otherwise do in ggplot2, such as plotting multidimensional data. If you would like to visualize the relationships between three variables you could add aesthetics to an otherwise two-dimensional plot: Adding color to the aesthetics will prompt plotnine to display a two-dimensional plot using displ (engine displacement, in liters) on its x- and hwy (highway miles per gallon) on its y-axis and color the data according to the variable class. We have also switched the geometric object to geom_point(), which will give us a scatter instead of a bar plot. Let us take a look at what that would look like: Conclusion As you can see, plotnine provides you with the ability to utilize the grammar of graphics within Python. This increases the readability of your code and allows you to specifically map parts of your data to visual objects. If you are already familiar with ggplot2, then you won’t have to learn anything new to master plotnine. If not, here is a link to the ggplot2 website on which you can find out plenty more about the grammar of graphics and all types of geometric objects available to you.</td>\n",
       "      <td>953</td>\n",
       "      <td>5632</td>\n",
       "      <td>45</td>\n",
       "      <td>1408.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Databricks: How to Save Data Frames as CSV Files on Your Local Computer</td>\n",
       "      <td>Photo credit to Mika Baumeister from Unsplash When I work on Python projects dealing with large datasets, I usually use Spyder. The environment of Spyder is very simple; I can browse through working directories, maintain large code bases and review data frames I create. However, if I don’t subset the large data, I constantly face memory issues and struggle with very long computational time. For this reason, I occasionally use Databricks. Databricks is a Microsoft Azure platform where you can easily parse large amounts of data into “notebooks” and perform Apache Spark-based analytics. If you want to work with data frames and run models using pyspark, you can easily refer to Databricks’ website for more information. However, while working on Databricks, I noticed that saving files in CSV, which is supposed to be quite easy, is not very straightforward. In the following section, I would like to share how you can save data frames from Databricks into CSV format on your local computer with no hassles. 1. Explore the Databricks File System (DBFS) From Azure Databricks home, you can go to “Upload Data” (under Common Tasks)→ “DBFS” → “FileStore”. DBFS FileStore is where you create folders and save your data frames into CSV format. By default, FileStore has three folders: import-stage, plots, and tables. 2. Save a data frame into CSV in FileStore Sample.coalesce(1).write.format(“com.databricks.spark.csv”).option(“header”, “true”).save(“dbfs:/FileStore/df/Sample.csv”) Using the above code on the notebook, I created a folder “df” and saved a data frame “Sample” into CSV. It is important to use coalesce(1) since it saves the data frame as a whole. At the end of this article, I will also demonstrate what happens when you don’t include coalesce(1) in the code.</td>\n",
       "      <td>280</td>\n",
       "      <td>1776</td>\n",
       "      <td>16</td>\n",
       "      <td>444.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Step-by-Step Implementation of Gradient Descent and Backpropagation</td>\n",
       "      <td>A Step-by-Step Implementation of Gradient Descent and Backpropagation The original intention behind this post was merely me brushing upon mathematics in neural network, as I like to be well versed in the inner workings of algorithms and get to the essence of things. I then think I might as well put together a story rather than just revisiting the formulas on my notepad over and over. Though you might find a number of tutorials for building a simple neural network from scratch. Different people have varied angles of seeing things as well as the emphasis of study. Another way of thinking might in some sense enhance understanding. So let’s dive in. Photo from Unsplash Neural network in a nutshell The core of neural network is a big function that maps some input to the desired target value, in the intermediate step does the operation to produce the network, which is by multiplying weights and add bias in a pipeline scenario that does this over and over again. The process of training a neural network is to determine a set of parameters that minimize the difference between expected value and model output. This is done using gradient descent (aka backpropagation), which by definition comprises two steps: calculating gradients of the loss/error function, then updating existing parameters in response to the gradients, which is how the descent is done. This cycle is repeated until reaching the minima of the loss function. This learning process can be described by the simple equation: W(t+1) = W(t) — dJ(W)/dW(t). The mathematical intuition Photo from https://blog.goodaudience.com/artificial-neural-networks-explained-436fcf36e75 For my own practice purpose, I like to use a small network with a single hidden layer as in the diagram. In this layout, X represents input, subscripts i, j, k denote the number of units in the input, hidden and output layers respectively; w_ij represents the weights connecting input to hidden layer, and w_jk is the weights connecting hidden to output layer. The model output calculation, in this case, would be: Often the choice of the loss function is the sum of squared error. Here I use sigmoid activation function and assume bias b is 0 for simplicity, meaning weights are the only variables that affect model output. Let’s derive the formula for calculating gradients of hidden to output weights w_jk. The complexity of determining input to hidden weights is that it affects output error indirectly. Each hidden unit output affects model output, thus input to hidden weights w_ij depend on the errors at all of the units it is connected to. The derivation starts the same, just to expand the chain rule at z_k to the subfunction. More thoughts: Notice that the gradients of the two weights have a similar form. The error is backpropagated via the derivative of activation function, then weighted by the input (the activation output) from the previous layer. In the second formula, the backpropagated error from the output layer is further projected onto w_jk, then repeat the same way of backpropagation and weighted by the input. This backpropagating process is iterated all the way back to the very first layer in an arbitrary-layer neural network. “The gradients with respect to each parameter are thus considered to be the contribution of the parameter to the error and should be negated during learning.” Putting the above process into code: Below is the complete example: import numpy as np class NeuralNetwork: def __init__(self): np.random.seed(10) # for generating the same results self.wij = np.random.rand(3,4) # input to hidden layer weights self.wjk = np.random.rand(4,1) # hidden layer to output weights def sigmoid(self, x, w): z = np.dot(x, w) return 1/(1 + np.exp(-z)) def sigmoid_derivative(self, x, w): return self.sigmoid(x, w) * (1 - self.sigmoid(x, w)) def gradient_descent(self, x, y, iterations): for i in range(iterations): Xi = x Xj = self.sigmoid(Xi, self.wij) yhat = self.sigmoid(Xj, self.wjk) # gradients for hidden to output weights g_wjk = np.dot(Xj.T, (y - yhat) * self.sigmoid_derivative(Xj, self.wjk)) # gradients for input to hidden weights g_wij = np.dot(Xi.T, np.dot((y - yhat) * self.sigmoid_derivative(Xj, self.wjk), self.wjk.T) * self.sigmoid_derivative(Xi, self.wij)) # update weights self.wij += g_wij self.wjk += g_wjk print('The final prediction from neural network are: ') print(yhat) if __name__ == '__main__': neural_network = NeuralNetwork() print('Random starting input to hidden weights: ') print(neural_network.wij) print('Random starting hidden to output weights: ') print(neural_network.wjk) X = np.array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]]) y = np.array([[0, 1, 1, 0]]).T neural_network.gradient_descent(X, y, 10000) References:</td>\n",
       "      <td>737</td>\n",
       "      <td>4744</td>\n",
       "      <td>28</td>\n",
       "      <td>1186.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 205
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:09:16.107481Z",
     "start_time": "2024-04-26T20:09:16.099309Z"
    }
   },
   "cell_type": "code",
   "source": "data.describe().round(2)",
   "id": "57b7d0f23dda3224",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       text_words_num  text_length  text_sentences_num  token_count\n",
       "count         1391.00      1391.00             1391.00      1391.00\n",
       "mean           901.54      5530.81               44.46      1382.70\n",
       "std            885.73      5521.48               44.17      1380.37\n",
       "min             49.00       249.00                2.00        62.25\n",
       "25%            315.00      1886.00               15.00       471.50\n",
       "50%            516.00      3040.00               26.00       760.00\n",
       "75%           1227.00      7577.50               60.00      1894.38\n",
       "max           7657.00     46966.00              376.00     11741.50"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_words_num</th>\n",
       "      <th>text_length</th>\n",
       "      <th>text_sentences_num</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1391.00</td>\n",
       "      <td>1391.00</td>\n",
       "      <td>1391.00</td>\n",
       "      <td>1391.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>901.54</td>\n",
       "      <td>5530.81</td>\n",
       "      <td>44.46</td>\n",
       "      <td>1382.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>885.73</td>\n",
       "      <td>5521.48</td>\n",
       "      <td>44.17</td>\n",
       "      <td>1380.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>49.00</td>\n",
       "      <td>249.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>62.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>315.00</td>\n",
       "      <td>1886.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>471.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>516.00</td>\n",
       "      <td>3040.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>760.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1227.00</td>\n",
       "      <td>7577.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>1894.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7657.00</td>\n",
       "      <td>46966.00</td>\n",
       "      <td>376.00</td>\n",
       "      <td>11741.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 206
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Chunking articles into smaller sentences",
   "id": "22d860d188b7736"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:09:16.124494Z",
     "start_time": "2024-04-26T20:09:16.108328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for _, row in data.sample(5).iterrows():\n",
    "    doc = nlp(row[\"Text\"])\n",
    "    sents = [sent.text for sent in doc.sents]\n",
    "    print(sents)\n",
    "    print(\"\\n\\n\")"
   ],
   "id": "92405363daaaef13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Let’s create a micro-drone swarm that utilizes artificial intelligence and mesh networking to mimic swarm behavior that is typically seen in nature.', 'Hold on!', 'Don’t go!', 'Hear me out with this concept.', 'By utilizing these buzzwordy technologies we can create something pretty cool and dare I say, useful?', 'Sure, small drones have terrible flight times (among other things) and yes microprocessors might not be the best solution for AI processing.', 'However, if we keep the scope of the project small then we may be able to pull it off.', 'Something such as mimicking swarm behavior in an indoor environment with multiple quadcopters equipped with an array of sensors.', 'Still think I’m full of it?', 'What if I told you that I already have the first prototype built?', 'Let’s jump into it!', 'How Are The Drones Made?', 'When starting any project that requires hardware the first thing you should do is determine what components you want to use.', 'For this case, I opted to use a PCB for the base of the quadcopter.', 'This will minimize the number of parts required for the project.', 'It’ll also allow me to quickly prototype various designs and make changes without having to make multiple parts.', 'The above base of the quadcopter measures 100mm by 100mm.', 'It is printed on a two-layer PCB, and includes everything needed to make the quadcopter fly!', 'Time to decide on the flight controller, inertial measurement unit, and how the motors will be controlled.', 'For my concept drone, I chose the ESP32 as the main flight controller.', 'I chose the ESP32 for its versatility, multiple cores, plenty of PWM pins (pulse width modulation for controlling the speed of the propellers), and mesh network capabilities.', 'The ESP32 can also be optimized for minimal power consumption while still delivering enough power for the given task.']\n",
      "\n",
      "\n",
      "\n",
      "['Deep Learning An Intuitive Explanation to AutoEncoders Photo by Aditya Chinchure on Unsplash Motivation Many of the recent deep learning models rely on extracting complex features from data.', 'The goal is to transform the input from its raw format, to another representation calculated by the neural network.', 'This representation contains features that describe hidden unique characteristics about the input.', 'Consider a dataset of people faces, where each input is an image of a person.', 'The representation of an image in its raw format is too complex to be used by machines.', 'Instead, why not make the neural network automatically calculate important features for each face, something maybe like: eye type, nose type, distance between eyes, nose position, etc.', 'Well, this sounds interesting… Using these features, we could easily compare two faces, find similar faces, generate new faces, and many other interesting applications.', 'This concept is called Encoding since we are generating an encoded version of the data.', 'In this article, we will learn more about encodings, how calculate them using AutoEncoders, and finally how to implement them in Keras.', 'AutoEncoders An AutoEncoder is a strange neural network, because both its input and output are the same.', 'So, it is a network that tries to learn itself!', 'This is crazy I know but you will see why this is useful.', 'Suppose we have the following neural network: An input layer with 100 neurons A Hidden layer with 3 neurons An output layer with 100 neurons (same as the input layer) Image by Author Now what happens if we fit the neural network to take the input and tries to predict the same value in the output?', 'Does not this mean that the network learned how to represent a 100-dimensions-input with only 3-dimensions (number of neurons in the hidden layer), then to reconstruct the same input again?', 'In addition, these 3-dimensions or features seem enough to represent what an input value describes.', 'Well this is very interesting.', 'It is like when compressing files.', 'We reduce the file size, but we can uncompress it again and get the same data.', 'In fact, it is not exactly the same data in AutoEncoders since they are lossy, but you got the point.', 'Objective We will use the famous MNIST digits dataset to demonstrate the idea.', 'The goal is to generate a 2D encoding from a given 28*28 image.', 'So, we are implementing a dimensionality reduction algorithm using AutoEncoders!', 'Cool right?', 'Let us start… Time to Code First, we import the dataset: from keras.datasets import mnist (data, labels), (_, _) = mnist.load_data() Need to reshape and rescale: data = data.reshape(-1, 28*28) / 255.', 'Time to define the network.', 'We need three layers: An input layer with size 28*28 A hidden layer with size 2 An output layer with size 28*28 Image by Author from keras import models, layers input_layer = layers.', 'Input(shape=(28*28,)) encoding_layer = layers.', 'Dense(2)(input_layer) decoding_layer = layers.', 'Dense(28*28) (encoding_layer) autoencoder = models.', \"Model(input_layer, decoding_layer) Let us compile and train… We will fit the model using a binary cross entropy loss between the pixel values: autoencoder.compile('adam', loss='binary_crossentropy') autoencoder.fit(x = data, y = data, epochs=5) Did you notice the trick?\", 'X = data and y = data as well.', 'After fitting the model, the network is supposed to learn how to calculate the hidden encodings.', 'But we still have to extract the layer responsible for this.', 'In the following, we define a new model where we remove the final layer since we do not need it anymore: encoder = models.', 'Model(input_layer, encoding_layer) Now instead of predicting the final output, we are predicting only the hidden representation.', 'Look how we use it: encodings = encoder.predict(data) That is it!', 'Now your encodings variable is an (n, m) array where n is the number of examples and m is the number of dimensions.', 'The first column is the first feature and the second column is the second one.', 'But what are those features?', 'Actually, we do not know.', 'We only know that they are well representatives of each input value.', 'Let us plot them and see what we get.', 'import matplotlib.pyplot as plt plt.scatter(encodings[:, 0], encodings[:, 1], c=labels) plt.colorbar() Image by Author Beautiful!', 'See how the neural network learned the hidden features.', 'Clearly it learned the different characteristics for each digit and how they are distributed in a 2D space.', 'Now we could use these features for visualizations, clustering or any other purpose… Final Thoughts In this article we learned about AutoEncoders and how to apply them for dimensionality reduction.', 'AutoEncoders are very powerful and used in many modern neural network architectures.', 'In future posts, you will learn about more complex Encoder/Decoder networks.', 'If you enjoyed this article, I would appreciate it if you hit the clap button 👏 so it could be spread to others.', 'You can also follow me on Twitter, Facebook, email me directly or find me on LinkedIn.']\n",
      "\n",
      "\n",
      "\n",
      "['Let’s face it, your model is probably still stuck in the stone age.', 'I bet you’re still using 32bit precision or *GASP* perhaps even training only on a single GPU.', '😱.', 'I get it though, there are 99 speed-up guides but a checklist ain’t 1? (', 'yup, that just happened).', 'Well, consider this the ultimate, step-by-step guide to making sure you’re squeezing all the (GP-Use) 😂 out of your model.', 'This guide is structured from simplest to most PITA modifications you can make to get the most out of your network.', 'I’ll show example Pytorch code and the related flags you can use in the Pytorch-Lightning Trainer in case you don’t feel like coding these yourself!', 'Who is this guide for?', 'Anyone working on non-trivial deep learning models in Pytorch such as industrial researchers, Ph.D. students, academics, etc… The models we’re talking about here might be taking you multiple days to train or even weeks or months.', 'We’ll cover (from simplest to most PITA) Using DataLoaders.', 'Number of workers in DataLoader.', 'Batch size.', 'Accumulated Gradients.', 'Retained graphs.', 'Moving to a single GPU.', '16-bit mixed-precision training.', 'Moving to multiple GPUs (model duplication).', 'Moving to multiple GPU-nodes (8+GPUs).', 'My tips for thinking through model speed-ups Pytorch-Lightning You can find every optimization I discuss here in the Pytorch library called Pytorch-Lightning.', 'Lightning is a light wrapper on top of Pytorch that automates training for researchers while giving them full control of the critical model parts.', 'Check out this tutorial for a more robust example.', 'Lightning uses the latest best practices and minimizes the places where you can make a mistake.', 'We’ll define our LightningModel for MNIST here and fit using the Trainer.']\n",
      "\n",
      "\n",
      "\n",
      "['The romantic side of data science: Analyzing a relationship through a year worth of text messages Guy Tsror · Follow Published in Towards Data Science · 11 min read · Apr 1, 2019 -- 1 Share This was supposed to be a private project, one I thought I would upload to a private Google Colab notebook, and share with just whomever it is relevant for (which is, one other person, referred to here as “OJ”).', 'But then, a lightbulb —why not clean it up and share with others as well?', 'Basically, to celebrate our first anniversary of dating, I decided to explore our relationship from the communication point of view, and look at our messaging behaviors: what are we saying to each other, when are we saying it, how do we respond to each other, and so on.', 'I would include call logs in it, but I actually never tracked that (shame on me!)', 'so I had to let it go, especially since we mostly have up to two phone calls a week, making it a bit less interesting.', 'So — I will go through my thought process to those who are interested in what I did and why, and towards the end, I will share some links and tools that came in handy during the process.', 'I am working on improving my Python skills as I go, so you could also find there some Python-related snippets that were handy. (', 'please note: this story is much better read on desktop, since infographics are interactive) Methodology The key communication channel we use is WhatsApp.', 'We do SMS-text occasionally, but it’s negligible, and the occasional Messenger link or YouTube message are very rare, so I chose to stick to WhatsApp to simplify the process.']\n",
      "\n",
      "\n",
      "\n",
      "['How to sample from language models Humans often choose words that surprise language models (Holtzman et al 2019) Causal language models like GPT-2 are trained to predict the probability of the next word given some context.', 'For example, given “I ate a delicious hot ___”, the model may predict “dog” with 80% probability, “pancake” 5% probability, etc.', 'The cool thing about this structure is they can be used to generate sequences of arbitrary length.', 'I can give the model “I ate,” sample a token from the resulting distribution to get “I ate a”, then put that through the model again to get another distribution and resulting token.', 'Repeat as long as we like.', 'It turns out that this generation often either gets stuck in repetitive loops or forgets the subject and goes off topic.', 'Why is this happening, and how might we better sample to generate more human-like text?', 'This post is a summary and exploration of The Curious Case of Neural Text Degeneration by Holtzman et al 2019.', 'I found it one of the most thorough and readable papers I’ve read in recent memory, so please check it out if this post piques your interest!', 'If we always sample the the most likely word, the standard language model training objective causes us to get stuck in loops like “I don’t know.', 'I don’t know.', 'I don’t know.”', 'This is unnatural, but most of the model’s attention in modern language models is only on the most recent few tokens.', 'Instead, popular sampling methods for generation are based on sampling from the distribution.', 'But sampling also runs into a problem: if we have 50K possible choices, even if the bottom 25K tokens are each extremely unlikely, in aggregate they might have for example 30% of the probability mass.', 'This means with each sample, we have a 1 in 3 chance of completely derailing our “train of thought.”', 'Because of the short context mentioned earlier, this will cause an unrecoverable error cascade as each next word depends heavily on this recent wrong word.', 'To combat sampling from the tail, the most popular methods are temperature and top k sampling.', 'Temperature sampling is inspired by statistical thermodynamics, where high temperature means low energy states are more likely encountered.', 'In probability models, logits play the role of energy and we can implement temperature sampling by dividing logits by the temperature before feeding them into softmax and obtaining our sampling probabilities.', 'For example: >>> import torch >>> import torch.nn.functional as F >>> a = torch.tensor([1,2,3,4.])', '>>> F.softmax(a, dim=0) tensor([0.0321, 0.0871, 0.2369, 0.6439]) >>> F.softmax(a/.5, dim=0) tensor([0.0021, 0.0158, 0.1171, 0.8650]) >>> F.softmax(a/1.5, dim=0) tensor([0.0708, 0.1378, 0.2685, 0.5229]) >>> F.softmax(a/1e-6, dim=0) tensor([0.,', '0.,', '0.,', '1.])', 'Or visually Lower temperatures make the model increasingly confident in its top choices, while temperatures greater than 1 decrease confidence.', '0 temperature is equivalent to argmax/max likelihood, while infinite temperature corresponds to a uniform sampling.', 'Top k sampling means sorting by probability and zero-ing out the probabilities for anything below the k’th token.', 'It appears to improve quality by removing the tail and making it less likely to go off topic.', 'But in some cases, there really are many words we could sample from reasonably (broad distribution below), and in some cases there aren’t (narrow distribution below).', 'Holtzman et al 2019 To address this problem, the authors propose top p sampling, aka nucleus sampling, in which we compute the cumulative distribution and cut off as soon as the CDF exceeds P. In the broad distribution example above, it may take the top 100 tokens to exceed top_p = .9.', 'In the narrow distribution, we may already exceed top_p = .9 with just “hot” and “warm” in our sample distribution.', 'In this way, we still avoid sampling egregiously wrong tokens, but preserve variety when the highest scoring tokens have low confidence.', 'Why doesn’t maximum likelihood sampling work?', 'In the training process, there’s never a chance to see compounding errors.', 'The model is trained to predict the next token based on a human-generated context.', 'If it gets one token wrong by generating a bad distribution, the next token uses the “correct” human generated context independent of the last prediction.', 'During generation it is forced to complete its own automatically-generated context, a setting it has not considered during training.', 'Qualitative results Here are samples using top_k=40 and context “I ate a delicious” And here are samples using top_p=0.9 and same “I ate a delicious” context: Try it yourself here!', 'You can enable GPU in Runtime > Change runtime type and get big batches for no additional runtime.', 'Beyond the paper: choosing p and k automatically I found it challenging to determine which of these samples is more human-like.', 'For this reason I designed an experiment to determine top_k and top_p empirically.', 'Our goal is to use top_k and top_p to maximize the probability of choosing the actual next word we’ve held out.', 'When searching for the optimal k and p values, it’s actually easy to determine analytically for a given sample.', 'For k, we find the sorted index where the “golden” token occurred.', 'For p, we find the CDF of the golden token.', 'For example, if the context is “I ate a delicious hot” and the actual word is “dog”, but the model’s predicted distribution had “pancake” as most likely, we’d search down the probabilities until we found “dog” at index 3.', 'At index 1, the CDF might be 62%.', 'At index 3, the CDF might be something like 86%, so we’ll record that as our optimal p. Across many examples, we can compute a histogram of optimal p and k values and compute summary statistics on them.', 'I tested on a random section of Wikipedia with a context length of 15.', 'This is much shorter than what the model was trained on (1024), but common for settings like https://duet.li or chat bots.', '===== ks ===== max 29094.00 mean 233.69 median 3.00 len 13376.00 ===== ps ===== max 1.00 mean 0.59 median 0.60 len 13376.00 Feel free to try it yourself in my colab notebook.', 'If the model were being evaluated on its training set, it would be optimal to choose top_k = 1.', 'But since the model is slightly out of domain, the most likely token sometimes appears further down the list.', 'In addition, we have a 50K token vocabulary.', 'In many datasets, we’ll never see all the tokens, but the model isn’t sure of that.', 'By zero-ing out much of the probability mass using top_p or top_k, we incorporate our prior to never choose these never-seen-even-in-training tokens.', 'That said, this search for k and p is still in the context of the model’s view of the world and as such it’s only a bandaid.', 'What we really want is to fix training.', 'Fixing training I’ve also started to think about changing the training objective to better match the generation task.', 'For example, could we train some kind of discriminator to punish the model when it generates whole sequences that don’t look human?', 'It’s not straightforward how to apply a GAN architecture to non-continuous domains.', 'I came upon Adversarial Text Generation without Reinforcement Learning and an RL-based idea, but it seems these have not yet become mainstream.', 'I think it’d be interesting to apply these ideas to the big transformers that have swept state of the art in the last few months.']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 207
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:09:16.127741Z",
     "start_time": "2024-04-26T20:09:16.125160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_sentences_in_chunk = 10\n",
    "CONTEXT_WINDOW = 384\n",
    "\n",
    "\n",
    "def split_text_into_chunks(text: str, n_sentences_in_chunk: int, overlap=0) -> list:\n",
    "    doc = nlp(text)\n",
    "    sents = [sent.text for sent in doc.sents]\n",
    "    chunks = []\n",
    "    n_sentences_in_chunk = n_sentences_in_chunk - overlap\n",
    "    remainder = 0\n",
    "    for i in range(overlap, len(sents), n_sentences_in_chunk):\n",
    "        if remainder == n_sentences_in_chunk:\n",
    "            remainder = 0\n",
    "        chunk = sents[i-overlap-remainder:i + n_sentences_in_chunk-remainder]\n",
    "        remainder = 0\n",
    "        while len(\" \".join(chunk)) / 4 > CONTEXT_WINDOW:\n",
    "            remainder += 1\n",
    "            chunk = chunk[:-remainder]\n",
    "            \n",
    "        chunk = \" \".join(chunk)\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ],
   "id": "977a964a77580255",
   "outputs": [],
   "execution_count": 208
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:09:19.595388Z",
     "start_time": "2024-04-26T20:09:16.128409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dict_data = data.to_dict(\"records\")  \n",
    "\n",
    "for elem in tqdm(dict_data):\n",
    "    elem[\"chunks\"] = split_text_into_chunks(elem[\"Text\"], n_sentences_in_chunk, overlap=1)"
   ],
   "id": "c6a107f76d07d0f9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1391/1391 [00:03<00:00, 402.28it/s]\n"
     ]
    }
   ],
   "execution_count": 209
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Example of chunking",
   "id": "2b4f31665d15786b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:09:19.599330Z",
     "start_time": "2024-04-26T20:09:19.597538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for chunk in dict_data[0][\"chunks\"]:\n",
    "    print(len(chunk) / 4)\n",
    "    print(chunk)\n",
    "    print(\"\\n\\n\")"
   ],
   "id": "8434fbb942475dae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285.0\n",
      "1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.\n",
      "\n",
      "\n",
      "\n",
      "293.5\n",
      "For instance, it will have two vector representations for Apple, one for the company and another for the fruit. For more details about the word2vec algorithm, please check here. 2. Gensim Python Library Introduction Gensim is an open source python library for natural language processing and it was developed and is maintained by the Czech natural language processing researcher Radim Řehůřek. Gensim library will enable us to develop word embeddings by training our own word2vec models on a custom corpus either with CBOW of skip-grams algorithms. At first, we need to install the genism package. Gensim runs on Linux, Windows and Mac OS X, and should run on any other platform that supports Python 2.7+ and NumPy. Gensim depends on the following software: Python >= 2.7 (tested with versions 2.7, 3.5 and 3.6) >= 2.7 (tested with versions 2.7, 3.5 and 3.6) NumPy >= 1.11.3 >= 1.11.3 SciPy >= 0.18.1 >= 0.18.1 Six >= 1.5.0 >= 1.5.0 smart_open >= 1.2.1 There are two ways for installation. We could run the following code in our terminal to install genism package. pip install --upgrade gensim Or, alternatively for Conda environments: conda install -c conda-forge gensim 3.\n",
      "\n",
      "\n",
      "\n",
      "322.0\n",
      "pip install --upgrade gensim Or, alternatively for Conda environments: conda install -c conda-forge gensim 3. Implementation of word Embedding with Gensim Word2Vec Model In this tutorial, I will show how to generate word embedding with genism using a concrete example. The dataset I used for this tutorial is from Kaggle Dataset. This vehicle dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The full python tutorial can be found here. >>> df = pd.read_csv('data.csv') >>> df.head() 3.1 Data Preprocessing: Since the purpose of this tutorial is to learn how to generate word embeddings using genism library, we will not do the EDA and feature selection for the word2vec model for the sake of simplicity. Genism word2vec requires that a format of ‘list of lists’ for training where every document is contained in a list and every list contains lists of tokens of that document. At first, we need to generate a format of ‘list of lists’ for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains lists of features of that make model.\n",
      "\n",
      "\n",
      "\n",
      "341.0\n",
      "To be more specific, each make model is contained in a list and every list contains lists of features of that make model. To achieve this, we need to do the following things : a. Create a new column for Make Model >>> df['Maker_Model']= df['Make']+ \" \" + df['Model'] b. Generate a format of ‘ list of lists’ for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size, Vehicle Style. # Select features from original dataset to form a new dataframe >>> df1 = df[['Engine Fuel Type','Transmission Type','Driven_Wheels','Market Category','Vehicle Size', 'Vehicle Style', 'Maker_Model']] # For each row, combine all the columns into one column >>> df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1) # Store them in a pandas dataframe >>> df_clean = pd. DataFrame({'clean': df2}) # Create the list of list format of the custom corpus for gensim modeling >>> sent = [row.split(',') for row in df_clean['clean']] # show the example of list of list format of the custom corpus for gensim modeling >>> sent[:2] [['premium unleaded (required)', 'MANUAL', 'rear wheel drive', 'Factory Tuner', 'Luxury', 'High-Performance', 'Compact', 'Coupe', 'BMW 1 Series M'], ['premium unleaded (required)', 'MANUAL', 'rear wheel drive', 'Luxury', 'Performance', 'Compact', 'Convertible', 'BMW 1 Series']] 3.2.\n",
      "\n",
      "\n",
      "\n",
      "359.5\n",
      "window: The maximum distance between a target word and words around the target word. The default window is 5. min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5. workers: The number of partitions during training and the default workers is 3. sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW. After training the word2vec model, we can obtain the word embedding directly from the training model as following. >>> model['Toyota Camry'] array([-0.11884457, 0.03035539, -0.0248678 , -0.06297892, -0.01703234, -0.03832747, -0.0825972 , -0.00268112, -0.09192555, -0.08458661, -0.07199778, 0.05235871, 0.21303181, 0.15767808, -0.1883737 , 0.01938575, -0.24431638, 0.04261152, 0.11865819, 0.09881561, -0.04580643, -0.08342388, -0.01355413, -0.07892415, -0.08467747, -0.0040625 , 0.16796461, 0.14578669, 0.04187112, -0.01436194, -0.25554284, 0.25494182, 0.05522631, 0.19295982, 0.14461821, 0.14022525, -0.2065216 , -0.05020927, -0.08133671, 0.18031682, 0.35042757, 0.0245426 , 0.15938364, -0.05617865, 0.00297452, 0.15442047, -0.01286271, 0.13923576, 0.085941 , 0.18811756], dtype=float32) 4. Compute Similarities Now we could even use Word2vec to compute the similarity between two Make Models in the vocabulary by invoking the model.similarity( ) and passing in the relevant words.\n",
      "\n",
      "\n",
      "\n",
      "327.5\n",
      "We also can use the built-in function model.most_similar() to get a set of the most similar make models for a given make model based on the Euclidean distance. >>> model1.most_similar('Mercedes-Benz SLK-Class')[:5] [('BMW M4', 0.9959905743598938), ('Maserati Coupe', 0.9949707984924316), ('Porsche Cayman', 0.9945154190063477), ('Mercedes-Benz SLS AMG GT', 0.9944609999656677), ('Maserati Spyder', 0.9942780137062073)] However, Euclidian similarity cannot work well for the high-dimensional word vectors. This is because Euclidian similarity will increase as the number of dimensions increases, even if the word embedding stands for different meanings. Alternatively, we can use cosine similarity to measure the similarity between two vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at a 0-degree angle. The following function shows how can we generate the most similar make model based on cosine similarity. def cosine_distance (model, word,target_list , num) : cosine_dict ={} word_list = [] a = model[word] for item in target_list : if item !\n",
      "\n",
      "\n",
      "\n",
      "317.25\n",
      "def cosine_distance (model, word,target_list , num) : cosine_dict ={} word_list = [] a = model[word] for item in target_list : if item ! = word : b = model [item] cos_sim = dot(a, b)/(norm(a)*norm(b)) cosine_dict[item] = cos_sim dist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order for item in dist_sort: word_list.append((item[0], item[1])) return word_list[0:num] # only get the unique Maker_Model >>> Maker_Model = list(df. Maker_Model.unique()) # Show the most similar Mercedes-Benz SLK-Class by cosine distance >>> cosine_distance (model,'Mercedes-Benz SLK-Class',Maker_Model,5) [('Mercedes-Benz CLK-Class', 0.99737006), ('Aston Martin DB9', 0.99593246), ('Maserati Spyder', 0.99571854), ('Ferrari 458 Italia', 0.9952333), ('Maserati GranTurismo Convertible', 0.994994)] 5. T-SNE Visualizations It’s hard to visualize the word embedding directly, for they usually have more than 3 dimensions. T-SNE is a useful tool to visualize high-dimensional data by dimension reduction while keeping relative pairwise distance between points. It can be said that T-SNE looking for a new data representation where the neighborhood relations are preserved. The following code shows how to plot the word embedding with T-SNE plot.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 210
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Creating a dataset with chunks",
   "id": "9ce2507de8bc9739"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:09:23.950466Z",
     "start_time": "2024-04-26T20:09:19.600140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "chunks = pd.DataFrame(columns=[\"Title\", \"Chunk\", \"Chunk_length\", \"Chunk_words_num\", \"Chunk_sentences_num\", \"Token_count\"])\n",
    "\n",
    "for d in dict_data:\n",
    "    for chunk in d[\"chunks\"]:\n",
    "        chunk = chunk.strip()\n",
    "        chunk = re.sub(r\"\\s+\", \" \", chunk)\n",
    "        row = pd.DataFrame.from_dict(\n",
    "            {\n",
    "                \"Title\": d[\"Title\"],\n",
    "                \"Chunk\": chunk,\n",
    "                \"Chunk_length\": len(chunk),\n",
    "                \"Chunk_words_num\": len(chunk.split()),\n",
    "                \"Chunk_sentences_num\": len(list(nlp(chunk).sents)),\n",
    "                \"Token_count\": len(chunk) / 4\n",
    "            },\n",
    "            orient=\"index\"\n",
    "        )\n",
    "        row = row.T\n",
    "        chunks = pd.concat([chunks, row], axis=0)\n",
    "        \n",
    "numeric = [\"Chunk_length\", \"Chunk_words_num\", \"Chunk_sentences_num\", \"Token_count\"]\n",
    "chunks[numeric] = chunks[numeric].astype(float)\n",
    "\n",
    "chunks.head()"
   ],
   "id": "cf491b4645bed9e8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                             Title  \\\n",
       "0  A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model   \n",
       "0  A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model   \n",
       "0  A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model   \n",
       "0  A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model   \n",
       "0  A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Chunk  \\\n",
       "0                                                                                                                                                                                                                                                                                                            1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.   \n",
       "0                                                                                                                                                                                                                                                                          For instance, it will have two vector representations for Apple, one for the company and another for the fruit. For more details about the word2vec algorithm, please check here. 2. Gensim Python Library Introduction Gensim is an open source python library for natural language processing and it was developed and is maintained by the Czech natural language processing researcher Radim Řehůřek. Gensim library will enable us to develop word embeddings by training our own word2vec models on a custom corpus either with CBOW of skip-grams algorithms. At first, we need to install the genism package. Gensim runs on Linux, Windows and Mac OS X, and should run on any other platform that supports Python 2.7+ and NumPy. Gensim depends on the following software: Python >= 2.7 (tested with versions 2.7, 3.5 and 3.6) >= 2.7 (tested with versions 2.7, 3.5 and 3.6) NumPy >= 1.11.3 >= 1.11.3 SciPy >= 0.18.1 >= 0.18.1 Six >= 1.5.0 >= 1.5.0 smart_open >= 1.2.1 There are two ways for installation. We could run the following code in our terminal to install genism package. pip install --upgrade gensim Or, alternatively for Conda environments: conda install -c conda-forge gensim 3.   \n",
       "0                                                                                                                                                        pip install --upgrade gensim Or, alternatively for Conda environments: conda install -c conda-forge gensim 3. Implementation of word Embedding with Gensim Word2Vec Model In this tutorial, I will show how to generate word embedding with genism using a concrete example. The dataset I used for this tutorial is from Kaggle Dataset. This vehicle dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The full python tutorial can be found here. >>> df = pd.read_csv('data.csv') >>> df.head() 3.1 Data Preprocessing: Since the purpose of this tutorial is to learn how to generate word embeddings using genism library, we will not do the EDA and feature selection for the word2vec model for the sake of simplicity. Genism word2vec requires that a format of ‘list of lists’ for training where every document is contained in a list and every list contains lists of tokens of that document. At first, we need to generate a format of ‘list of lists’ for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains lists of features of that make model.   \n",
       "0                                                                            To be more specific, each make model is contained in a list and every list contains lists of features of that make model. To achieve this, we need to do the following things : a. Create a new column for Make Model >>> df['Maker_Model']= df['Make']+ \" \" + df['Model'] b. Generate a format of ‘ list of lists’ for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size, Vehicle Style. # Select features from original dataset to form a new dataframe >>> df1 = df[['Engine Fuel Type','Transmission Type','Driven_Wheels','Market Category','Vehicle Size', 'Vehicle Style', 'Maker_Model']] # For each row, combine all the columns into one column >>> df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1) # Store them in a pandas dataframe >>> df_clean = pd. DataFrame({'clean': df2}) # Create the list of list format of the custom corpus for gensim modeling >>> sent = [row.split(',') for row in df_clean['clean']] # show the example of list of list format of the custom corpus for gensim modeling >>> sent[:2] [['premium unleaded (required)', 'MANUAL', 'rear wheel drive', 'Factory Tuner', 'Luxury', 'High-Performance', 'Compact', 'Coupe', 'BMW 1 Series M'], ['premium unleaded (required)', 'MANUAL', 'rear wheel drive', 'Luxury', 'Performance', 'Compact', 'Convertible', 'BMW 1 Series']] 3.2.   \n",
       "0  window: The maximum distance between a target word and words around the target word. The default window is 5. min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5. workers: The number of partitions during training and the default workers is 3. sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW. After training the word2vec model, we can obtain the word embedding directly from the training model as following. >>> model['Toyota Camry'] array([-0.11884457, 0.03035539, -0.0248678 , -0.06297892, -0.01703234, -0.03832747, -0.0825972 , -0.00268112, -0.09192555, -0.08458661, -0.07199778, 0.05235871, 0.21303181, 0.15767808, -0.1883737 , 0.01938575, -0.24431638, 0.04261152, 0.11865819, 0.09881561, -0.04580643, -0.08342388, -0.01355413, -0.07892415, -0.08467747, -0.0040625 , 0.16796461, 0.14578669, 0.04187112, -0.01436194, -0.25554284, 0.25494182, 0.05522631, 0.19295982, 0.14461821, 0.14022525, -0.2065216 , -0.05020927, -0.08133671, 0.18031682, 0.35042757, 0.0245426 , 0.15938364, -0.05617865, 0.00297452, 0.15442047, -0.01286271, 0.13923576, 0.085941 , 0.18811756], dtype=float32) 4. Compute Similarities Now we could even use Word2vec to compute the similarity between two Make Models in the vocabulary by invoking the model.similarity( ) and passing in the relevant words.   \n",
       "\n",
       "   Chunk_length  Chunk_words_num  Chunk_sentences_num  Token_count  \n",
       "0        1140.0            188.0                 10.0        285.0  \n",
       "0        1174.0            194.0                 10.0        293.5  \n",
       "0        1288.0            215.0                 10.0        322.0  \n",
       "0        1364.0            204.0                  4.0        341.0  \n",
       "0        1438.0            185.0                 10.0        359.5  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Chunk</th>\n",
       "      <th>Chunk_length</th>\n",
       "      <th>Chunk_words_num</th>\n",
       "      <th>Chunk_sentences_num</th>\n",
       "      <th>Token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model</td>\n",
       "      <td>1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>285.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model</td>\n",
       "      <td>For instance, it will have two vector representations for Apple, one for the company and another for the fruit. For more details about the word2vec algorithm, please check here. 2. Gensim Python Library Introduction Gensim is an open source python library for natural language processing and it was developed and is maintained by the Czech natural language processing researcher Radim Řehůřek. Gensim library will enable us to develop word embeddings by training our own word2vec models on a custom corpus either with CBOW of skip-grams algorithms. At first, we need to install the genism package. Gensim runs on Linux, Windows and Mac OS X, and should run on any other platform that supports Python 2.7+ and NumPy. Gensim depends on the following software: Python &gt;= 2.7 (tested with versions 2.7, 3.5 and 3.6) &gt;= 2.7 (tested with versions 2.7, 3.5 and 3.6) NumPy &gt;= 1.11.3 &gt;= 1.11.3 SciPy &gt;= 0.18.1 &gt;= 0.18.1 Six &gt;= 1.5.0 &gt;= 1.5.0 smart_open &gt;= 1.2.1 There are two ways for installation. We could run the following code in our terminal to install genism package. pip install --upgrade gensim Or, alternatively for Conda environments: conda install -c conda-forge gensim 3.</td>\n",
       "      <td>1174.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>293.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model</td>\n",
       "      <td>pip install --upgrade gensim Or, alternatively for Conda environments: conda install -c conda-forge gensim 3. Implementation of word Embedding with Gensim Word2Vec Model In this tutorial, I will show how to generate word embedding with genism using a concrete example. The dataset I used for this tutorial is from Kaggle Dataset. This vehicle dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The full python tutorial can be found here. &gt;&gt;&gt; df = pd.read_csv('data.csv') &gt;&gt;&gt; df.head() 3.1 Data Preprocessing: Since the purpose of this tutorial is to learn how to generate word embeddings using genism library, we will not do the EDA and feature selection for the word2vec model for the sake of simplicity. Genism word2vec requires that a format of ‘list of lists’ for training where every document is contained in a list and every list contains lists of tokens of that document. At first, we need to generate a format of ‘list of lists’ for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains lists of features of that make model.</td>\n",
       "      <td>1288.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>322.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model</td>\n",
       "      <td>To be more specific, each make model is contained in a list and every list contains lists of features of that make model. To achieve this, we need to do the following things : a. Create a new column for Make Model &gt;&gt;&gt; df['Maker_Model']= df['Make']+ \" \" + df['Model'] b. Generate a format of ‘ list of lists’ for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size, Vehicle Style. # Select features from original dataset to form a new dataframe &gt;&gt;&gt; df1 = df[['Engine Fuel Type','Transmission Type','Driven_Wheels','Market Category','Vehicle Size', 'Vehicle Style', 'Maker_Model']] # For each row, combine all the columns into one column &gt;&gt;&gt; df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1) # Store them in a pandas dataframe &gt;&gt;&gt; df_clean = pd. DataFrame({'clean': df2}) # Create the list of list format of the custom corpus for gensim modeling &gt;&gt;&gt; sent = [row.split(',') for row in df_clean['clean']] # show the example of list of list format of the custom corpus for gensim modeling &gt;&gt;&gt; sent[:2] [['premium unleaded (required)', 'MANUAL', 'rear wheel drive', 'Factory Tuner', 'Luxury', 'High-Performance', 'Compact', 'Coupe', 'BMW 1 Series M'], ['premium unleaded (required)', 'MANUAL', 'rear wheel drive', 'Luxury', 'Performance', 'Compact', 'Convertible', 'BMW 1 Series']] 3.2.</td>\n",
       "      <td>1364.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>341.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model</td>\n",
       "      <td>window: The maximum distance between a target word and words around the target word. The default window is 5. min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5. workers: The number of partitions during training and the default workers is 3. sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW. After training the word2vec model, we can obtain the word embedding directly from the training model as following. &gt;&gt;&gt; model['Toyota Camry'] array([-0.11884457, 0.03035539, -0.0248678 , -0.06297892, -0.01703234, -0.03832747, -0.0825972 , -0.00268112, -0.09192555, -0.08458661, -0.07199778, 0.05235871, 0.21303181, 0.15767808, -0.1883737 , 0.01938575, -0.24431638, 0.04261152, 0.11865819, 0.09881561, -0.04580643, -0.08342388, -0.01355413, -0.07892415, -0.08467747, -0.0040625 , 0.16796461, 0.14578669, 0.04187112, -0.01436194, -0.25554284, 0.25494182, 0.05522631, 0.19295982, 0.14461821, 0.14022525, -0.2065216 , -0.05020927, -0.08133671, 0.18031682, 0.35042757, 0.0245426 , 0.15938364, -0.05617865, 0.00297452, 0.15442047, -0.01286271, 0.13923576, 0.085941 , 0.18811756], dtype=float32) 4. Compute Similarities Now we could even use Word2vec to compute the similarity between two Make Models in the vocabulary by invoking the model.similarity( ) and passing in the relevant words.</td>\n",
       "      <td>1438.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>359.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 211
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:09:23.954865Z",
     "start_time": "2024-04-26T20:09:23.951114Z"
    }
   },
   "cell_type": "code",
   "source": "chunks.sample(1)",
   "id": "3b943d25bf1b0980",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                   Title  \\\n",
       "0  Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Chunk  \\\n",
       "0  CREATING LIBRARIES IN R R is one of the “big 2” languages of machine learning. At the time of this writing it has well-over 10,000 libraries. Going to Available CRAN Packages By Date of Publication and running… document.getElementsByTagName('tr').length …in the browser console gives me 13858. Minus the header and final row this gives 13856 packages. Needless to say R is not in need of variety. With strong community support and a concise (if not intuitive) language, R sits comfortably at the top of statistical languages worth learning. The most well-known treatise on creating R packages is Hadley Wickam’s book R Packages. Its contents are available for free online. For a deeper dive on topic I recommend looking there. We will use Hadley’s devtools package to abstract away the tedious tasks involved in creating packages.   \n",
       "\n",
       "   Chunk_length  Chunk_words_num  Chunk_sentences_num  Token_count  \n",
       "0         830.0            134.0                 10.0        207.5  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Chunk</th>\n",
       "      <th>Chunk_length</th>\n",
       "      <th>Chunk_words_num</th>\n",
       "      <th>Chunk_sentences_num</th>\n",
       "      <th>Token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab)</td>\n",
       "      <td>CREATING LIBRARIES IN R R is one of the “big 2” languages of machine learning. At the time of this writing it has well-over 10,000 libraries. Going to Available CRAN Packages By Date of Publication and running… document.getElementsByTagName('tr').length …in the browser console gives me 13858. Minus the header and final row this gives 13856 packages. Needless to say R is not in need of variety. With strong community support and a concise (if not intuitive) language, R sits comfortably at the top of statistical languages worth learning. The most well-known treatise on creating R packages is Hadley Wickam’s book R Packages. Its contents are available for free online. For a deeper dive on topic I recommend looking there. We will use Hadley’s devtools package to abstract away the tedious tasks involved in creating packages.</td>\n",
       "      <td>830.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>207.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 212
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:09:23.963365Z",
     "start_time": "2024-04-26T20:09:23.955679Z"
    }
   },
   "cell_type": "code",
   "source": "chunks.describe().astype(float).round(2)",
   "id": "182c909727869b70",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Chunk_length  Chunk_words_num  Chunk_sentences_num  Token_count\n",
       "count       7334.00          7334.00              7334.00      7334.00\n",
       "mean        1052.25           173.26                 8.80       263.06\n",
       "std          331.35            54.58                 2.16        82.84\n",
       "min            0.00             0.00                 0.00         0.00\n",
       "25%          865.00           144.00                 9.00       216.25\n",
       "50%         1100.00           182.00                10.00       275.00\n",
       "75%         1313.00           213.00                10.00       328.25\n",
       "max         1536.00           291.00                10.00       384.00"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chunk_length</th>\n",
       "      <th>Chunk_words_num</th>\n",
       "      <th>Chunk_sentences_num</th>\n",
       "      <th>Token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7334.00</td>\n",
       "      <td>7334.00</td>\n",
       "      <td>7334.00</td>\n",
       "      <td>7334.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1052.25</td>\n",
       "      <td>173.26</td>\n",
       "      <td>8.80</td>\n",
       "      <td>263.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>331.35</td>\n",
       "      <td>54.58</td>\n",
       "      <td>2.16</td>\n",
       "      <td>82.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>865.00</td>\n",
       "      <td>144.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>216.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1100.00</td>\n",
       "      <td>182.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>275.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1313.00</td>\n",
       "      <td>213.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>328.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1536.00</td>\n",
       "      <td>291.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>384.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 213
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:09:23.966746Z",
     "start_time": "2024-04-26T20:09:23.964137Z"
    }
   },
   "cell_type": "code",
   "source": "chunks = chunks[chunks[\"Token_count\"] > 0]",
   "id": "84904f60068b9204",
   "outputs": [],
   "execution_count": 214
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:09:23.974676Z",
     "start_time": "2024-04-26T20:09:23.967496Z"
    }
   },
   "cell_type": "code",
   "source": "chunks.describe()",
   "id": "a84e3540a747eec4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Chunk_length  Chunk_words_num  Chunk_sentences_num  Token_count\n",
       "count   7292.000000      7292.000000          7292.000000  7292.000000\n",
       "mean    1058.311437       174.255623             8.854361   264.577859\n",
       "std      322.506653        53.123813             2.061306    80.626663\n",
       "min        9.000000         2.000000             1.000000     2.250000\n",
       "25%      871.000000       145.000000             9.000000   217.750000\n",
       "50%     1101.000000       182.000000            10.000000   275.250000\n",
       "75%     1314.000000       213.250000            10.000000   328.500000\n",
       "max     1536.000000       291.000000            10.000000   384.000000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chunk_length</th>\n",
       "      <th>Chunk_words_num</th>\n",
       "      <th>Chunk_sentences_num</th>\n",
       "      <th>Token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7292.000000</td>\n",
       "      <td>7292.000000</td>\n",
       "      <td>7292.000000</td>\n",
       "      <td>7292.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1058.311437</td>\n",
       "      <td>174.255623</td>\n",
       "      <td>8.854361</td>\n",
       "      <td>264.577859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>322.506653</td>\n",
       "      <td>53.123813</td>\n",
       "      <td>2.061306</td>\n",
       "      <td>80.626663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>871.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>217.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1101.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>275.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1314.000000</td>\n",
       "      <td>213.250000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>328.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1536.000000</td>\n",
       "      <td>291.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>384.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 215
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Printing some chunks with low token count",
   "id": "789065d133527bc7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:09:23.978360Z",
     "start_time": "2024-04-26T20:09:23.975376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MIN_TOKENS = 50\n",
    "\n",
    "for ch in chunks[chunks[\"Token_count\"] < 50].sample(5)[\"Chunk\"].values:\n",
    "    print(ch)\n",
    "    print(\"\\n\\n\")\n",
    "\n"
   ],
   "id": "fcedfd78f12eea02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Or will it simply distract from more pressing problems here on Earth? Let us know your thoughts below…\n",
      "\n",
      "\n",
      "\n",
      "GitHub is an online platform based on Git. What do you need to get started with GitHub? Installation\n",
      "\n",
      "\n",
      "\n",
      "8. Horizontal Flip The generator will generate images, which on a random basis, will be horizontally flipped. 9. Vertical Flip Instead of flipping horizontally, we can also apply a vertical flip.\n",
      "\n",
      "\n",
      "\n",
      "I’m currently seeking a full time data science position! More at lukepersola.com.\n",
      "\n",
      "\n",
      "\n",
      "Informally, this is called a Swiss roll, a canonical problem in the field of non-linear dimensionality reduction. Some popular manifold learning methods are,\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 216
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can see most of these short sentences come from the end of the articles. We can safely remove them from the dataset as they bring little information to the table.",
   "id": "d5a5f2674e24c2cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:09:23.981688Z",
     "start_time": "2024-04-26T20:09:23.978978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(chunks.shape)\n",
    "chunks = chunks[chunks[\"Token_count\"] > MIN_TOKENS]\n",
    "print(chunks.shape)"
   ],
   "id": "3849783fd100b09e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7292, 6)\n",
      "(7185, 6)\n"
     ]
    }
   ],
   "execution_count": 217
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Embedding the chunks",
   "id": "127c33dbe4ad5909"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:09:25.332604Z",
     "start_time": "2024-04-26T20:09:25.331223Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6ea949ed430b97dc",
   "outputs": [],
   "execution_count": 218
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T20:09:25.334725Z",
     "start_time": "2024-04-26T20:09:25.333337Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7b63ce8600a0969",
   "outputs": [],
   "execution_count": 218
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
