{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-06T19:35:12.468422Z",
     "start_time": "2024-05-06T19:35:11.133757Z"
    }
   },
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Text preprocessing"
   ],
   "id": "3a3f0d23d628665c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:35:12.542702Z",
     "start_time": "2024-05-06T19:35:12.469505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(\"../data/medium.csv\")\n",
    "data.head()"
   ],
   "id": "f865993fea6e49f6",
   "outputs": [
    {
     "data": {
      "text/plain": "                                               Title  \\\n0  A Beginner’s Guide to Word Embedding with Gens...   \n1  Hands-on Graph Neural Networks with PyTorch & ...   \n2                       How to Use ggplot2 in Python   \n3  Databricks: How to Save Data Frames as CSV Fil...   \n4  A Step-by-Step Implementation of Gradient Desc...   \n\n                                                Text  \n0  1. Introduction of Word2vec\\n\\nWord2vec is one...  \n1  In my last article, I introduced the concept o...  \n2  Introduction\\n\\nThanks to its strict implement...  \n3  Photo credit to Mika Baumeister from Unsplash\\...  \n4  A Step-by-Step Implementation of Gradient Desc...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>1. Introduction of Word2vec\\n\\nWord2vec is one...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n      <td>In my last article, I introduced the concept o...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>How to Use ggplot2 in Python</td>\n      <td>Introduction\\n\\nThanks to its strict implement...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Databricks: How to Save Data Frames as CSV Fil...</td>\n      <td>Photo credit to Mika Baumeister from Unsplash\\...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:35:12.545809Z",
     "start_time": "2024-05-06T19:35:12.543423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_text(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    data[\"Text\"] = data[\"Text\"].str.replace(\"\\n\", \" \")\n",
    "    data[\"Text\"] = data[\"Text\"].str.replace(\"\\t\", \" \")\n",
    "    data[\"Text\"] = data[\"Text\"].str.replace(\"\\r\", \" \")\n",
    "    data[\"Text\"] = data[\"Text\"].str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    data[\"Text\"] = data[\"Text\"].str.strip()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def sample_random_texts(data: pd.DataFrame, n=5) -> None:\n",
    "    for text in data.sample(n)[\"Text\"].values:\n",
    "        print(text)\n",
    "        print(\"\\n\\n\")\n",
    "        \n",
    "        "
   ],
   "id": "8c40e3043864d4c1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:35:12.734723Z",
     "start_time": "2024-05-06T19:35:12.546416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = preprocess_text(data)\n",
    "data.head()"
   ],
   "id": "b24eff34bcbc7220",
   "outputs": [
    {
     "data": {
      "text/plain": "                                               Title  \\\n0  A Beginner’s Guide to Word Embedding with Gens...   \n1  Hands-on Graph Neural Networks with PyTorch & ...   \n2                       How to Use ggplot2 in Python   \n3  Databricks: How to Save Data Frames as CSV Fil...   \n4  A Step-by-Step Implementation of Gradient Desc...   \n\n                                                Text  \n0  1. Introduction of Word2vec Word2vec is one of...  \n1  In my last article, I introduced the concept o...  \n2  Introduction Thanks to its strict implementati...  \n3  Photo credit to Mika Baumeister from Unsplash ...  \n4  A Step-by-Step Implementation of Gradient Desc...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>1. Introduction of Word2vec Word2vec is one of...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n      <td>In my last article, I introduced the concept o...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>How to Use ggplot2 in Python</td>\n      <td>Introduction Thanks to its strict implementati...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Databricks: How to Save Data Frames as CSV Fil...</td>\n      <td>Photo credit to Mika Baumeister from Unsplash ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Feature engineering for text data EDA"
   ],
   "id": "5cd572873b2395ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:35:17.221191Z",
     "start_time": "2024-05-06T19:35:12.736315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from spacy.lang.en import English\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "nlp = English()\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "data[\"text_words_num\"] = data[\"Text\"].apply(lambda x: len(x.split()))\n",
    "data[\"text_length\"] = data[\"Text\"].apply(lambda x: len(x))\n",
    "data[\"text_sentences_num\"] = data[\"Text\"].progress_apply(lambda x: len(list(nlp(x).sents)))\n",
    "data[\"token_count\"] = data[\"Text\"].apply(lambda x: len(x) / 4)\n",
    "data.head()"
   ],
   "id": "c11a3100d66138be",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1391/1391 [00:03<00:00, 398.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                               Title  \\\n0  A Beginner’s Guide to Word Embedding with Gens...   \n1  Hands-on Graph Neural Networks with PyTorch & ...   \n2                       How to Use ggplot2 in Python   \n3  Databricks: How to Save Data Frames as CSV Fil...   \n4  A Step-by-Step Implementation of Gradient Desc...   \n\n                                                Text  text_words_num  \\\n0  1. Introduction of Word2vec Word2vec is one of...            1489   \n1  In my last article, I introduced the concept o...             139   \n2  Introduction Thanks to its strict implementati...             953   \n3  Photo credit to Mika Baumeister from Unsplash ...             280   \n4  A Step-by-Step Implementation of Gradient Desc...             737   \n\n   text_length  text_sentences_num  token_count  \n0        10432                  64      2608.00  \n1          827                   7       206.75  \n2         5632                  45      1408.00  \n3         1776                  16       444.00  \n4         4744                  28      1186.00  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Text</th>\n      <th>text_words_num</th>\n      <th>text_length</th>\n      <th>text_sentences_num</th>\n      <th>token_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>1. Introduction of Word2vec Word2vec is one of...</td>\n      <td>1489</td>\n      <td>10432</td>\n      <td>64</td>\n      <td>2608.00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n      <td>In my last article, I introduced the concept o...</td>\n      <td>139</td>\n      <td>827</td>\n      <td>7</td>\n      <td>206.75</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>How to Use ggplot2 in Python</td>\n      <td>Introduction Thanks to its strict implementati...</td>\n      <td>953</td>\n      <td>5632</td>\n      <td>45</td>\n      <td>1408.00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Databricks: How to Save Data Frames as CSV Fil...</td>\n      <td>Photo credit to Mika Baumeister from Unsplash ...</td>\n      <td>280</td>\n      <td>1776</td>\n      <td>16</td>\n      <td>444.00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n      <td>737</td>\n      <td>4744</td>\n      <td>28</td>\n      <td>1186.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:35:17.230040Z",
     "start_time": "2024-05-06T19:35:17.222030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data.describe().round(2)"
   ],
   "id": "57b7d0f23dda3224",
   "outputs": [
    {
     "data": {
      "text/plain": "       text_words_num  text_length  text_sentences_num  token_count\ncount         1391.00      1391.00             1391.00      1391.00\nmean           901.54      5530.81               44.46      1382.70\nstd            885.73      5521.48               44.17      1380.37\nmin             49.00       249.00                2.00        62.25\n25%            315.00      1886.00               15.00       471.50\n50%            516.00      3040.00               26.00       760.00\n75%           1227.00      7577.50               60.00      1894.38\nmax           7657.00     46966.00              376.00     11741.50",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_words_num</th>\n      <th>text_length</th>\n      <th>text_sentences_num</th>\n      <th>token_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1391.00</td>\n      <td>1391.00</td>\n      <td>1391.00</td>\n      <td>1391.00</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>901.54</td>\n      <td>5530.81</td>\n      <td>44.46</td>\n      <td>1382.70</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>885.73</td>\n      <td>5521.48</td>\n      <td>44.17</td>\n      <td>1380.37</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>49.00</td>\n      <td>249.00</td>\n      <td>2.00</td>\n      <td>62.25</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>315.00</td>\n      <td>1886.00</td>\n      <td>15.00</td>\n      <td>471.50</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>516.00</td>\n      <td>3040.00</td>\n      <td>26.00</td>\n      <td>760.00</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1227.00</td>\n      <td>7577.50</td>\n      <td>60.00</td>\n      <td>1894.38</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>7657.00</td>\n      <td>46966.00</td>\n      <td>376.00</td>\n      <td>11741.50</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Chunking articles into smaller sentences"
   ],
   "id": "22d860d188b7736"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:35:17.247615Z",
     "start_time": "2024-05-06T19:35:17.230641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for _, row in data.sample(5).iterrows():\n",
    "    doc = nlp(row[\"Text\"])\n",
    "    sents = [sent.text for sent in doc.sents]\n",
    "    print(sents)\n",
    "    print(\"\\n\\n\")"
   ],
   "id": "92405363daaaef13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Introduction Have you ever tried to use a movie recommender?', 'In theory, it is something useful that can help figure out what to watch next instead of browsing through Netflix for a few hours, but their results tend to be hit-or-miss.', 'This is a problem that most people can relate to, so I decided to create a homemade recommender system myself and share it in this blog post.', 'I will show you how to create 3 simple recommender models from scratch that accept a movie as input and return the “n” most similar movies as output, with “n” being provided by the user.', 'In general, recommender systems are either content-based or collaborative with the user’s history and interests.', 'I chose to create content-based models since they make predictions based on the specific input item (movie) and not based on the user.', 'Note that the recommendations for this blog are based entirely off of movie keywords.', 'As you will see, it is fairly simple to use other text information, such as plot outlines or plot synopses, instead of movie keywords.', 'To limit the scope of this post, I did not include numeric features (e.g. runtime, year of release, rating, etc.)', 'and genre for recommendations.', 'If you want to follow along or see the code for this post on GitHub, visit: https://github.com/gblinick/Movie-Recommender-with-NLP/blob/master/Keyword%20Movie%20Recommender.ipynb Dataset Creation The first step in any data science project is to acquire your dataset.', 'Sometimes your dataset(s) will be given to you (like with Kaggle competitions).', 'In our case, since this project is “from scratch” we must obtain the data ourselves.', 'The first place to look for data on movies is IMDb’s public datasets repository: https://www.imdb.com/interfaces/. This repository contains 7 tables with all sorts of great movie data.', 'Unfortunately, none of the tables contain movie keywords that we can use for recommendations.', 'That does not mean that we cannot use any of the tables.', 'Looking at what the tables have to offer, we notice that one of them, title.ratings.tsv.gz contains a column called “numVotes” representing the “number of votes the title has received”.', 'This is useful to us because we can use this column to get the top 10,000 movies by number of votes.', 'Of course, it would be nice to be able to include every movie ever made in our model but that is not practical due to the extra time needed to compute similarity.', 'Additionally, even if we did not have a practical computation drawback, I am not sure most people are interested in watching movies outside of the top 10,000.', 'The top 10,000 movies should suffice to keep the vast majority of viewers happy.', 'We also need a second table, title.basics.tsv.gz, which provides the column TitleType, which tells us if the title is a movie (as opposed to a TV episode).', 'Since we only want to include movies in our recommenders, we use this column to filter for movies.', 'Putting everything together, this is our workflow so far: Import necessary libraries (pandas and numpy): 2.', 'Import the 2 IMDb tables we need into pandas’ DataFrames (in the image, the files are in a folder called ‘data’ which is parallel to our notebook): 3.', 'Select only movies from the title.basics table and create a new DataFrame, ‘movies’: 4.', 'Create a common index for the tables (the IMDb unique identifier of each title in both tables is a great choice here) and then join the tables with a simple inner join: Great!', 'We now have a table called “movies_with_rating” that has all the IMDb movies with their ratings (and all the other information in the 2 tables we used).', 'Let’s take a look at the top row: We see some interesting information about the first movie in the table.', 'We don’t need all this data for our future models, however.', 'All we care about for this project is getting the top 10,000 movies by numVotes, and using movie keywords to find similarity between movies.', 'We can do that easily using the pandas’ .sort_values function: Perfect.', 'We can now turn our attention to getting the plot information we want to use for our recommender models.', 'We can get such plot information using the wonderful IMDbPY API: https://imdbpy.readthedocs.io/en/latest/index.html.', 'You will need to install the API using pip or conda.', 'Before using the API, we need to import it and instantiate an IMDb movie object.', 'We’ll do that and also import a couple other libraries that make running for-loops with an API a nicer experience: The syntax for pulling plot keywords from IMDb is as follows: In the above code, I first instantiate a dictionary to store the keywords I will get back from IMDb (in line 1).', 'I will use the movie IDs as keys and the list of keywords for each movie as values.', 'In the for loop beginning on line 2, I loop through the indexes of the top 10,000 movies.', 'On line 5, I use each movie index to get the corresponding IMDb movie object.', 'Note that on line 5 I subset the movie index from index 2 and on because the indexes start with ‘tt’ followed by a number and we only need the number.', 'I then subset the movie object by ‘plot outline’ to get the required plot outline, and then store the resulting keywords list in my keywords dictionary as a value corresponding to the movie index key.', 'Note my use of the tqdm and sleep libraries in the code above.', 'tqdm allows you to see where you are in the running of a for-loop.', 'When you are running a for-loop 10,000 times (as we are), it’s nice to know how far along you are.', 'The sleep library allows us to be courteous to IMDb.', 'Given that we are asking the website to give us information 10,000 times, it is nice to space our requests to it so that we do not overwhelm the server.', 'After finishing the for-loop, we have a dictionary where the keys are movie IDs, and the values are keywords lists.', 'It looks like this: We should convert this dictionary to a pandas DataFrame to take advantage of pandas’ abilities.', 'However, if we just do a simple pd.', 'DataFrame(keywords_dict), pandas will complain that our dictionaries aren’t the same length.', 'So we need a solution to get us what we want, as illustrated in the following code: In line 1, we create a new dictionary where we have the same keys but different values from our earlier dictionary.', 'Instead of the values being keywords lists, they are now pandas’ Series.', 'We can convert this dictionary to a DataFrame and then join all the keywords (which are now their own columns) with a simple lambda function in line 10.', 'After applying the lambda function we get back a Series, so we can convert this Series to a DataFrame, rename our columns, and save to a CSV.', 'We don’t need to save the plot outlines to a CSV, but anytime you obtain data from a process that takes a long time, it is very smart to save it so that you do not have to re-run the process that got you the data.', 'To use our plot outlines data, we can load it from the keywords.csv file, rename the 2 columns that exist in the file (i.e. the key and value) and join it to our existing table: And that’s it!', 'We now have a dataset of plot text information that we can use to calculate similarity between movies.', 'EDA Before creating our models, let’s do some quick EDA.', 'Text data is not the easiest data to do exploratory data analysis with, so we’ll keep the EDA short.', 'The first thing we can do is take a look at the distribution for the number of characters in each keyword list.', 'Let’s first get rid of movies with no keywords: We can now find the length (in characters) of each keyword list: We see from the distribution and the average number of character per column that the distribution is quite right-tailed.', 'Let’s see if we get a similar-looking distribution if we plot the distribution for the number of words in each keyword list: Yep, also quite right-tailed.', 'So we looked at 2 different measures for our keyword lists — number of characters of the list and number of words in the list — and they showed the same pattern.', 'Which makes sense, as the measures are intuitively correlated.', 'What the right-tails tell us is that most movies do not have a lot of keywords or characters but a small few — presumably the most successful movies — have a lot of keywords and characters.', 'Creating Models We can now create our models.', 'First, I would like to discuss a point about NLP and using text in general.', 'Ordinarily, when using text, you do not have nice, easy-to-use keyword lists.', 'This would have been the case for us if we had decided to use plot synopses instead of keyword lists (which would have happened if IMDb did not provide movie keywords).', 'In that case, there are a number of standard, simple steps you can take in order to essentially create your own keyword lists.', 'They are: 1.', 'Lowercasing words 2.', 'Removing punctuation 3.', 'Removing stop words 4.', 'Lemmatizing words These can all be accomplished relatively simply using the nltk library.', 'Back to our models: We are going to create 3 different models, based both on different notions of similarity as well as different formulations of movie text ‘vectors’.', 'The first 2 models will use cosine similarity, and the last one will use Jaccard similarity.', 'Within the first 2 models, the first will use tf-idf to create movie vectors, and the second will use simple word counts.', 'We start by selecting the columns we will need.', 'Since we are creating keyword-based recommenders, we need the keywords column.', 'Because we also want to input a movie title and get back other movies, we will need the ‘primaryTitle’ column.', 'So we will select those 2 columns.', 'We also reset the index because our recommending function later will use a movie’s position in the DataFrame, not its IMDb ID, for recommendations: Next, we need to get a list of lists, where the inner lists are the lists of keywords for each movie.', 'What we have in our keywords column currently for each movie is a string with all the keywords in it and separated by commas.', 'We can get such a list of lists by using the nltk library: In line 2, we get the aforementioned list of strings.', 'In line 4, we import the work_tokenize module from the nltk library.', 'In line 5, we have a list comprehension where we loop through every keyword string and for each one we get the individual words with word_tokenize.', 'Note that word_tokenize(keyword.lower()) returns a list of keywords, so we end up with a list of lists.', 'We are not quite done at this point since word_tokenize left us with many commas in our list of keyword lists.', 'We can easily get rid of them by defining a custom function, no_commas, and applying it to every keyword list in our list of keyword lists: Nice.', 'So we now have what we want in processed_keywords.', 'We can now create our first model, a tf-idf model that uses cosine similarity between word-vectors.', 'I’m not going to explain how those work here.', 'A quick Google search should return many helpful resources.', 'We start by creating a dictionary of words using gensim: The dictionary here just contains every word in our processed keywords list matched with an ID.', 'Next, we create a genism corpus, where corpus here just means ‘bag-of-words for each movie’: Next, we can convert these bags-of-words into tf-idf models using a genism tf-idf model: Finally, we create an index for our set of movie keywords that allows us to compute similarities between any given set of keywords and the keywords of every movie in our dataset: We are now at the point where we can write a function that accepts a movie, and returns the “n” most similar movies.', 'The way our function works will be as follows: 1.', 'Accept a movie from a user 2.', 'Accept “n” from the user where “n” is how many movies the user wants to be returned 3.', 'Retrieve the movie’s keywords 4.', 'Convert the movie’s keywords into a bag-of-words 5.', 'Convert the bag-of-words representation into a tf-idf representation 6.', 'Use the tf-idf representation as a query doc to query our similarity measure object 7.', 'Get back the similarity results for every movie in our set, sort the set by decreasing similarity, and return the “n” most similar movies along with their similarity measures.', 'Do not return the most similar movie because every movie will be most similar to itself In code: Now let’s test it out.', 'Because the Avengers are so popular these days, let’s give the original movie to our function: All seem to be good matches.', 'Note that this kind of recommender system isn’t limited to just recommending movies based off of other movies.', 'We can query our similarity measure object using any given set of keywords.', 'It just so happens that most people will want to query it using movies they already know, but it is not limited to that.', 'Here’s the more general function for recommending based on provided keywords: We can move on to our second recommender, which also uses cosine similarity to calculate the similarity between word vectors.', 'It will differ from the first model by using simpler word counts instead of creating tf-idf word vectors.', 'This alternate implementation is performed with the CountVectorizer class in scikit-learn, which converts a collection of text documents (in this case, keyword lists) into a matrix of token counts.', 'We will also compute cosine similarity a little differently for simplicity.', 'Instead of using MatrixSimilarity, we will use the cosine_similarity function from the scikit-learn metrics.pairwise submodule.', 'We compute the word counts for each movie with the following code: Then, when given a movie, all we need to do is compute the cosine similarity between that movie’s word vector and every other word vector and return the most similar matches.', 'This is achieved with our cosine_recommender function: Let’s try it out with “The Avengers” as well: It works!', 'Similar matches to our first model.', 'For our third model, we use Jaccard similarity instead of cosine similarity.', 'As a result, we have no need for word vectors at all.', 'We simply calculate similarity as the intersection of the set of keywords divided by the union of the set of keywords.', 'The code for computing Jaccard Similarity between any two lists of keywords is straightforward: Creating a recommender model from this is correspondingly simple.', 'We find the keyword list for the given inputted movie, compute the Jaccard similarity between that list and every other movie keyword list and then rank the movies by their similarities and return the top “n” results.', 'In code: Testing this with our favourite movie, we get back good results: Next Steps There are a number of ways we can improve the models as they currently exist.', 'At the top of the list is deploying the models with Flask so people can use them.', 'Usability is pretty important.', 'The next step would be to find some way of measuring performance for our models.', 'This is tricky because recommender systems often have no obvious evaluation criteria.', 'Still, we can come up with some (e.g. using user feedback to rank models), and there are good resources out there for doing so.', 'A third improvement to consider is using deep-learning methods on plot summaries with embeddings: https://tfhub.dev/google/universal-sentence-encoder/2.', 'These methods should allow us to incorporate context into our recommendations, rather than simple individual words like our models currently use.', 'Lastly, the models could be improved by incorporating other non-text features such as genre or numeric features.', 'Check back in this space in a few weeks time.', 'I might do another blog post where I implement these improvements.', 'Please let me know what you think of the project and if you have any recommendations for improvement.', 'All constructive comments are much appreciated.', 'References For inspiration for the use of the Gensim MatrixSimilarity class to compare documents, I used O’Rielly’s wonderful tutorial “How do I compare document similarity using Python”: https://www.oreilly.com/learning/how-do-i-compare-document-similarity-using-python For inspiration for the use of Jaccard and cosine similarity recommenders, I am indebted to Sanket Gupta for his tutorial “Overview of Text Similarity Metrics in Python”: https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50']\n",
      "\n",
      "\n",
      "\n",
      "['Using Data Science to save money on my next trip to Mexico How am I using basic data work to ensure I am getting a good price on my trip.', 'Marc-Olivier Arsenault · Follow Published in Towards Data Science · 6 min read · Oct 28, 2019 -- 1 Listen Share It has been 4 years since my wife and I took some vacation in a sunny place.', 'Last time, for our honeymoon, we spent some quality time in Mexico.', 'We enjoyed 10 days in a very nice all-inclusive resort in Riviera Maya.', 'Since then, a house, two kids, a new job and many other things.', 'After some reflexion we decided that it was time to go back on the beach.', 'So, next December (2019) we (my wife, our 3 years old, our 4 months old and I) will be heading to Riviera Maya once again.', 'Don’t worry, I am not turning my Data blog into a travel and lifestyle blog.', 'I want to share with you how I am making sure I am getting the right price for the trip.', 'So here is where it is interesting, when we signed the contract this summer (in June) the contract said, if the price go down, I could, once, ask for a price match.', 'Since the trip was +-6 months away, that looked like a very interesting feature.', 'BTW, this is one of the factors that made us pick that travel company.', 'To be upfront with the numbers, we paid 4317 CAD$ for the full family.', 'The no so easy part After a couple of weeks, I went back to check the price.', 'It was still the same.', 'Then I realize, how can I track the price.', 'There is no way I can take the time to go, click through a series of web interfaces to query the price.', 'That would represent an annoying 2–3 minutes a day and I just don’t have that time.', 'Here is what it looks like to get the price update: The full process of getting the price update.', 'Obviously, the travel company does not feel like it would be a great feature to track the price.', 'After a couple of time searching for it, there is just no way to do this.', 'At least with a tool on the website.', 'Then what if it is below my original price, do I take this price or I wait a bit more.', 'Because remember, I can only do a price match once.', 'It would be really useful to have all the historic prices, then if/when it gets below original price, I could look at the trend and take a more informed decision.', 'So now I would need to spend couple of minutes daily to fetch the price, and then couple of more to copy the value into some sort of spreadsheet.', 'Anyone that did something like this knows that it works kinda of OK for the first few days, maybe weeks.', 'But at some point you start missing days, do copy paste error, etc.', 'Manual process in data is more or less the equivalent of no process.', 'Just script it.', 'My idea was, I can just get the URL, download the HTML from python or something and do some regex magic to extract the price.', 'Of course, it could not be that easy.', 'First thing, the URL does not really change.', 'All of the price things are some sort of modal on top of the other page.', 'So copying the URL basically brings you back to the home page.', 'Now when I started to look at Chrome Developer tools, I assumed I could see somewhere the data coming in.', 'Data has to come in right… right… After a couple of times digging in each of these files, I found the golden nugget.', 'Seems like we are on the right track.', 'We have what seems to be a JSON file and the custom URL to get it.', 'OBVIOUSLY, when I use this URL directly in a new page it is not working.', 'I receive the equivalent of a page unavailable.', 'Really it seems like they don’t want us to do this.', 'They have set many roadblocks to prevent us from doing it.', 'Thanks Obama.', 'Then, I found a very neat option in chrome Dev tools.', 'Copy as cURL.', 'You end up with a very long command you can paste in your terminal and get the JSON.', 'Finally, something is working.', 'I now have a way to extract the price.', 'As you can see the query is pretty nasty.', 'Really, it is like if they do not want us to extract the prices automatically.', 'At least, now, we have a pretty nice dictionary to work with.', 'What to do with this value now Now that we can access the value, how do we extract it.', 'I have decided to create a AWS Lambda function that run every 6 hours to extract the price.', 'With this price, 4 times a day, we do 3 things: We check if price is good.', 'Since I paid a bit more than 4300, if price would go below 4k$, I send myself an email.', 'To make sure I can act quickly if needed.', 'I store the value (with the timestamp) in a database. (', 'DynamoDB) I store the value (with the timestamp) in AWS S3 Why 2 and 3, I was not sure how I would use it, since AWS have some rules about what can access what and because storage is shockingly cheap, I stored it twice.', 'The price So sadly, the price did not go below 4300$ yet, and to be fair I doubt it will.', 'The trip is now priced at 4700/5000 depending on the days.', 'To build this view, I have used the fantastic tool call Dash which allows you with fewer than 100 lines of code build this visual.', 'If you are interested, you can go see the app here: https://voyage.coffeeanddata.ca.', 'Discoveries For a couple of days I did not collect any new data points.', 'In fact the cURL command was getting back a 404 from the server.', 'Took me a while to realize that I had no new data.', 'Because I did not implement any validations in my code.', 'The Lambda script would simply silently fail and I would not collect new data points.', 'So I added a validation in my code that sends me an email when the cookie needed to be updated.', 'I assume the cookie has some expiration encrypted in it.', 'So simply getting a new token seems enough.', 'Pick wisely The other interesting points I got from the data up to now is this part of the curve: For a couple of days in a row, the price surged by 200$ at around 7am in the morning.', 'This is something I will be careful with, next time I look to book a trip.', 'Conclusion Sadly, I did not save any money…yet.', 'I am flying for Mexico in more than 2 months so I will keep tracking the price on my website.', 'Travel companies seems to be making some effort to prevent us to automatically extracting the price.', 'The price looks to be very volatile, I would be curious to understand what influence the price on an hourly basis.']\n",
      "\n",
      "\n",
      "\n",
      "['First thing first, what is TensorFrames?', 'TensorFrames is an open source created by Apache Spark contributors.', 'Its functions and parameters are named the same as in the TensorFlow framework.', 'Under the hood, it is an Apache Spark DSL (domain-specific language) wrapper for Apache Spark DataFrames.', 'It allows us to manipulate the DataFrames with TensorFlow functionality.', 'And no, it is not pandas DataFrame, it is based on Apache Spark DataFrame. ..', 'but wait, what is TensorFlow (TF)?', 'TensorFlow is an open-source software library for dataflow and differentiable programming across a range of tasks.', 'It is a symbolic math library and is also used for machine learning applications such as neural networks. ..', 'and Apache Spark?', 'Apache Spark is an open-source distributed general-purpose cluster-computing framework.', 'A word about scale Today when we mention scale, we usually talk about two options; scale horizontally, and scaling vertically.', 'Horizontal scale — add additional machines with more or less the same computing power — add additional machines with more or less the same computing power Vertical scale — adding more resources to machine/s we are currently working with.', 'It can be a processor upgraded from a CPU to GPU, more memory (RAM), and etc.', 'With TensorFrames, we can do both, more processor computing power, and more machines.', 'Where with only TensorFlow we would usually focus on adding more power through scaling vertically, now with Apache Spark support, we can scale both vertically and horizontally.', 'But, how do we know how much of each we actually need?', 'To answer this question, we need to understand the full usage of our applications and plan accordingly.', 'For each change, like adding a machine or upgrading from CPU to GPU, we have downtime.', 'In the cloud, resizing a cluster or adding more compute power, is a matter of minutes, versus on-prem where we need to deal with adding new machines and upgrading machines processors, this can take days, and sometimes months.', 'So, A more flexible solution is the public cloud.', 'In the picture below, scale horizontally is the X-axis where scale vertically is the Y-axis. **', 'Slide from Tim Hunter presentation at Apache Spark conf Before jumping to the functions, let’s understand some important TensorFlow vocabulary: Tensor A statically typed multi-dimensional array whose elements are of a generic type.', 'GraphDef Graph or Computional Graph is the core concept of TensorFlow to present computation.', 'When we use TensorFlow, we first create our own Computation Graph and pass the Graph to TensorFlow.', 'GraphDf is the serialized version of Graph .', 'Operation A Graph node that performs computation on Tensors.', 'An Operation is a node in a Graph that takes zero or more Tensors (produced by other Operations in the Graph) as input and produces zero or more Tensor s as output.', 'Identity tf.identity is used when we want to explicitly transport tensor between devices (like, from GPU to a CPU).', 'The operation adds nodes to the graph, which makes a copy when the devices of the input and the output are different.', 'Constant A constant has the following arguments which can be tweaked as required to get the desired function.', 'It the same as a variable, but its value can’t be changed.', 'Constant can be: value : A constant value (or list) of output type dtype . :', 'A constant value (or list) of output type .', 'dtype : The type of the elements of the resulting tensor. :', 'The type of the elements of the resulting tensor.', 'shape : Optional dimensions of resulting tensor. :', 'Optional dimensions of resulting tensor.', 'name : Optional name for the tensor. :', 'Optional name for the tensor.', 'verify_shape : Boolean that enables verification of a shape of values.', 'Placeholders Allocate storage for data (such as for image pixel data during a feed).', 'Initial values are not required (but can be set, see tf.placeholder_with_default ).', 'Versus variables, where you need to declare the initial value. \\\\', 'Some Apache Spark Vocabulary Dataframe This is a distributed collection of data organized into named columns that provide operations to filter, group, or compute aggregates.', 'Dataframe data is often distributed across multiple machines.', 'It can be in memory data or on disk.', 'RelationalGroupedDataset A set of methods for aggregations on a DataFrame , created by groupBy, cube or rollup.', 'The main method is the agg function, which has multiple variants.', 'This class also contains some first-order statistics such as mean , sum for convenience.', 'Now that we understand the terminology better, let’s look at the functionality.', 'The Functionality — TensorFlow version 0.6.0 Apache Spark is known for being an analytics platform for data at scale, together with TensorFlow, we get TensorFrames which have three categories of data manipulations: Let’s understand each functionality.', '-1- Mapping Mapping operations transform and/or adds columns to a given dataframe.', 'Each functionality is accessed through two API, one which receives Operation and the other which receives DataFrame, GraphDef, and ShapeDescription.', 'Exposed API: MapRows def mapRows(o0: Operation, os: Operation*): DataFrame For the user, this is the function that will be more often in use, since there is no direct request to create the GraphDef and ShapeDescription object.', 'This way is more readable for experienced TensorFlow developers: mapRows receives two parameters, operation, and operation* which means the second operation can be a collection of operations.', 'Later it turns them into a sequence and translates it into a graph, it creates the ShapeDiscription out of the graph and sends it with the DataFrame to an internal function.', 'Where it transforms the distributed data row by row according to the transformations given in the graph.', 'All input in the graph should be filled with some data from the given DataFrame or constants.', 'Meaning, we can’t use null.', 'At the end the function returns a new DataFrame with the new schema, the schema will contain the original schema plus new columns that correspond to the graph output.', 'ShapeDiscription provides the shape of the output, it is used, behind the scenes, for optimization and going around limitations of the kernel.', 'MapBlock Performs a similar task as MapRows , however, since it is optimized for compact, it applies the graph transformers in blocks of data and not row by row.', 'def mapBlocks(o0: Operation, os: Operation*): DataFrame The often more used function is: Code example: We create val df, which is of type DataFrame, with two rows, one contains value 1.0 and the second data row contain value 2.0.', 'The column name is x. val x is a declaration of the placeholder for the output, y is the identity for transporting tensors from CPU to GPU or from machine to machine, it received val x as it’s value.', 'z is the computation function itself.', 'Here, df.', 'MapBlock functions gets two operations, y and z, and retunes a new DataFrame named df2 with extra column z. Column z is the sum of x+x.', 'In the output, column x is the original value, column y is the identity value and column z is the output of the graph.', 'MapBlocksTrimmed This is the same as MapBlock , BUT, it drops the original DataFrame columns from the result DataFrame.', 'Meaning the output DataFrame will contain only the calculated columns.', 'def mapBlocksTrimmed(o0: Operation, os: Operation*): DataFrame Let’s look at: Code example: we create a DataFrame named df with two rows with values 3.0 and 4.0 .', 'Notice that we create a constant named out with value 1.0 and 2.0, this constant is the TensorFrame dsl functionality that mimics the TensorFlow functionality.', 'Then we call df.', 'MapBlocksTrimmed .', 'The output schema will only contain the result column, which is named \"out\" and in our case will only hold the constant values which are 1.0 and 2.0 .', 'Important Note in the first line of code we import TesnorFrames dsl and we name it to tf, which stands for TensorFlow, we do it since this is how TesnorFlow users used to work with it and we are adhering to the best practices of TensorFlow.', '-2- Reducing Reduction operations coalesce a pair or a collection of rows and transform them into a single row, it repeats the same operation until there is one row left.', 'Under the hood, TensorFrames minimizes the data transfer between computers by reducing all the rows on each computer first and then sending the remainder over the network to perform the last reductions.', 'f(f(a, b), c) == f(a, f(b, c)) The transforms function must be classified as morphism: the order in which they are done should not matter.', 'In mathematical terms, given some function f and some function inputs a , b , c , the following must hold: Map reduce schema by Christopher Scherb The reduce functionality API, same as the rest, we have 2 API for each functionality, the one which receives Operation is more intuitive, however, in TensorFlow there is no direct reduce rows operation, instead, there are many reduce operations like tf.math.reduce_sum and tf.reduce_sum .', 'ReduceRows This functionality uses TensorFlow operations to merge two rows together until there is one row left.', 'It receives the datafram, graph, and a ShapeDescription.', 'def reduceRows(o0: Operation, os: Operation*): Row User interface: In the next code example.', 'We create a DataFrame with a column named in and two rows.', 'x1 and x2 placeholder for dtype and x- which is an add operation of x1 and x2.', 'reduceRows, return a Row with value 3 which is the sum of 1.0 and 2.0.', 'ReduceBlocks Works the same as ReduceRows , BUT, it does it on a vector of rows and not row by row.', 'def reduceBlocks(o0: Operation, os: Operation*): Row More used function: Code example: Here we create a DataFrame with two columns — key2 and x. One placeholder names x1, one reduce_sum TensorFlow operation named x. The reduce functionality return the sum of the rows in the DataFrame according to the desired columns that the reduce_sum named after which is x. -3- Aggregation def aggregate(data: RelationalGroupedDataset, graph: GraphDef, shapeHints: ShapeDescription): DataFrame Aggregation is an extra operation for Apache Spark and TensorFlow.', 'It is different from the aggregation functionality in TensorFlow and works with RelationalGroupedDataset.', 'API functionality: Aggregate receives a RelationalGroupedDataset which is an Apache Spark object, it wraps DataFrame and adds aggregation functionality, a sequence of expressions and a group type.', 'The aggregate function receives the graph and ShareDescriptiom.', 'It aggregates rows together using reducing transformation on grouped data.', 'This is useful when data is already grouped by key.', 'At the moment, only numerical data is supported.', 'Code example: In the example, we have a DataFrame with two columns, key, and x. x1 as a placeholder and x as the reduce_sum functionality named x. Using groupby functionality we group the rows by key, and after it, we call aggregate with the operations.', 'We can see in the output that the aggregated was calculated according to the key, for the key with value 1- we received 2.1 as the value for column x and for the key with value 2 we received 2.0 as the value for column x. TensorFrames basic process In all TensorFrames functionality, the DataFrame is sent together with the computations graph.', 'The DataFrame represents the distributed data, meaning in every machine there is a chunk of the data that will go through the graph operations/ transformations.', 'This will happen in every machine with the relevant data.', 'Tungsten binary format is the actual binary in-memory data that goes through the transformation, first to Apache Spark Java object and from there it is sent to TensorFlow Jave API for graph calculations.', 'This all happens in the Spark Worker process, the Spark worker process can spin many tasks which mean various calculation at the same time over the in-memory data.', 'Noteworthy DataFrames with scala is currently an experimental version.', 'an experimental version.', 'The Scala DSL only features a subset of TensorFlow transforms.', 'TensorFrames is open source and can be supported here.', 'Python was the first client language supported by TensorFlow and currently supports the most features.', 'More and more of that functionality is being moved into the core of TensorFlow (implemented in C++) and exposed via a C API.', 'Which later exposed through other languages API, such as Java and JavaScript.', 'Interested in working with Keras?', 'check out Elephas: Distributed Deep Learning with Keras & Spark.', 'interested in TensorFrames project on the public cloud?', 'check this and this.', 'Now that you know more about TensorFrames, how will you take it forward?', 'Follow me on Twitter, happy to take your suggestions on topics.']\n",
      "\n",
      "\n",
      "\n",
      "['What is the journey to the cloud?', 'The time is ripe, guys.', 'What we have seen in the last years was upsetting.', 'Cloud coming out as a concept and in a decade changes our lives.', 'Just think about your photos that are stored nowhere in the cloud and accessible by all places.', 'Nowadays everybody, from developers to CEOs, knows that there is an opportunity called the cloud.', 'This article, after a brief introduction to the cloud advantages, show how to move your on-prem application.', 'What are the cloud advantages?', 'At the beginning of the cloud age, the keyword for convincing people to go cloud was “it’s cheap”.', 'Well, after ten years of cloud we understood that is not a question of costs.', 'In some cases, we experienced that the cloud cost more than an on-prem solution.', 'The keywords for the cloud are flexibility, efficiency, and strategy.', 'Flexibility The most common requirement in IT projects is time to marked.', 'Nowadays everybody needs solutions soon and before the competitor.', 'This continuous hurry conflict with usual timing to implement on-prem solutions.', 'Moreover, you need to experiment with a lot of solutions to be competitive.', 'This means that not all the projects you will finish will see the end or long and prolific life.', 'But hardware cannot be simply resold at the end, and times of sysadmin never come back.', 'In this scenario, the Cloud helps us.', 'It is flexible, so you can scale and shrink based on your needs.', 'Moreover, you are sure that, if you have enough money to buy it, there will be enough resources for you.', 'Efficiency The on-prem solution is hard to maintain a need a lot of effort to keep it efficient.', 'You need to monitor the physical data center, monitor the infrastructure, get the best contracts whit ISP and pay for unuseful resources like redundancy hardware.', 'You also have to pay for security aspects like intrusion tests, insurance and so on.', 'Basically, the cloud provider is more efficient than a single company, that because this is their job.', 'They capitalize on costs and can afford better security checks.', 'Moreover thinks to the most valuable services like AI API.', 'How many companies can hire a machine learning engineer to ask him to solve an image classification problem?', 'Maybe is quicker and cheaper at the end pay for an expensive ready to use service.']\n",
      "\n",
      "\n",
      "\n",
      "['Why Harley?', 'B/C Data Science Rocks!', 'Introduction In a series of posts (why experiments, two-way causal direction, pitfalls, correlation & causation, and Natural experiments), we have covered topics including what, why, and how to conduct experimentation.', 'Regardless of how desirable they appear to be, it is impossible to run experiments for all types of business questions, for various reasons.', 'It could be unethical to do so.', 'Let’s say we are interested in the effect of undergraduate education on students’ future earnings.', 'It would be fundamentally wrong to randomly assign some high students to receive the education and not to others.', 'Or, it could be expensive, time-consuming, or technically infeasible.', 'Even top companies like Netflix and Airbnb can’t guarantee the internal validity of randomization at the individual user level all the time.', 'Under these scenarios, we have to rely on other methods, both observational and quasi-experimental designs, to derive causal effects.', 'As an established quasi-experimental technique, Regress Discontinuity Design, RDD, has been through a long period of dormancy and comes back strong until recently.', 'In this post, we elaborate on RDD’s underlying constructs, such as research ideology, statistical assumptions, potential outcomes framework (POF), merits, limitations, and R illustration.', 'What is RDD?', 'First off, it is a quasi-experimental method with a pretest-posttest design, implying that researchers administer the measure of interest before and after the intervention (treatment).', 'By setting up a “cutoff” point, we select the subjects slightly above the threshold line as the treatment group and the ones slightly below the line as the control group.', 'Since these two groups are geographically close to each other, we can control potential confounding variables and treat it as an as-if random treatment assignment.', 'If there is any difference in the…']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:35:17.250816Z",
     "start_time": "2024-05-06T19:35:17.248280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_sentences_in_chunk = 10\n",
    "CONTEXT_WINDOW = 384\n",
    "TOKEN_SIZE = 4\n",
    "\n",
    "\n",
    "def split_text_into_chunks(text: str, n_sentences_in_chunk: int, overlap=0) -> list:\n",
    "    doc = nlp(text)\n",
    "    sents = [sent.text for sent in doc.sents]\n",
    "    chunks = []\n",
    "    n_sentences_in_chunk = n_sentences_in_chunk - overlap\n",
    "    remainder = 0\n",
    "    for i in range(overlap, len(sents), n_sentences_in_chunk):\n",
    "        if remainder == n_sentences_in_chunk:\n",
    "            remainder = 0\n",
    "        chunk = sents[i-overlap-remainder:i + n_sentences_in_chunk-remainder]\n",
    "        remainder = 0\n",
    "        while len(\" \".join(chunk)) / TOKEN_SIZE > CONTEXT_WINDOW:\n",
    "            remainder += 1\n",
    "            chunk = chunk[:-remainder]\n",
    "            \n",
    "        chunk = \" \".join(chunk)\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ],
   "id": "977a964a77580255",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:35:20.033299Z",
     "start_time": "2024-05-06T19:35:17.251297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dict_data = data.to_dict(\"records\")  \n",
    "\n",
    "for elem in tqdm(dict_data):\n",
    "    elem[\"chunks\"] = split_text_into_chunks(elem[\"Text\"], n_sentences_in_chunk, overlap=1)"
   ],
   "id": "c6a107f76d07d0f9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1391/1391 [00:02<00:00, 500.85it/s]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Example of chunking"
   ],
   "id": "2b4f31665d15786b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:35:20.035987Z",
     "start_time": "2024-05-06T19:35:20.033912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for chunk in dict_data[0][\"chunks\"]:\n",
    "    print(len(chunk) / 4)\n",
    "    print(chunk)\n",
    "    print(\"\\n\\n\")"
   ],
   "id": "8434fbb942475dae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285.0\n",
      "1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.\n",
      "\n",
      "\n",
      "\n",
      "293.5\n",
      "For instance, it will have two vector representations for Apple, one for the company and another for the fruit. For more details about the word2vec algorithm, please check here. 2. Gensim Python Library Introduction Gensim is an open source python library for natural language processing and it was developed and is maintained by the Czech natural language processing researcher Radim Řehůřek. Gensim library will enable us to develop word embeddings by training our own word2vec models on a custom corpus either with CBOW of skip-grams algorithms. At first, we need to install the genism package. Gensim runs on Linux, Windows and Mac OS X, and should run on any other platform that supports Python 2.7+ and NumPy. Gensim depends on the following software: Python >= 2.7 (tested with versions 2.7, 3.5 and 3.6) >= 2.7 (tested with versions 2.7, 3.5 and 3.6) NumPy >= 1.11.3 >= 1.11.3 SciPy >= 0.18.1 >= 0.18.1 Six >= 1.5.0 >= 1.5.0 smart_open >= 1.2.1 There are two ways for installation. We could run the following code in our terminal to install genism package. pip install --upgrade gensim Or, alternatively for Conda environments: conda install -c conda-forge gensim 3.\n",
      "\n",
      "\n",
      "\n",
      "322.0\n",
      "pip install --upgrade gensim Or, alternatively for Conda environments: conda install -c conda-forge gensim 3. Implementation of word Embedding with Gensim Word2Vec Model In this tutorial, I will show how to generate word embedding with genism using a concrete example. The dataset I used for this tutorial is from Kaggle Dataset. This vehicle dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The full python tutorial can be found here. >>> df = pd.read_csv('data.csv') >>> df.head() 3.1 Data Preprocessing: Since the purpose of this tutorial is to learn how to generate word embeddings using genism library, we will not do the EDA and feature selection for the word2vec model for the sake of simplicity. Genism word2vec requires that a format of ‘list of lists’ for training where every document is contained in a list and every list contains lists of tokens of that document. At first, we need to generate a format of ‘list of lists’ for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains lists of features of that make model.\n",
      "\n",
      "\n",
      "\n",
      "341.0\n",
      "To be more specific, each make model is contained in a list and every list contains lists of features of that make model. To achieve this, we need to do the following things : a. Create a new column for Make Model >>> df['Maker_Model']= df['Make']+ \" \" + df['Model'] b. Generate a format of ‘ list of lists’ for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size, Vehicle Style. # Select features from original dataset to form a new dataframe >>> df1 = df[['Engine Fuel Type','Transmission Type','Driven_Wheels','Market Category','Vehicle Size', 'Vehicle Style', 'Maker_Model']] # For each row, combine all the columns into one column >>> df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1) # Store them in a pandas dataframe >>> df_clean = pd. DataFrame({'clean': df2}) # Create the list of list format of the custom corpus for gensim modeling >>> sent = [row.split(',') for row in df_clean['clean']] # show the example of list of list format of the custom corpus for gensim modeling >>> sent[:2] [['premium unleaded (required)', 'MANUAL', 'rear wheel drive', 'Factory Tuner', 'Luxury', 'High-Performance', 'Compact', 'Coupe', 'BMW 1 Series M'], ['premium unleaded (required)', 'MANUAL', 'rear wheel drive', 'Luxury', 'Performance', 'Compact', 'Convertible', 'BMW 1 Series']] 3.2.\n",
      "\n",
      "\n",
      "\n",
      "359.5\n",
      "window: The maximum distance between a target word and words around the target word. The default window is 5. min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5. workers: The number of partitions during training and the default workers is 3. sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW. After training the word2vec model, we can obtain the word embedding directly from the training model as following. >>> model['Toyota Camry'] array([-0.11884457, 0.03035539, -0.0248678 , -0.06297892, -0.01703234, -0.03832747, -0.0825972 , -0.00268112, -0.09192555, -0.08458661, -0.07199778, 0.05235871, 0.21303181, 0.15767808, -0.1883737 , 0.01938575, -0.24431638, 0.04261152, 0.11865819, 0.09881561, -0.04580643, -0.08342388, -0.01355413, -0.07892415, -0.08467747, -0.0040625 , 0.16796461, 0.14578669, 0.04187112, -0.01436194, -0.25554284, 0.25494182, 0.05522631, 0.19295982, 0.14461821, 0.14022525, -0.2065216 , -0.05020927, -0.08133671, 0.18031682, 0.35042757, 0.0245426 , 0.15938364, -0.05617865, 0.00297452, 0.15442047, -0.01286271, 0.13923576, 0.085941 , 0.18811756], dtype=float32) 4. Compute Similarities Now we could even use Word2vec to compute the similarity between two Make Models in the vocabulary by invoking the model.similarity( ) and passing in the relevant words.\n",
      "\n",
      "\n",
      "\n",
      "327.5\n",
      "We also can use the built-in function model.most_similar() to get a set of the most similar make models for a given make model based on the Euclidean distance. >>> model1.most_similar('Mercedes-Benz SLK-Class')[:5] [('BMW M4', 0.9959905743598938), ('Maserati Coupe', 0.9949707984924316), ('Porsche Cayman', 0.9945154190063477), ('Mercedes-Benz SLS AMG GT', 0.9944609999656677), ('Maserati Spyder', 0.9942780137062073)] However, Euclidian similarity cannot work well for the high-dimensional word vectors. This is because Euclidian similarity will increase as the number of dimensions increases, even if the word embedding stands for different meanings. Alternatively, we can use cosine similarity to measure the similarity between two vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at a 0-degree angle. The following function shows how can we generate the most similar make model based on cosine similarity. def cosine_distance (model, word,target_list , num) : cosine_dict ={} word_list = [] a = model[word] for item in target_list : if item !\n",
      "\n",
      "\n",
      "\n",
      "317.25\n",
      "def cosine_distance (model, word,target_list , num) : cosine_dict ={} word_list = [] a = model[word] for item in target_list : if item ! = word : b = model [item] cos_sim = dot(a, b)/(norm(a)*norm(b)) cosine_dict[item] = cos_sim dist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order for item in dist_sort: word_list.append((item[0], item[1])) return word_list[0:num] # only get the unique Maker_Model >>> Maker_Model = list(df. Maker_Model.unique()) # Show the most similar Mercedes-Benz SLK-Class by cosine distance >>> cosine_distance (model,'Mercedes-Benz SLK-Class',Maker_Model,5) [('Mercedes-Benz CLK-Class', 0.99737006), ('Aston Martin DB9', 0.99593246), ('Maserati Spyder', 0.99571854), ('Ferrari 458 Italia', 0.9952333), ('Maserati GranTurismo Convertible', 0.994994)] 5. T-SNE Visualizations It’s hard to visualize the word embedding directly, for they usually have more than 3 dimensions. T-SNE is a useful tool to visualize high-dimensional data by dimension reduction while keeping relative pairwise distance between points. It can be said that T-SNE looking for a new data representation where the neighborhood relations are preserved. The following code shows how to plot the word embedding with T-SNE plot.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Creating a dataset with chunks"
   ],
   "id": "9ce2507de8bc9739"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:35:24.009699Z",
     "start_time": "2024-05-06T19:35:20.036568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "chunks = pd.DataFrame(columns=[\"Title\", \"Chunk\", \"Chunk_length\", \"Chunk_words_num\", \"Chunk_sentences_num\", \"Token_count\"])\n",
    "\n",
    "for d in tqdm(dict_data):\n",
    "    for chunk in d[\"chunks\"]:\n",
    "        chunk = chunk.strip()\n",
    "        chunk = re.sub(r\"\\s+\", \" \", chunk)\n",
    "        row = pd.DataFrame.from_dict(\n",
    "            {\n",
    "                \"Title\": d[\"Title\"],\n",
    "                \"Chunk\": chunk,\n",
    "                \"Chunk_length\": len(chunk),\n",
    "                \"Chunk_words_num\": len(chunk.split()),\n",
    "                \"Chunk_sentences_num\": len(list(nlp(chunk).sents)),\n",
    "                \"Token_count\": len(chunk) / 4\n",
    "            },\n",
    "            orient=\"index\"\n",
    "        )\n",
    "        row = row.T\n",
    "        chunks = pd.concat([chunks, row], axis=0)\n",
    "        \n",
    "numeric = [\"Chunk_length\", \"Chunk_words_num\", \"Chunk_sentences_num\", \"Token_count\"]\n",
    "chunks[numeric] = chunks[numeric].astype(float)\n",
    "\n",
    "chunks.head()"
   ],
   "id": "cf491b4645bed9e8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1391/1391 [00:03<00:00, 350.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                               Title  \\\n0  A Beginner’s Guide to Word Embedding with Gens...   \n0  A Beginner’s Guide to Word Embedding with Gens...   \n0  A Beginner’s Guide to Word Embedding with Gens...   \n0  A Beginner’s Guide to Word Embedding with Gens...   \n0  A Beginner’s Guide to Word Embedding with Gens...   \n\n                                               Chunk  Chunk_length  \\\n0  1. Introduction of Word2vec Word2vec is one of...        1140.0   \n0  For instance, it will have two vector represen...        1174.0   \n0  pip install --upgrade gensim Or, alternatively...        1288.0   \n0  To be more specific, each make model is contai...        1364.0   \n0  window: The maximum distance between a target ...        1438.0   \n\n   Chunk_words_num  Chunk_sentences_num  Token_count  \n0            188.0                 10.0        285.0  \n0            194.0                 10.0        293.5  \n0            215.0                 10.0        322.0  \n0            204.0                  4.0        341.0  \n0            185.0                 10.0        359.5  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Chunk</th>\n      <th>Chunk_length</th>\n      <th>Chunk_words_num</th>\n      <th>Chunk_sentences_num</th>\n      <th>Token_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>1. Introduction of Word2vec Word2vec is one of...</td>\n      <td>1140.0</td>\n      <td>188.0</td>\n      <td>10.0</td>\n      <td>285.0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>For instance, it will have two vector represen...</td>\n      <td>1174.0</td>\n      <td>194.0</td>\n      <td>10.0</td>\n      <td>293.5</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>pip install --upgrade gensim Or, alternatively...</td>\n      <td>1288.0</td>\n      <td>215.0</td>\n      <td>10.0</td>\n      <td>322.0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>To be more specific, each make model is contai...</td>\n      <td>1364.0</td>\n      <td>204.0</td>\n      <td>4.0</td>\n      <td>341.0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>window: The maximum distance between a target ...</td>\n      <td>1438.0</td>\n      <td>185.0</td>\n      <td>10.0</td>\n      <td>359.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:35:24.014042Z",
     "start_time": "2024-05-06T19:35:24.010472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunks.sample(1)"
   ],
   "id": "3b943d25bf1b0980",
   "outputs": [
    {
     "data": {
      "text/plain": "                            Title  \\\n0  Data analytics with MODIS data   \n\n                                               Chunk  Chunk_length  \\\n0  Variation of Combined AOD and Cloud Fraction o...        1085.0   \n\n   Chunk_words_num  Chunk_sentences_num  Token_count  \n0            186.0                 10.0       271.25  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Chunk</th>\n      <th>Chunk_length</th>\n      <th>Chunk_words_num</th>\n      <th>Chunk_sentences_num</th>\n      <th>Token_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Data analytics with MODIS data</td>\n      <td>Variation of Combined AOD and Cloud Fraction o...</td>\n      <td>1085.0</td>\n      <td>186.0</td>\n      <td>10.0</td>\n      <td>271.25</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:35:24.021625Z",
     "start_time": "2024-05-06T19:35:24.014789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunks.describe().astype(float).round(2)"
   ],
   "id": "182c909727869b70",
   "outputs": [
    {
     "data": {
      "text/plain": "       Chunk_length  Chunk_words_num  Chunk_sentences_num  Token_count\ncount       7334.00          7334.00              7334.00      7334.00\nmean        1052.25           173.26                 8.80       263.06\nstd          331.35            54.58                 2.16        82.84\nmin            0.00             0.00                 0.00         0.00\n25%          865.00           144.00                 9.00       216.25\n50%         1100.00           182.00                10.00       275.00\n75%         1313.00           213.00                10.00       328.25\nmax         1536.00           291.00                10.00       384.00",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Chunk_length</th>\n      <th>Chunk_words_num</th>\n      <th>Chunk_sentences_num</th>\n      <th>Token_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>7334.00</td>\n      <td>7334.00</td>\n      <td>7334.00</td>\n      <td>7334.00</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1052.25</td>\n      <td>173.26</td>\n      <td>8.80</td>\n      <td>263.06</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>331.35</td>\n      <td>54.58</td>\n      <td>2.16</td>\n      <td>82.84</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>865.00</td>\n      <td>144.00</td>\n      <td>9.00</td>\n      <td>216.25</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1100.00</td>\n      <td>182.00</td>\n      <td>10.00</td>\n      <td>275.00</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1313.00</td>\n      <td>213.00</td>\n      <td>10.00</td>\n      <td>328.25</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1536.00</td>\n      <td>291.00</td>\n      <td>10.00</td>\n      <td>384.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:35:24.026207Z",
     "start_time": "2024-05-06T19:35:24.024144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunks = chunks[chunks[\"Token_count\"] > 0]"
   ],
   "id": "84904f60068b9204",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:35:24.033453Z",
     "start_time": "2024-05-06T19:35:24.026755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunks.describe()"
   ],
   "id": "a84e3540a747eec4",
   "outputs": [
    {
     "data": {
      "text/plain": "       Chunk_length  Chunk_words_num  Chunk_sentences_num  Token_count\ncount   7292.000000      7292.000000          7292.000000  7292.000000\nmean    1058.311437       174.255623             8.854361   264.577859\nstd      322.506653        53.123813             2.061306    80.626663\nmin        9.000000         2.000000             1.000000     2.250000\n25%      871.000000       145.000000             9.000000   217.750000\n50%     1101.000000       182.000000            10.000000   275.250000\n75%     1314.000000       213.250000            10.000000   328.500000\nmax     1536.000000       291.000000            10.000000   384.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Chunk_length</th>\n      <th>Chunk_words_num</th>\n      <th>Chunk_sentences_num</th>\n      <th>Token_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>7292.000000</td>\n      <td>7292.000000</td>\n      <td>7292.000000</td>\n      <td>7292.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1058.311437</td>\n      <td>174.255623</td>\n      <td>8.854361</td>\n      <td>264.577859</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>322.506653</td>\n      <td>53.123813</td>\n      <td>2.061306</td>\n      <td>80.626663</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>9.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>2.250000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>871.000000</td>\n      <td>145.000000</td>\n      <td>9.000000</td>\n      <td>217.750000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1101.000000</td>\n      <td>182.000000</td>\n      <td>10.000000</td>\n      <td>275.250000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1314.000000</td>\n      <td>213.250000</td>\n      <td>10.000000</td>\n      <td>328.500000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1536.000000</td>\n      <td>291.000000</td>\n      <td>10.000000</td>\n      <td>384.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Printing some chunks with low token count"
   ],
   "id": "789065d133527bc7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:35:24.036648Z",
     "start_time": "2024-05-06T19:35:24.034133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MIN_TOKENS = 50\n",
    "\n",
    "for ch in chunks[chunks[\"Token_count\"] < 50].sample(5)[\"Chunk\"].values:\n",
    "    print(ch)\n",
    "    print(\"\\n\\n\")\n",
    "\n"
   ],
   "id": "fcedfd78f12eea02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If your course if making some of these mistakes you might be able to help save your education by encouraging some better practices.) Part 1 — Why Learning…\n",
      "\n",
      "\n",
      "\n",
      "That day is coming. Trust me, I’m human. This post was adapted from my book: Trust Me, I’m A Bot — Building Digital Trust Using Conversational AI\n",
      "\n",
      "\n",
      "\n",
      "After this, you just have to feed them and wait for finishing the learning. Thanks for reading, see you in the next article. — — — — — References\n",
      "\n",
      "\n",
      "\n",
      "Also, follow me on Twitter and LinkedIn. Cheers!\n",
      "\n",
      "\n",
      "\n",
      "This is sometimes called the ‘Bell Curve’ or the ‘Gaussian Curve’. A simple way to do this is to determine the normality of each variable separately using the Shapiro-Wilk Test.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we can see most of these short sentences come from the end of the articles. We can safely remove them from the dataset as they bring little information to the table."
   ],
   "id": "d5a5f2674e24c2cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:35:24.039794Z",
     "start_time": "2024-05-06T19:35:24.037373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(chunks.shape)\n",
    "chunks = chunks[chunks[\"Token_count\"] > MIN_TOKENS]\n",
    "print(chunks.shape)"
   ],
   "id": "3849783fd100b09e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7292, 6)\n",
      "(7185, 6)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Embedding the chunks"
   ],
   "id": "127c33dbe4ad5909"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d20d0d2861f3ef57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:35:28.707137Z",
     "start_time": "2024-05-06T19:35:24.040234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "embeddings_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "sentences = [\"This is a sample sentence\", \"I like to eat apples\"]\n",
    "embeddings = embeddings_model.encode(sentences)\n",
    "cos_sim = np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]))\n",
    "cos_sim"
   ],
   "id": "6ea949ed430b97dc",
   "outputs": [
    {
     "data": {
      "text/plain": "0.1260562"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Enabling CUDA if possible"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "276cd8e10b12b68f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T19:35:28.709934Z",
     "start_time": "2024-05-06T19:35:28.707802Z"
    }
   },
   "id": "54f65a30f42a10",
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:48:37.517683Z",
     "start_time": "2024-05-06T19:35:28.710657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embeddings_model.to(device)\n",
    "chunks_dict = chunks.to_dict(\"records\")\n",
    "\n",
    "for chunk in tqdm(chunks_dict):\n",
    "    embedding = embeddings_model.encode(chunk[\"Chunk\"])\n",
    "    chunk[\"Embedding\"] = embedding"
   ],
   "id": "7b63ce8600a0969",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7185/7185 [13:08<00:00,  9.11it/s]   \n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  Title  \\\n0     A Beginner’s Guide to Word Embedding with Gens...   \n1     A Beginner’s Guide to Word Embedding with Gens...   \n2     A Beginner’s Guide to Word Embedding with Gens...   \n3     A Beginner’s Guide to Word Embedding with Gens...   \n4     A Beginner’s Guide to Word Embedding with Gens...   \n...                                                 ...   \n7180  Primer on The Importance of Mindful Data Colle...   \n7181  Primer on The Importance of Mindful Data Colle...   \n7182  Primer on The Importance of Mindful Data Colle...   \n7183  Primer on The Importance of Mindful Data Colle...   \n7184  Primer on The Importance of Mindful Data Colle...   \n\n                                                  Chunk  Chunk_length  \\\n0     1. Introduction of Word2vec Word2vec is one of...        1140.0   \n1     For instance, it will have two vector represen...        1174.0   \n2     pip install --upgrade gensim Or, alternatively...        1288.0   \n3     To be more specific, each make model is contai...        1364.0   \n4     window: The maximum distance between a target ...        1438.0   \n...                                                 ...           ...   \n7180  This is true of any research institution, even...        1387.0   \n7181  The issues of differing standards can ultimate...        1435.0   \n7182  The above excerpt comes from the abstract to t...         915.0   \n7183  Show me your final data. This is all extremely...         981.0   \n7184  Is the information that you have fully anonymi...         870.0   \n\n      Chunk_words_num  Chunk_sentences_num  Token_count  \\\n0               188.0                 10.0       285.00   \n1               194.0                 10.0       293.50   \n2               215.0                 10.0       322.00   \n3               204.0                  4.0       341.00   \n4               185.0                 10.0       359.50   \n...               ...                  ...          ...   \n7180            227.0                 10.0       346.75   \n7181            226.0                  9.0       358.75   \n7182            153.0                 10.0       228.75   \n7183            167.0                 10.0       245.25   \n7184            144.0                  8.0       217.50   \n\n                                              Embedding  \n0     [0.040338237, 0.012804469, -0.006745447, 0.042...  \n1     [0.05746932, 0.0059202667, -0.01848462, 0.0432...  \n2     [-0.008038099, 0.018741773, -0.010320838, 0.06...  \n3     [0.028384276, 0.01910937, -0.023285013, 0.0194...  \n4     [0.028748535, -0.047673915, 0.012459073, 0.064...  \n...                                                 ...  \n7180  [0.04206838, 0.033875473, -0.034986444, -0.038...  \n7181  [0.03998078, 0.11374257, -0.031096485, -0.0056...  \n7182  [0.04979147, 0.06545234, -0.033590827, -0.0121...  \n7183  [0.01828431, 0.087652445, -0.06963481, 0.00219...  \n7184  [-0.0077653565, 0.13505033, -0.03715556, 0.018...  \n\n[7185 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Chunk</th>\n      <th>Chunk_length</th>\n      <th>Chunk_words_num</th>\n      <th>Chunk_sentences_num</th>\n      <th>Token_count</th>\n      <th>Embedding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>1. Introduction of Word2vec Word2vec is one of...</td>\n      <td>1140.0</td>\n      <td>188.0</td>\n      <td>10.0</td>\n      <td>285.00</td>\n      <td>[0.040338237, 0.012804469, -0.006745447, 0.042...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>For instance, it will have two vector represen...</td>\n      <td>1174.0</td>\n      <td>194.0</td>\n      <td>10.0</td>\n      <td>293.50</td>\n      <td>[0.05746932, 0.0059202667, -0.01848462, 0.0432...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>pip install --upgrade gensim Or, alternatively...</td>\n      <td>1288.0</td>\n      <td>215.0</td>\n      <td>10.0</td>\n      <td>322.00</td>\n      <td>[-0.008038099, 0.018741773, -0.010320838, 0.06...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>To be more specific, each make model is contai...</td>\n      <td>1364.0</td>\n      <td>204.0</td>\n      <td>4.0</td>\n      <td>341.00</td>\n      <td>[0.028384276, 0.01910937, -0.023285013, 0.0194...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>window: The maximum distance between a target ...</td>\n      <td>1438.0</td>\n      <td>185.0</td>\n      <td>10.0</td>\n      <td>359.50</td>\n      <td>[0.028748535, -0.047673915, 0.012459073, 0.064...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7180</th>\n      <td>Primer on The Importance of Mindful Data Colle...</td>\n      <td>This is true of any research institution, even...</td>\n      <td>1387.0</td>\n      <td>227.0</td>\n      <td>10.0</td>\n      <td>346.75</td>\n      <td>[0.04206838, 0.033875473, -0.034986444, -0.038...</td>\n    </tr>\n    <tr>\n      <th>7181</th>\n      <td>Primer on The Importance of Mindful Data Colle...</td>\n      <td>The issues of differing standards can ultimate...</td>\n      <td>1435.0</td>\n      <td>226.0</td>\n      <td>9.0</td>\n      <td>358.75</td>\n      <td>[0.03998078, 0.11374257, -0.031096485, -0.0056...</td>\n    </tr>\n    <tr>\n      <th>7182</th>\n      <td>Primer on The Importance of Mindful Data Colle...</td>\n      <td>The above excerpt comes from the abstract to t...</td>\n      <td>915.0</td>\n      <td>153.0</td>\n      <td>10.0</td>\n      <td>228.75</td>\n      <td>[0.04979147, 0.06545234, -0.033590827, -0.0121...</td>\n    </tr>\n    <tr>\n      <th>7183</th>\n      <td>Primer on The Importance of Mindful Data Colle...</td>\n      <td>Show me your final data. This is all extremely...</td>\n      <td>981.0</td>\n      <td>167.0</td>\n      <td>10.0</td>\n      <td>245.25</td>\n      <td>[0.01828431, 0.087652445, -0.06963481, 0.00219...</td>\n    </tr>\n    <tr>\n      <th>7184</th>\n      <td>Primer on The Importance of Mindful Data Colle...</td>\n      <td>Is the information that you have fully anonymi...</td>\n      <td>870.0</td>\n      <td>144.0</td>\n      <td>8.0</td>\n      <td>217.50</td>\n      <td>[-0.0077653565, 0.13505033, -0.03715556, 0.018...</td>\n    </tr>\n  </tbody>\n</table>\n<p>7185 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = pd.DataFrame(chunks_dict)\n",
    "chunks"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T19:48:37.540771Z",
     "start_time": "2024-05-06T19:48:37.518989Z"
    }
   },
   "id": "f79874853b2ade9f",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chunks.to_csv(\"../data/chunks_embedded.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T19:48:50.205297Z",
     "start_time": "2024-05-06T19:48:37.541472Z"
    }
   },
   "id": "b35421e235108c3b",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.'"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks = [chunk[\"Chunk\"] for chunk in chunks_dict]\n",
    "text_chunks[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T19:48:50.208525Z",
     "start_time": "2024-05-06T19:48:50.205986Z"
    }
   },
   "id": "605663d25870fcda",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/225 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4609ed6d2859440ca588e2880394f9e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tensor([[ 0.0403,  0.0128, -0.0067,  ..., -0.0049, -0.0416, -0.0356],\n        [ 0.0575,  0.0059, -0.0185,  ..., -0.0489, -0.0511, -0.0437],\n        [-0.0080,  0.0187, -0.0103,  ..., -0.0336, -0.0822, -0.0343],\n        ...,\n        [ 0.0498,  0.0655, -0.0336,  ..., -0.0047, -0.0249, -0.0174],\n        [ 0.0183,  0.0877, -0.0696,  ...,  0.0583,  0.0100,  0.0260],\n        [-0.0078,  0.1351, -0.0372,  ...,  0.0125, -0.0042,  0.0256]],\n       device='mps:0')"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings = embeddings_model.encode(text_chunks,\n",
    "                                          batch_size=32,\n",
    "                                          show_progress_bar=True,\n",
    "                                          convert_to_tensor=True)\n",
    "text_embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T19:53:33.500278Z",
     "start_time": "2024-05-06T19:48:50.209101Z"
    }
   },
   "id": "bb442471f5c259c",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.save(text_embeddings, f\"../data/text_embeddings_{device}.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T19:53:33.539385Z",
     "start_time": "2024-05-06T19:53:33.501700Z"
    }
   },
   "id": "8df59afb868ff025",
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RAG pipeline \n",
    "(Checkpoint 1) - work above is saved in the data folder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34fec35887939725"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 4.0338e-02,  1.2804e-02, -6.7454e-03,  4.2980e-02, -4.3603e-02,\n         1.6660e-02, -3.8313e-02,  3.4725e-02, -4.9667e-03, -3.8526e-02,\n         4.6374e-02, -5.1384e-02,  4.2376e-02,  1.3069e-02,  4.0606e-02,\n        -1.1125e-02,  7.3396e-02,  3.7244e-02, -5.2825e-02, -3.8347e-03,\n         1.3996e-02,  1.2569e-02, -1.7039e-03,  4.6136e-02, -3.2836e-02,\n         1.1327e-02,  2.5366e-02, -2.0369e-02,  2.4939e-03, -1.9886e-03,\n         2.1593e-02, -2.6125e-03,  8.8577e-03,  7.1654e-03,  1.8332e-06,\n        -2.1929e-02, -2.9749e-02, -1.1231e-03,  4.3468e-02, -1.9777e-02,\n         6.6936e-02, -1.2584e-02,  4.0857e-03,  2.1768e-02, -3.9587e-02,\n        -3.9380e-02,  6.4695e-02,  8.3795e-02,  2.7839e-02,  2.4199e-02,\n        -2.0639e-02, -8.7842e-02,  7.2766e-05, -1.5067e-02, -8.0752e-03,\n         5.4452e-03,  4.4952e-02, -2.2253e-02, -5.6358e-03,  5.0621e-03,\n        -4.8909e-02,  1.9777e-02, -4.4482e-02,  5.6928e-03,  4.2703e-02,\n         7.4282e-02, -5.1439e-02, -3.6327e-02,  1.5568e-02,  4.5734e-02,\n        -6.9814e-02, -2.3512e-02, -8.2065e-03,  3.1195e-02, -4.2038e-02,\n         5.4484e-02, -2.1119e-02, -4.8982e-02,  1.2412e-02,  4.7215e-02,\n         1.9225e-02,  2.0540e-02, -1.6174e-02, -9.1987e-03,  3.9902e-03,\n         9.0083e-03,  1.1403e-02, -3.1551e-02,  2.1063e-02,  5.3906e-03,\n        -3.9024e-02, -5.7090e-03,  3.2892e-02,  5.5936e-03,  4.6279e-02,\n         3.0679e-02, -8.1164e-03, -4.0032e-03,  1.3855e-02, -1.1066e-01,\n         1.7350e-02,  5.8734e-03, -8.1978e-03,  2.3331e-02, -8.3980e-02,\n        -1.3682e-02,  6.9244e-03,  3.4408e-02, -4.8677e-02, -2.7210e-02,\n        -4.3091e-02, -6.8824e-03, -5.2639e-02,  6.9034e-03,  8.2994e-03,\n        -6.4403e-02, -4.6465e-02, -1.4128e-02, -1.5669e-02, -1.5759e-02,\n        -2.5019e-03, -3.7707e-03,  1.1415e-02,  3.6307e-02, -5.0931e-03,\n        -2.5787e-02, -7.4555e-02,  3.3685e-02, -5.3894e-02, -1.7967e-02,\n        -1.0666e-02,  1.0209e-02,  3.5566e-02,  1.9196e-03, -2.1287e-04,\n         1.7665e-02,  1.1624e-03, -4.4006e-02, -4.2662e-02,  2.1699e-02,\n         6.3293e-03, -1.9838e-02, -1.8612e-02, -6.3355e-02,  5.4747e-02,\n         1.2408e-03, -6.5764e-03,  9.8922e-03,  2.7518e-03,  1.4020e-02,\n        -1.0641e-01,  9.4920e-02,  7.1589e-04, -2.1373e-02,  6.3663e-02,\n         2.0496e-02,  6.3503e-02,  3.0006e-02, -2.9892e-03,  2.4640e-03,\n         3.2903e-02,  1.4926e-02, -1.6931e-02, -9.9224e-03, -1.6327e-03,\n        -2.6710e-02, -2.7259e-02, -9.4161e-03,  6.1306e-02,  5.8812e-02,\n         2.3428e-02,  6.4554e-02, -5.0515e-02, -1.3765e-02,  1.6717e-02,\n         1.1004e-01,  6.8854e-02,  2.5919e-02,  4.4721e-02,  5.6924e-03,\n         1.3686e-03,  6.5999e-03, -7.7021e-03, -5.2941e-03, -6.1323e-02,\n        -3.1052e-02,  2.9818e-03, -2.1445e-02, -3.5433e-02, -1.4948e-02,\n        -8.5475e-03, -1.8804e-02,  7.3164e-02, -8.2104e-02,  1.5273e-02,\n         1.9736e-02, -3.5481e-02,  4.1343e-02,  2.1724e-02, -6.4630e-03,\n        -3.2484e-02, -9.9847e-04,  2.0462e-02, -3.6495e-03,  4.7215e-02,\n         1.8474e-02,  3.3292e-02, -2.4360e-02, -3.3443e-02,  4.9137e-03,\n         2.0790e-02, -1.3813e-02, -2.7619e-02, -6.2033e-02,  5.0690e-03,\n         1.7652e-02, -1.1191e-02, -5.4068e-03,  2.4587e-04,  6.3875e-02,\n        -2.8096e-03,  2.0485e-02, -1.5413e-03,  5.6374e-02, -5.9243e-03,\n         1.7175e-02,  3.9663e-03,  2.3582e-03, -3.3076e-03,  2.3728e-02,\n         1.3045e-02,  8.6086e-03,  3.7945e-02, -4.6797e-02, -3.6649e-02,\n         1.0662e-02, -2.5261e-02, -2.2052e-02,  2.8243e-02, -5.6062e-02,\n         5.9129e-02,  4.2087e-02, -4.1109e-03, -1.2835e-02, -4.0066e-02,\n        -2.9585e-02,  6.9311e-02, -4.5173e-02,  2.9595e-02, -5.0514e-02,\n         5.5798e-02,  9.6746e-03,  4.6358e-02, -4.4633e-02,  2.4983e-02,\n        -6.2620e-02,  3.3865e-02,  2.4810e-03,  1.0979e-02, -2.5550e-02,\n         3.8334e-02,  1.5955e-02, -1.1015e-02, -6.4810e-03,  1.3049e-02,\n        -2.0583e-02, -1.7488e-02, -9.3122e-02, -1.1887e-02,  1.0688e-02,\n        -2.5635e-02, -3.6331e-02, -2.1370e-02,  9.0411e-03,  9.8020e-03,\n        -5.8344e-02,  9.4708e-02, -3.5288e-02,  6.7093e-03,  4.8990e-02,\n        -1.0660e-02, -4.9656e-02, -4.5616e-02, -3.7784e-02,  2.0120e-03,\n         6.2116e-02,  2.1148e-03, -4.7143e-02, -4.3755e-02,  1.8829e-02,\n        -2.5856e-02,  4.2640e-02, -1.2108e-03, -3.7812e-02, -3.2396e-02,\n        -1.0772e-03,  4.8470e-02,  2.9009e-02,  5.4998e-02,  5.4786e-02,\n         1.8730e-02,  4.2511e-02, -7.0360e-03, -2.7515e-02, -2.8157e-02,\n         5.6851e-02, -1.3369e-02,  4.0362e-03, -4.3507e-02,  6.4669e-02,\n         2.3271e-02, -5.8750e-03, -2.6182e-02,  3.1748e-02, -2.2170e-02,\n        -4.4358e-03, -2.1300e-02, -4.7316e-02, -2.1868e-02, -2.3009e-02,\n        -1.0518e-02, -2.7838e-03,  3.1461e-02, -2.1896e-02,  5.1027e-03,\n        -8.2296e-02, -4.1595e-02, -2.2327e-02,  1.5725e-02,  1.0463e-02,\n         2.4343e-02, -5.0474e-02, -9.3056e-03, -9.5722e-03,  1.8149e-02,\n         1.0542e-02, -4.4574e-02,  2.3499e-03,  1.9129e-02,  1.6178e-02,\n        -4.7368e-03, -7.4853e-03, -1.9592e-02, -6.1684e-02, -1.9216e-02,\n         2.6107e-02,  5.3410e-02, -2.9589e-04, -3.0168e-02,  3.5141e-02,\n        -4.0609e-02,  4.6494e-04, -6.5181e-03, -1.5895e-02, -8.9241e-03,\n         6.6092e-02,  5.9327e-02,  2.5177e-02, -3.0538e-03,  3.9438e-02,\n        -2.4695e-03,  3.5744e-02, -2.3450e-02,  1.6282e-02, -2.2283e-02,\n         2.0444e-02,  5.2639e-02, -4.8475e-02, -1.1420e-02, -2.5014e-02,\n         2.3278e-02,  2.0238e-02, -2.1236e-02,  6.0703e-02,  2.3050e-02,\n        -7.1616e-02,  2.1884e-02, -3.8773e-02, -4.5307e-02,  1.7414e-02,\n         2.9120e-02, -3.2075e-02, -8.0505e-03, -2.8858e-02, -1.8512e-02,\n         7.4910e-03, -3.1102e-02, -1.2657e-02, -9.3850e-02,  4.0955e-03,\n         1.8655e-02, -4.4176e-02, -2.0912e-02, -5.6691e-02, -2.4886e-02,\n        -2.6517e-03, -2.8342e-02, -2.1722e-02, -1.1166e-02, -4.3291e-02,\n        -4.2569e-04, -3.2671e-02, -1.1506e-02,  4.4697e-02, -6.3844e-02,\n         3.8272e-02,  6.5488e-02,  7.0109e-02, -1.7519e-02, -3.2642e-02,\n         2.5020e-02, -5.7008e-02, -1.2018e-02, -4.4883e-02,  3.5135e-02,\n        -5.0799e-02,  2.7771e-02, -9.5076e-03,  2.9873e-02,  3.9807e-02,\n        -5.0510e-03, -3.4743e-02,  5.5519e-02,  1.5044e-02, -5.9464e-02,\n        -5.9287e-02,  3.5313e-02,  1.3216e-02,  4.9230e-02,  3.0191e-02,\n        -4.9923e-02, -3.7641e-03,  2.5596e-02,  3.1593e-02,  8.4745e-03,\n        -3.3374e-02, -4.2287e-03, -4.6147e-04, -1.8654e-02, -5.2096e-02,\n        -3.8613e-02,  2.7336e-02,  3.7965e-02,  7.4905e-03,  6.5040e-02,\n         1.8984e-03, -1.7864e-02, -8.2078e-02, -3.8695e-02,  5.1317e-02,\n         6.3565e-02,  2.5538e-02, -2.0416e-02,  4.0569e-02,  1.2866e-02,\n        -4.3217e-02, -2.1023e-02, -5.3889e-02, -3.9480e-02, -3.4362e-03,\n         2.4566e-02,  1.2069e-02,  6.7633e-02, -3.7755e-02, -8.5897e-02,\n         5.1692e-02,  5.9214e-03, -3.9576e-02, -4.4924e-03, -1.3299e-02,\n         3.2938e-02, -6.9333e-04,  2.9899e-02, -6.0269e-03,  4.1086e-02,\n        -2.9213e-02,  3.0973e-02, -1.5868e-02, -2.0589e-02, -1.0789e-02,\n        -5.4202e-02,  6.6151e-02, -1.7258e-02,  1.7344e-02, -5.5295e-02,\n        -1.7145e-02, -1.0538e-02,  1.1231e-02, -7.6914e-03,  9.5006e-03,\n         3.6312e-02, -1.7368e-02, -3.4962e-02, -4.0799e-02, -7.4599e-03,\n         4.8584e-03,  2.4638e-02,  6.7507e-02,  4.1681e-02,  5.9626e-02,\n        -2.4109e-02,  3.9863e-02, -2.9409e-02, -3.6364e-02, -1.6249e-02,\n         1.8472e-02, -5.7591e-02,  3.4990e-02,  1.7438e-02,  1.1545e-02,\n        -5.1641e-02,  5.9184e-02, -1.7433e-02,  2.8969e-02, -2.5939e-02,\n         3.6833e-02,  2.2220e-02, -7.3983e-02,  3.6730e-03, -4.9616e-03,\n        -2.8638e-02,  7.0342e-03,  5.8020e-03, -5.7426e-02,  1.3324e-02,\n         4.3954e-02,  6.3868e-03, -1.6699e-02, -9.8630e-03,  2.1701e-02,\n        -8.9306e-03, -2.3797e-02, -2.4204e-02, -2.1439e-02, -2.4742e-02,\n         5.7075e-02,  7.0567e-02,  6.7286e-02, -1.1287e-02,  2.9831e-02,\n        -6.4173e-02, -1.7028e-02,  4.7620e-02, -1.4078e-03, -1.0523e-02,\n         4.4901e-02,  3.0617e-02,  4.6855e-02,  5.3896e-02,  2.7912e-02,\n        -4.2356e-02,  1.7155e-02, -4.3507e-02, -6.2437e-02, -9.3900e-04,\n        -6.0955e-33, -1.5531e-02, -4.8107e-02, -1.6007e-02,  7.0986e-03,\n        -5.3599e-02,  1.9782e-02, -3.6739e-03, -2.1252e-02, -3.5942e-02,\n         2.6183e-02, -1.1784e-02,  1.6808e-02,  7.3935e-03, -4.6172e-02,\n        -1.5122e-02,  9.2817e-03,  5.5605e-03,  1.4320e-02,  5.1121e-03,\n        -1.6208e-02,  1.5799e-02,  8.4198e-02,  2.7278e-02, -5.8789e-02,\n         3.5549e-02, -1.7686e-02,  5.3469e-02,  6.5103e-04,  4.9715e-02,\n         4.4775e-02, -5.2611e-02,  2.0857e-02,  3.1678e-02,  7.8672e-03,\n         8.9479e-03,  6.9453e-02, -9.4216e-02, -6.4373e-02, -2.7900e-03,\n         6.2007e-03, -9.0077e-02,  1.1519e-02,  4.2121e-02,  2.2113e-02,\n        -3.7287e-02,  2.3732e-02,  5.9268e-02, -2.2784e-02, -7.4168e-02,\n         1.5361e-02, -5.4825e-02,  1.2803e-02,  7.7091e-03,  2.9509e-02,\n         4.4752e-02, -1.4630e-02,  4.3444e-02, -1.1163e-02, -6.8349e-02,\n        -6.2489e-04, -6.0438e-02,  1.7778e-02,  5.5640e-03,  3.1206e-03,\n         3.5786e-03,  7.6922e-02,  5.1393e-02, -7.0269e-02, -7.2741e-03,\n         2.6484e-02,  2.8018e-02,  3.2627e-02,  1.9111e-02,  2.2364e-02,\n         7.4360e-02, -7.8762e-02, -1.9037e-02,  3.8081e-02,  1.2337e-02,\n         2.3304e-02,  1.2089e-03, -1.9533e-02, -5.7168e-02, -2.0120e-02,\n        -6.0326e-04, -5.6932e-02, -4.0208e-02, -4.2724e-02,  2.0878e-02,\n         1.8570e-02, -2.6654e-02,  2.2706e-02, -4.7357e-03, -3.5769e-02,\n         1.1706e-03,  2.8404e-02,  5.5882e-03, -8.2944e-03, -4.1969e-03,\n         2.5875e-02, -1.0121e-02, -6.1092e-02, -4.2737e-02,  1.1433e-03,\n         2.2780e-02, -8.6300e-03,  7.6811e-03,  2.2248e-03, -3.9198e-02,\n        -9.6497e-03,  4.4523e-04, -7.1237e-02, -3.6763e-02,  3.0291e-03,\n         8.8223e-04,  1.3320e-02,  2.5637e-03,  3.5796e-02,  4.5423e-02,\n         6.1223e-03, -2.0381e-03, -6.5276e-02,  2.8891e-02,  3.4197e-02,\n        -3.6006e-02, -5.6297e-03, -3.0483e-02,  5.2160e-02,  4.4743e-02,\n        -8.2420e-02,  1.8474e-03,  3.6759e-02,  2.5770e-07,  4.3233e-02,\n         5.2532e-02,  1.0703e-02, -4.6429e-02, -2.0519e-03, -1.6382e-02,\n         1.1641e-02,  4.6093e-02, -5.2596e-02,  1.5429e-02,  6.2698e-03,\n         2.6984e-02,  1.6455e-02,  1.1468e-02, -5.3987e-02,  3.3069e-02,\n        -6.5980e-02,  2.7588e-02, -4.0315e-02,  1.3747e-02,  7.9731e-02,\n         1.0426e-01,  5.3706e-02, -4.6293e-03, -4.1634e-02, -2.3175e-02,\n         1.9650e-02, -1.1191e-02,  2.7770e-02,  5.6879e-05, -9.5837e-03,\n         2.1413e-02,  1.5023e-02, -6.5604e-03, -2.3111e-02,  8.4525e-02,\n         6.2702e-02,  7.1242e-03, -4.5455e-02,  2.8458e-02,  1.4833e-02,\n        -1.3782e-02, -2.2681e-02, -1.6218e-02,  7.6284e-02, -1.8914e-02,\n        -3.2173e-02, -4.2537e-02,  4.3697e-02,  3.7832e-02,  6.2611e-02,\n         6.6882e-03, -4.0621e-02, -8.5162e-03,  3.5008e-02, -1.7046e-02,\n         2.2553e-03, -8.1312e-02,  2.1368e-02,  1.9106e-02,  1.2892e-03,\n         4.5419e-04,  6.9134e-04,  8.9144e-03,  1.0071e-01,  3.0770e-02,\n        -1.0060e-02,  1.9801e-34,  1.2513e-02,  5.7043e-03,  6.4225e-03,\n         3.8398e-02,  2.3528e-03, -2.6968e-02,  2.7612e-02,  2.5686e-02,\n        -4.9020e-03, -4.1622e-02, -3.5611e-02], device='mps:0')"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "text_chunks_df = pd.read_csv(\"../data/chunks_embedded.csv\")\n",
    "text_chunks_dict = text_chunks_df.to_dict(\"records\")\n",
    "\n",
    "text_embeddings = torch.load(f\"../data/text_embeddings_{device}.pt\")\n",
    "text_embeddings[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:00:10.520835Z",
     "start_time": "2024-05-06T20:00:08.898388Z"
    }
   },
   "id": "ccd3b8add3a7af14",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Query embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4c55077e9568d22"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (2): Normalize()\n)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\", device=device)\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:00:37.811490Z",
     "start_time": "2024-05-06T20:00:34.749631Z"
    }
   },
   "id": "6ae2eca028b177da",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Retrieval indexing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccde52af445ad454"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.return_types.topk(\nvalues=tensor([[0.7937, 0.7876, 0.7609, 0.7162, 0.6967]], device='mps:0'),\nindices=tensor([[1032,  393, 1982, 6746, 3390]], device='mps:0'))"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query \n",
    "query = \"What is machine learning?\"\n",
    "\n",
    "# embedding\n",
    "query_embedding = model.encode(query, convert_to_tensor=True, device=device)\n",
    "\n",
    "# similarity - dot because of a Normalize layer in the model\n",
    "dot_products = util.dot_score(query_embedding, text_embeddings)\n",
    "\n",
    "# best results\n",
    "torch.topk(dot_products, k=5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T19:53:50.840048Z",
     "start_time": "2024-05-06T19:53:48.277885Z"
    }
   },
   "id": "6f1f4cafd03a8e3b",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                        Title  \\\n1032  On the Journey to Machine Learning / AI   \n393     Microsoft Introduction to AI — Part 1   \n1982             So what is Machine Learning?   \n6746                    Why Machine Learning?   \n3390               Machine Learning in Energy   \n\n                                                  Chunk  Chunk_length  \\\n1032  What is Machine Learning? There are millions o...         953.0   \n393   Learning from this data it can understand our ...         792.0   \n1982  Photo by fabio on Unsplash I am sure by now yo...        1194.0   \n6746  Image by the author In my previous post I talk...        1104.0   \n3390  What is machine learning? The business plans o...        1206.0   \n\n      Chunk_words_num  Chunk_sentences_num  Token_count  \\\n1032            159.0                 10.0       238.25   \n393             131.0                 10.0       198.00   \n1982            215.0                 10.0       298.50   \n6746            181.0                 10.0       276.00   \n3390            200.0                 10.0       301.50   \n\n                                              Embedding  \n1032  [ 2.16269530e-02 -2.55265869e-02 -6.68139756e-...  \n393   [ 2.41031330e-02 -5.80267282e-03 -4.96849753e-...  \n1982  [ 3.63377593e-02  2.96800584e-02 -4.95566875e-...  \n6746  [ 3.04092318e-02  5.43406904e-02 -4.28669490e-...  \n3390  [ 6.98239403e-03  6.78827018e-02 -6.71653450e-...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Chunk</th>\n      <th>Chunk_length</th>\n      <th>Chunk_words_num</th>\n      <th>Chunk_sentences_num</th>\n      <th>Token_count</th>\n      <th>Embedding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1032</th>\n      <td>On the Journey to Machine Learning / AI</td>\n      <td>What is Machine Learning? There are millions o...</td>\n      <td>953.0</td>\n      <td>159.0</td>\n      <td>10.0</td>\n      <td>238.25</td>\n      <td>[ 2.16269530e-02 -2.55265869e-02 -6.68139756e-...</td>\n    </tr>\n    <tr>\n      <th>393</th>\n      <td>Microsoft Introduction to AI — Part 1</td>\n      <td>Learning from this data it can understand our ...</td>\n      <td>792.0</td>\n      <td>131.0</td>\n      <td>10.0</td>\n      <td>198.00</td>\n      <td>[ 2.41031330e-02 -5.80267282e-03 -4.96849753e-...</td>\n    </tr>\n    <tr>\n      <th>1982</th>\n      <td>So what is Machine Learning?</td>\n      <td>Photo by fabio on Unsplash I am sure by now yo...</td>\n      <td>1194.0</td>\n      <td>215.0</td>\n      <td>10.0</td>\n      <td>298.50</td>\n      <td>[ 3.63377593e-02  2.96800584e-02 -4.95566875e-...</td>\n    </tr>\n    <tr>\n      <th>6746</th>\n      <td>Why Machine Learning?</td>\n      <td>Image by the author In my previous post I talk...</td>\n      <td>1104.0</td>\n      <td>181.0</td>\n      <td>10.0</td>\n      <td>276.00</td>\n      <td>[ 3.04092318e-02  5.43406904e-02 -4.28669490e-...</td>\n    </tr>\n    <tr>\n      <th>3390</th>\n      <td>Machine Learning in Energy</td>\n      <td>What is machine learning? The business plans o...</td>\n      <td>1206.0</td>\n      <td>200.0</td>\n      <td>10.0</td>\n      <td>301.50</td>\n      <td>[ 6.98239403e-03  6.78827018e-02 -6.71653450e-...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks_df.iloc[torch.topk(dot_products, k=5).indices.cpu().numpy().ravel()]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T19:53:50.867244Z",
     "start_time": "2024-05-06T19:53:50.842312Z"
    }
   },
   "id": "762b1e5df6960cf9",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([7185, 768])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T19:53:50.878537Z",
     "start_time": "2024-05-06T19:53:50.872210Z"
    }
   },
   "id": "1954f10d380749a8",
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Helper functions for retrieval and printing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24388e1261b968be"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is a convolutional neural net?\n",
      "=======\n",
      "Article title: A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model || Score: 2903\n",
      "\n",
      "1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings\n",
      "using a two-layer neural network. Its input is a text corpus and its output is a set of vectors.\n",
      "Word embedding via word2vec can make natural language computer-readable, then further implementation\n",
      "of mathematical operations on words can be used to detect their similarities. A well-trained set of\n",
      "word vectors will place similar words close to each other in that space. For instance, the words\n",
      "women, men, and human might cluster in one corner, while yellow, red and blue cluster together in\n",
      "another. There are two main training algorithms for word2vec, one is the continuous bag of\n",
      "words(CBOW), another is called skip-gram. The major difference between these two methods is that\n",
      "CBOW is using context to predict a target word while skip-gram is using a word to predict a target\n",
      "context. Generally, the skip-gram method can have a better performance compared with CBOW method,\n",
      "for it can capture two semantics for a single word. For instance, it will have two vector\n",
      "representations for Apple, one for the company and another for the fruit.\n",
      "\n",
      "\n",
      "=======\n",
      "Article title: A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model || Score: 6250\n",
      "\n",
      "1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings\n",
      "using a two-layer neural network. Its input is a text corpus and its output is a set of vectors.\n",
      "Word embedding via word2vec can make natural language computer-readable, then further implementation\n",
      "of mathematical operations on words can be used to detect their similarities. A well-trained set of\n",
      "word vectors will place similar words close to each other in that space. For instance, the words\n",
      "women, men, and human might cluster in one corner, while yellow, red and blue cluster together in\n",
      "another. There are two main training algorithms for word2vec, one is the continuous bag of\n",
      "words(CBOW), another is called skip-gram. The major difference between these two methods is that\n",
      "CBOW is using context to predict a target word while skip-gram is using a word to predict a target\n",
      "context. Generally, the skip-gram method can have a better performance compared with CBOW method,\n",
      "for it can capture two semantics for a single word. For instance, it will have two vector\n",
      "representations for Apple, one for the company and another for the fruit.\n",
      "\n",
      "\n",
      "=======\n",
      "Article title: A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model || Score: 7111\n",
      "\n",
      "1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings\n",
      "using a two-layer neural network. Its input is a text corpus and its output is a set of vectors.\n",
      "Word embedding via word2vec can make natural language computer-readable, then further implementation\n",
      "of mathematical operations on words can be used to detect their similarities. A well-trained set of\n",
      "word vectors will place similar words close to each other in that space. For instance, the words\n",
      "women, men, and human might cluster in one corner, while yellow, red and blue cluster together in\n",
      "another. There are two main training algorithms for word2vec, one is the continuous bag of\n",
      "words(CBOW), another is called skip-gram. The major difference between these two methods is that\n",
      "CBOW is using context to predict a target word while skip-gram is using a word to predict a target\n",
      "context. Generally, the skip-gram method can have a better performance compared with CBOW method,\n",
      "for it can capture two semantics for a single word. For instance, it will have two vector\n",
      "representations for Apple, one for the company and another for the fruit.\n",
      "\n",
      "\n",
      "=======\n",
      "Article title: A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model || Score: 2904\n",
      "\n",
      "1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings\n",
      "using a two-layer neural network. Its input is a text corpus and its output is a set of vectors.\n",
      "Word embedding via word2vec can make natural language computer-readable, then further implementation\n",
      "of mathematical operations on words can be used to detect their similarities. A well-trained set of\n",
      "word vectors will place similar words close to each other in that space. For instance, the words\n",
      "women, men, and human might cluster in one corner, while yellow, red and blue cluster together in\n",
      "another. There are two main training algorithms for word2vec, one is the continuous bag of\n",
      "words(CBOW), another is called skip-gram. The major difference between these two methods is that\n",
      "CBOW is using context to predict a target word while skip-gram is using a word to predict a target\n",
      "context. Generally, the skip-gram method can have a better performance compared with CBOW method,\n",
      "for it can capture two semantics for a single word. For instance, it will have two vector\n",
      "representations for Apple, one for the company and another for the fruit.\n",
      "\n",
      "\n",
      "=======\n",
      "Article title: A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model || Score: 691\n",
      "\n",
      "1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings\n",
      "using a two-layer neural network. Its input is a text corpus and its output is a set of vectors.\n",
      "Word embedding via word2vec can make natural language computer-readable, then further implementation\n",
      "of mathematical operations on words can be used to detect their similarities. A well-trained set of\n",
      "word vectors will place similar words close to each other in that space. For instance, the words\n",
      "women, men, and human might cluster in one corner, while yellow, red and blue cluster together in\n",
      "another. There are two main training algorithms for word2vec, one is the continuous bag of\n",
      "words(CBOW), another is called skip-gram. The major difference between these two methods is that\n",
      "CBOW is using context to predict a target word while skip-gram is using a word to predict a target\n",
      "context. Generally, the skip-gram method can have a better performance compared with CBOW method,\n",
      "for it can capture two semantics for a single word. For instance, it will have two vector\n",
      "representations for Apple, one for the company and another for the fruit.\n",
      "\n",
      "\n",
      "=======\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "\n",
    "def retrieve_simillar_embeddings(query, embeddings, model=model, device=device, n=5):\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True, device=device)\n",
    "    dot_products = util.dot_score(query_embedding, embeddings)\n",
    "    top_results = torch.topk(dot_products, k=n)\n",
    "    \n",
    "    indices, scores = top_results\n",
    "    \n",
    "    return indices, scores\n",
    "\n",
    "\n",
    "def search_text(query, embeddings, model=model, device=device, n=5):\n",
    "    indices, scores = retrieve_simillar_embeddings(query, embeddings, model, device, n)\n",
    "    indices = indices.cpu().numpy().ravel()\n",
    "    scores = scores.cpu().numpy().ravel()\n",
    "    \n",
    "    results = text_chunks_df.iloc[indices]\n",
    "    chunks = results[\"Chunk\"].values\n",
    "    titles = results[\"Title\"].values\n",
    "    \n",
    "    return chunks, titles, scores\n",
    "    \n",
    "    \n",
    "def print_text(chunks, titles, scores, query, width=100):\n",
    "    print(\"Query:\", query)\n",
    "    print(\"=======\")\n",
    "    wrapper = textwrap.TextWrapper(width=width)\n",
    "\n",
    "    for chunk, title, score in zip(chunks, titles, scores):\n",
    "        print(f\"Article title: {title} || Score: {score}\\n\")\n",
    "        word_list = wrapper.wrap(text=chunk)\n",
    "        for element in word_list:\n",
    "            print(element)\n",
    "        print(\"\\n\")\n",
    "        print(\"=======\")\n",
    "        \n",
    "query = \"What is a convolutional neural net?\"\n",
    "\n",
    "chunks, titles, scores = search_text(query, text_embeddings)\n",
    "\n",
    "print_text(chunks, titles, scores, query)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:01:52.437921Z",
     "start_time": "2024-05-06T20:01:51.185148Z"
    }
   },
   "id": "75b99730ad03b1c3",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0.7434, 0.6638, 0.6285, 0.6077, 0.6074]], device='mps:0'),\n tensor([[2903, 6250, 7111, 2904,  691]], device='mps:0'))"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_simillar_embeddings(\"What is a convolutional neural net?\", text_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T19:53:54.580589Z",
     "start_time": "2024-05-06T19:53:54.431095Z"
    }
   },
   "id": "4290a1eaf0428353",
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Local LLM generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15f036e540957a45"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# model https://huggingface.co/google/gemma-2b-it\n",
    "# huggingface-cli https://huggingface.co/docs/huggingface_hub/main/en/guides/cli"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T19:53:54.586806Z",
     "start_time": "2024-05-06T19:53:54.583311Z"
    }
   },
   "id": "b56442b116af7616",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eda9e070ff1740a29466029938f9f665"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "GemmaForCausalLM(\n  (model): GemmaModel(\n    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n    (layers): ModuleList(\n      (0-17): 18 x GemmaDecoderLayer(\n        (self_attn): GemmaSdpaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n    )\n    (norm): GemmaRMSNorm()\n  )\n  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# using 7b is possible if 24gb of VRAM on a GPU is available\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\",\n",
    "                                           attn_implementation=\"sdpa\",\n",
    "                                           torch_dtype=torch.float16,\n",
    "                                           low_cpu_mem_usage=False)\n",
    "\n",
    "llm.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:00:27.307442Z",
     "start_time": "2024-05-06T20:00:15.848080Z"
    }
   },
   "id": "93fe66bd957e8a5e",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used memory (MPS GPU): 5.60 GB\n",
      "model size: 2.51B\n",
      "Model memory: 5.55 GB\n",
      "Model parameters: 5549215744 bytes\n"
     ]
    }
   ],
   "source": [
    "def print_model_data(model, device):\n",
    "    if device == torch.device(\"mps\"):\n",
    "        print(f\"Used memory (MPS GPU): {(torch.mps.current_allocated_memory() / 1024 ** 3):.2f} GB\", )\n",
    "    print(f\"model size: {(sum([p.numel() for p in model.parameters()]) / 1e9):.2f}B\")\n",
    "    \n",
    "    mem_params = sum([p.nelement() * p.element_size() for p in model.parameters()])\n",
    "    mem_buffers = sum([b.nelement() * b.element_size() for b in model.buffers()])\n",
    "    \n",
    "    print(f\"Model memory: {(mem_params + mem_buffers) / 1e9:.2f} GB\")\n",
    "    print(f\"Model parameters: {mem_params + mem_buffers} bytes\")\n",
    "\n",
    "\n",
    "print_model_data(llm, device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:00:43.308016Z",
     "start_time": "2024-05-06T20:00:43.304386Z"
    }
   },
   "id": "a46ab5207c2dee2b",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccb42699499bf0d5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a convolutional neural net?\n"
     ]
    },
    {
     "data": {
      "text/plain": "'<bos><start_of_turn>user\\nWhat is a convolutional neural net?<end_of_turn>\\n<start_of_turn>model\\n'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"What is a convolutional neural net?\"\n",
    "print(input_text)\n",
    "\n",
    "llm_prompt_template = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": input_text,\n",
    "}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(llm_prompt_template, tokenize=False, add_generation_prompt=True)\n",
    "prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:00:45.316962Z",
     "start_time": "2024-05-06T20:00:45.292855Z"
    }
   },
   "id": "da71c2eccd74d6b2",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "What is a convolutional neural net?\n",
      "model\n",
      "A convolutional neural network (CNN) is a type of artificial neural network (ANN) used for image recognition and classification. It is a specialized type of neural network that is designed to process and analyze visual information.\n",
      "\n",
      "**Key features of CNNs:**\n",
      "\n",
      "* **Layers:** CNNs consist of multiple layers, each containing a set of interconnected neurons.\n",
      "* **Convolutional layers:** These layers perform a convolution operation on the input image, extracting features and identifying patterns.\n",
      "* **Pooling layers:** After the convolution layer, the feature maps are reduced in size, reducing computation and allowing for efficient processing.\n",
      "* **Max-pooling:** This operation takes the maximum value from each feature map cell in a given region.\n",
      "* **Activation function:** After the convolution and pooling operations, an activation function is applied to each neuron, introducing non-linearity into the model.\n",
      "* **Fully connected layers:** These layers connect the output of the convolutional and pooling layers to the output layer, which makes the final classification decision.\n",
      "\n",
      "**How CNNs work:**\n",
      "\n",
      "1. **Input:** The input image is fed into the network.\n",
      "2. **Convolution:** The input image is convolved with a set of filters, extracting features from the image.\n",
      "3. **Pooling\n"
     ]
    }
   ],
   "source": [
    "tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "output = llm.generate(**tokenized_prompt, max_new_tokens=256)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:01:40.277052Z",
     "start_time": "2024-05-06T20:00:47.192041Z"
    }
   },
   "id": "b49af16688a41acb",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output from the model looks fine, let's prompt engineer it"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b99e6fa5356c37b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is the difference between supervised and unsupervised learning in machine learning?\",\n",
    "    \"Can you explain the bias-variance tradeoff in machine learning?\",\n",
    "    \"What are some common activation functions used in neural networks?\",\n",
    "    \"What is the purpose of regularization in machine learning models?\",\n",
    "    \"How does gradient descent optimize the parameters of a machine learning model?\",\n",
    "    \"What is cross-validation and why is it used in machine learning?\",\n",
    "    \"Explain the concept of feature engineering in machine learning.\",\n",
    "    \"What is the role of hyperparameters in machine learning algorithms?\"\n",
    "]\n",
    "\n",
    "\n",
    "def create_prompt(query, context_chunks):\n",
    "    query_start = \"Answer the question: \" + query\n",
    "    answer_requirements = \"\"\"\n",
    "Give yourself room to think by extracting relevant passages from the context before answering.\n",
    "Return just the answer to the question.\n",
    "Make sure the answer is as explanatory as possible.\n",
    "Use the following reference questions and answers as a style guideline but answer only to the question above:\n",
    "1. What is overfitting in machine learning?\n",
    "   - Overfitting occurs when a model learns to memorize the training data instead of capturing the underlying patterns, leading to poor generalization on unseen data.\n",
    "\n",
    "2. What is the purpose of a validation set in machine learning?\n",
    "   - The validation set is used to evaluate the performance of a model during training and to tune hyperparameters to prevent overfitting.\n",
    "\n",
    "3. What is the difference between precision and recall in binary classification?\n",
    "   - Precision measures the proportion of true positives among all predicted positives, while recall measures the proportion of true positives among all actual positives.\n",
    "\n",
    "4. What is the softmax function used for in neural networks?\n",
    "   - The softmax function is used to convert the raw output of a neural network into probabilities, enabling it to make multi-class predictions.\n",
    "\n",
    "5. What is transfer learning in deep learning?\n",
    "   - Transfer learning involves using pre-trained neural network models as a starting point for training on a new task, often resulting in faster convergence and better performance with less data.\n",
    "\n",
    "6. What is batch normalization in neural networks?\n",
    "   - Batch normalization is a technique used to normalize the inputs of each layer in a neural network, stabilizing training and accelerating convergence.\n",
    "\n",
    "7. What is the purpose of the Adam optimizer in deep learning?\n",
    "   - The Adam optimizer is an adaptive learning rate optimization algorithm that combines the advantages of both AdaGrad and RMSProp, making it widely used in training deep neural networks.\n",
    "\n",
    "8. What is the curse of dimensionality in machine learning?\n",
    "   - The curse of dimensionality refers to the increased difficulty of learning and generalizing from data in high-dimensional spaces, leading to sparsity and increased computational complexity.\n",
    "    \"\"\"\n",
    "\n",
    "    query_end = \"Based on the following context.\"\n",
    "    context = \"- \" + \"\\n- \".join([chunk for chunk in context_chunks])\n",
    "\n",
    "    prompt = \"\\n\".join([query_start, answer_requirements, context, query_end])\n",
    "\n",
    "    llm_prompt_template = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": prompt,\n",
    "    }]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(llm_prompt_template,\n",
    "                                           tokenize=False,\n",
    "                                           add_generation_prompt=True)\n",
    "\n",
    "    return prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:01:43.532490Z",
     "start_time": "2024-05-06T20:01:43.524637Z"
    }
   },
   "id": "a8910b4c0cbe1d30",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Test the prompt generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11b11e72398568db"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Answer the question: What is the role of hyperparameters in machine learning algorithms?\n",
      "\n",
      "Give yourself room to think by extracting relevant passages from the context before answering.\n",
      "Return just the answer to the question.\n",
      "Make sure the answer is as explanatory as possible.\n",
      "Use the following reference questions and answers as a style guideline but answer only to the question above:\n",
      "1. What is overfitting in machine learning?\n",
      "   - Overfitting occurs when a model learns to memorize the training data instead of capturing the underlying patterns, leading to poor generalization on unseen data.\n",
      "\n",
      "2. What is the purpose of a validation set in machine learning?\n",
      "   - The validation set is used to evaluate the performance of a model during training and to tune hyperparameters to prevent overfitting.\n",
      "\n",
      "3. What is the difference between precision and recall in binary classification?\n",
      "   - Precision measures the proportion of true positives among all predicted positives, while recall measures the proportion of true positives among all actual positives.\n",
      "\n",
      "4. What is the softmax function used for in neural networks?\n",
      "   - The softmax function is used to convert the raw output of a neural network into probabilities, enabling it to make multi-class predictions.\n",
      "\n",
      "5. What is transfer learning in deep learning?\n",
      "   - Transfer learning involves using pre-trained neural network models as a starting point for training on a new task, often resulting in faster convergence and better performance with less data.\n",
      "\n",
      "6. What is batch normalization in neural networks?\n",
      "   - Batch normalization is a technique used to normalize the inputs of each layer in a neural network, stabilizing training and accelerating convergence.\n",
      "\n",
      "7. What is the purpose of the Adam optimizer in deep learning?\n",
      "   - The Adam optimizer is an adaptive learning rate optimization algorithm that combines the advantages of both AdaGrad and RMSProp, making it widely used in training deep neural networks.\n",
      "\n",
      "8. What is the curse of dimensionality in machine learning?\n",
      "   - The curse of dimensionality refers to the increased difficulty of learning and generalizing from data in high-dimensional spaces, leading to sparsity and increased computational complexity.\n",
      "    \n",
      "- 1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.\n",
      "- 1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.\n",
      "- 1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.\n",
      "- 1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.\n",
      "- 1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.\n",
      "Based on the following context.<end_of_turn>\n",
      "<start_of_turn>model\n"
     ]
    }
   ],
   "source": [
    "query_idx = np.random.randint(0, len(questions))\n",
    "query = questions[query_idx]\n",
    "\n",
    "chunks, titles, scores = search_text(query, text_embeddings)\n",
    "prompt = create_prompt(query, chunks)\n",
    "\n",
    "print(prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:01:56.808335Z",
     "start_time": "2024-05-06T20:01:56.083185Z"
    }
   },
   "id": "aa62017c1f887974",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Answer the question: What is the role of hyperparameters in machine learning algorithms?\n",
      "\n",
      "Give yourself room to think by extracting relevant passages from the context before answering.\n",
      "Return just the answer to the question.\n",
      "Make sure the answer is as explanatory as possible.\n",
      "Use the following reference questions and answers as a style guideline but answer only to the question above:\n",
      "1. What is overfitting in machine learning?\n",
      "   - Overfitting occurs when a model learns to memorize the training data instead of capturing the underlying patterns, leading to poor generalization on unseen data.\n",
      "\n",
      "2. What is the purpose of a validation set in machine learning?\n",
      "   - The validation set is used to evaluate the performance of a model during training and to tune hyperparameters to prevent overfitting.\n",
      "\n",
      "3. What is the difference between precision and recall in binary classification?\n",
      "   - Precision measures the proportion of true positives among all predicted positives, while recall measures the proportion of true positives among all actual positives.\n",
      "\n",
      "4. What is the softmax function used for in neural networks?\n",
      "   - The softmax function is used to convert the raw output of a neural network into probabilities, enabling it to make multi-class predictions.\n",
      "\n",
      "5. What is transfer learning in deep learning?\n",
      "   - Transfer learning involves using pre-trained neural network models as a starting point for training on a new task, often resulting in faster convergence and better performance with less data.\n",
      "\n",
      "6. What is batch normalization in neural networks?\n",
      "   - Batch normalization is a technique used to normalize the inputs of each layer in a neural network, stabilizing training and accelerating convergence.\n",
      "\n",
      "7. What is the purpose of the Adam optimizer in deep learning?\n",
      "   - The Adam optimizer is an adaptive learning rate optimization algorithm that combines the advantages of both AdaGrad and RMSProp, making it widely used in training deep neural networks.\n",
      "\n",
      "8. What is the curse of dimensionality in machine learning?\n",
      "   - The curse of dimensionality refers to the increased difficulty of learning and generalizing from data in high-dimensional spaces, leading to sparsity and increased computational complexity.\n",
      "    \n",
      "- 1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.\n",
      "- 1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.\n",
      "- 1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.\n",
      "- 1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.\n",
      "- 1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.\n",
      "Based on the following context.\n",
      "model\n",
      "Sure, here's the answer to your question:\n",
      "\n",
      "Hyperparameters are adjustable parameters of a machine learning algorithm that influence the learning process. They are not learned from the data but rather set by the user. Hyperparameters can be related to various aspects of the model, such as the learning rate, the number of hidden layers, or the size of the neural network.\n",
      "\n",
      "The purpose of hyperparameters is to optimize the performance of the model by finding the set of parameters that leads to the best possible generalization performance on unseen data. Hyperparameters are often tuned using grid search or other optimization techniques.\n",
      "\n",
      "The context also provides some insights about the role of hyperparameters in machine learning:\n",
      "\n",
      "- Overfitting: Hyperparameters can be used to prevent overfitting by tuning the model to fit the training data as closely as possible.\n",
      "- Validation set: The validation set is used to evaluate the performance of the model during training and to prevent overfitting.\n",
      "- Precision and recall: Precision measures the proportion of true positives among all predicted positives, while recall measures the proportion of true positives among all actual positives.\n",
      "- Softmax function: The softmax function is used to convert the raw output of a neural network into probabilities, enabling it to make multi-class predictions.\n"
     ]
    }
   ],
   "source": [
    "llm_prompt_template = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": prompt,\n",
    "}]\n",
    "\n",
    "tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "output = llm.generate(**tokenized_prompt, max_new_tokens=256, temperature=0.7, do_sample=True)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:04:14.102108Z",
     "start_time": "2024-05-06T20:01:59.113069Z"
    }
   },
   "id": "c219a190a8e22692",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "39f6d056c668e2f6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
