{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:04:56.180169Z",
     "start_time": "2024-05-06T20:04:56.170016Z"
    }
   },
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Text preprocessing"
   ],
   "id": "3a3f0d23d628665c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T20:04:56.523268Z",
     "start_time": "2024-05-06T20:04:56.339416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(\"../data/medium.csv\")\n",
    "data.head()"
   ],
   "id": "f865993fea6e49f6",
   "outputs": [
    {
     "data": {
      "text/plain": "                                               Title  \\\n0  A Beginner’s Guide to Word Embedding with Gens...   \n1  Hands-on Graph Neural Networks with PyTorch & ...   \n2                       How to Use ggplot2 in Python   \n3  Databricks: How to Save Data Frames as CSV Fil...   \n4  A Step-by-Step Implementation of Gradient Desc...   \n\n                                                Text  \n0  1. Introduction of Word2vec\\n\\nWord2vec is one...  \n1  In my last article, I introduced the concept o...  \n2  Introduction\\n\\nThanks to its strict implement...  \n3  Photo credit to Mika Baumeister from Unsplash\\...  \n4  A Step-by-Step Implementation of Gradient Desc...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>1. Introduction of Word2vec\\n\\nWord2vec is one...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n      <td>In my last article, I introduced the concept o...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>How to Use ggplot2 in Python</td>\n      <td>Introduction\\n\\nThanks to its strict implement...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Databricks: How to Save Data Frames as CSV Fil...</td>\n      <td>Photo credit to Mika Baumeister from Unsplash\\...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T20:04:56.527894Z",
     "start_time": "2024-05-06T20:04:56.524681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_text(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    data[\"Text\"] = data[\"Text\"].str.replace(\"\\n\", \" \")\n",
    "    data[\"Text\"] = data[\"Text\"].str.replace(\"\\t\", \" \")\n",
    "    data[\"Text\"] = data[\"Text\"].str.replace(\"\\r\", \" \")\n",
    "    data[\"Text\"] = data[\"Text\"].str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    data[\"Text\"] = data[\"Text\"].str.strip()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def sample_random_texts(data: pd.DataFrame, n=5) -> None:\n",
    "    for text in data.sample(n)[\"Text\"].values:\n",
    "        print(text)\n",
    "        print(\"\\n\\n\")\n",
    "        \n",
    "        "
   ],
   "id": "8c40e3043864d4c1",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T20:04:56.734841Z",
     "start_time": "2024-05-06T20:04:56.528623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = preprocess_text(data)\n",
    "data.head()"
   ],
   "id": "b24eff34bcbc7220",
   "outputs": [
    {
     "data": {
      "text/plain": "                                               Title  \\\n0  A Beginner’s Guide to Word Embedding with Gens...   \n1  Hands-on Graph Neural Networks with PyTorch & ...   \n2                       How to Use ggplot2 in Python   \n3  Databricks: How to Save Data Frames as CSV Fil...   \n4  A Step-by-Step Implementation of Gradient Desc...   \n\n                                                Text  \n0  1. Introduction of Word2vec Word2vec is one of...  \n1  In my last article, I introduced the concept o...  \n2  Introduction Thanks to its strict implementati...  \n3  Photo credit to Mika Baumeister from Unsplash ...  \n4  A Step-by-Step Implementation of Gradient Desc...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>1. Introduction of Word2vec Word2vec is one of...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n      <td>In my last article, I introduced the concept o...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>How to Use ggplot2 in Python</td>\n      <td>Introduction Thanks to its strict implementati...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Databricks: How to Save Data Frames as CSV Fil...</td>\n      <td>Photo credit to Mika Baumeister from Unsplash ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Feature engineering for text data EDA"
   ],
   "id": "5cd572873b2395ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T20:05:02.337003Z",
     "start_time": "2024-05-06T20:04:56.739052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from spacy.lang.en import English\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "nlp = English()\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "data[\"text_words_num\"] = data[\"Text\"].apply(lambda x: len(x.split()))\n",
    "data[\"text_length\"] = data[\"Text\"].apply(lambda x: len(x))\n",
    "data[\"text_sentences_num\"] = data[\"Text\"].progress_apply(lambda x: len(list(nlp(x).sents)))\n",
    "data[\"token_count\"] = data[\"Text\"].apply(lambda x: len(x) / 4)\n",
    "data.head()"
   ],
   "id": "c11a3100d66138be",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1391/1391 [00:03<00:00, 352.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                               Title  \\\n0  A Beginner’s Guide to Word Embedding with Gens...   \n1  Hands-on Graph Neural Networks with PyTorch & ...   \n2                       How to Use ggplot2 in Python   \n3  Databricks: How to Save Data Frames as CSV Fil...   \n4  A Step-by-Step Implementation of Gradient Desc...   \n\n                                                Text  text_words_num  \\\n0  1. Introduction of Word2vec Word2vec is one of...            1489   \n1  In my last article, I introduced the concept o...             139   \n2  Introduction Thanks to its strict implementati...             953   \n3  Photo credit to Mika Baumeister from Unsplash ...             280   \n4  A Step-by-Step Implementation of Gradient Desc...             737   \n\n   text_length  text_sentences_num  token_count  \n0        10432                  64      2608.00  \n1          827                   7       206.75  \n2         5632                  45      1408.00  \n3         1776                  16       444.00  \n4         4744                  28      1186.00  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Text</th>\n      <th>text_words_num</th>\n      <th>text_length</th>\n      <th>text_sentences_num</th>\n      <th>token_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>1. Introduction of Word2vec Word2vec is one of...</td>\n      <td>1489</td>\n      <td>10432</td>\n      <td>64</td>\n      <td>2608.00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n      <td>In my last article, I introduced the concept o...</td>\n      <td>139</td>\n      <td>827</td>\n      <td>7</td>\n      <td>206.75</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>How to Use ggplot2 in Python</td>\n      <td>Introduction Thanks to its strict implementati...</td>\n      <td>953</td>\n      <td>5632</td>\n      <td>45</td>\n      <td>1408.00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Databricks: How to Save Data Frames as CSV Fil...</td>\n      <td>Photo credit to Mika Baumeister from Unsplash ...</td>\n      <td>280</td>\n      <td>1776</td>\n      <td>16</td>\n      <td>444.00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n      <td>737</td>\n      <td>4744</td>\n      <td>28</td>\n      <td>1186.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T20:05:02.346688Z",
     "start_time": "2024-05-06T20:05:02.337878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data.describe().round(2)"
   ],
   "id": "57b7d0f23dda3224",
   "outputs": [
    {
     "data": {
      "text/plain": "       text_words_num  text_length  text_sentences_num  token_count\ncount         1391.00      1391.00             1391.00      1391.00\nmean           901.54      5530.81               44.46      1382.70\nstd            885.73      5521.48               44.17      1380.37\nmin             49.00       249.00                2.00        62.25\n25%            315.00      1886.00               15.00       471.50\n50%            516.00      3040.00               26.00       760.00\n75%           1227.00      7577.50               60.00      1894.38\nmax           7657.00     46966.00              376.00     11741.50",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_words_num</th>\n      <th>text_length</th>\n      <th>text_sentences_num</th>\n      <th>token_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1391.00</td>\n      <td>1391.00</td>\n      <td>1391.00</td>\n      <td>1391.00</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>901.54</td>\n      <td>5530.81</td>\n      <td>44.46</td>\n      <td>1382.70</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>885.73</td>\n      <td>5521.48</td>\n      <td>44.17</td>\n      <td>1380.37</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>49.00</td>\n      <td>249.00</td>\n      <td>2.00</td>\n      <td>62.25</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>315.00</td>\n      <td>1886.00</td>\n      <td>15.00</td>\n      <td>471.50</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>516.00</td>\n      <td>3040.00</td>\n      <td>26.00</td>\n      <td>760.00</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1227.00</td>\n      <td>7577.50</td>\n      <td>60.00</td>\n      <td>1894.38</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>7657.00</td>\n      <td>46966.00</td>\n      <td>376.00</td>\n      <td>11741.50</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Chunking articles into smaller sentences"
   ],
   "id": "22d860d188b7736"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T20:05:02.359451Z",
     "start_time": "2024-05-06T20:05:02.347356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for _, row in data.sample(5).iterrows():\n",
    "    doc = nlp(row[\"Text\"])\n",
    "    sents = [sent.text for sent in doc.sents]\n",
    "    print(sents)\n",
    "    print(\"\\n\\n\")"
   ],
   "id": "92405363daaaef13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['January 1st is looming, along with the promises that come with the stroke of midnight.', 'So many things to commit to starting or conversely stop.', 'But sometimes our career goals are omitted from our new year resolutions.', 'This is not very prudent since work is such an important part of our lives that should be in our best interest to become better at what we do for living.', 'We should carefully be considering where we want to be ten years down the line and plan 🅝🅞🅦 how to get there.', 'The end of the decade is a great time to reflect on what has or has not worked for us, and to think about what we can do differently to achieve our goals in the next one.', 'So do take a breather from programming and before you immerse yourself completely in the same old grind, take a few moments to reflect and resolve.', 'With that in mind, here are some resolution ideas and a few accompanied resources to set yourself on a new track.', '⭐️ Pro Tip: As you approach this list, do not be vague.', 'Think to yourself: \"What shall I do, by when\".', 'You need to make a plan: As you approach this list, do not be vague.', 'Think to yourself: \"What shall I do, by when\".', 'You need to make a plan: SMART goals are specific, measurable, achievable, relevant and time bound.', 'Let’s go through the list together: — 1: Learn the art of software crafting It doesn’t take a huge amount of knowledge and skill to get a program working.', 'Everyone can do it.', 'Getting something to work is not a hard thing — getting it right is!', 'Imagine the very first program you wrote when you took your first programming class.', 'And compare it to now.', 'See how far you have come?', 'Let me now introduce you to the magical world of design patterns, solid principles and software architecture paradigms to see how you can position yourself with the elect few that write elegant software that lasts, has high cohesion and loose coupling and most notably is easy to comprehend, extend and maintain. “', 'Architecture is about the important stuff.', 'Whatever that is.” —', 'Ralph Johnson Here are a few all-time classics you should strive to read this year:']\n",
      "\n",
      "\n",
      "\n",
      "['Introduction Thanks to its strict implementation of the grammar of graphics, ggplot2 provides an extremely intuitive and consistent way of plotting your data.', 'Not only does ggplot2’s approach to plotting ensure that each plot comprises certain basic elements but it also simplifies the readability of your code to a great extent.', 'However, if you are a frequent user of Python, then implementing the grammar of graphics can be extremely challenging due to the lack of standardized syntax in popular plotting libraries, such as matplotlib or seaborn.', 'Should you still want to make use of the grammar of graphics, then the Python package plotnine provides an alternative for you.', 'The Grammar of Graphics In case you should be unfamiliar with the grammar of graphics, here is a quick overview: Main Components of the Grammar of Graphics As you can see, there are several components that make up the grammar of graphics, starting with your data.', 'After identifying the data you would like to visualize, you have to specify the variables you are interested in.', 'For instance, you might want to display one variable on the x-axis and another on the y-axis.', 'Third, you have to define what type of geometric object (geom for short) you would like to utilize.', 'This could be anything from a bar plot to a scatter plot or any of the other existing plot types.', 'These first three components are compulsory.', 'Without data, there is nothing to plot.', 'Without axis definitions, there is nothing to plot either.', 'And finally, without defining a geometric object, you will only see an empty coordinate system.', 'The remaining components making up the grammar of graphics are optional and can be implemented to improve visualizations.', 'Facets refer to specifications of subplots, that is, plotting several variables within your data next to one another in separate plots.', 'Statistical transformations mainly refer to the inclusion of summary statistics in your plot, such as the median or percentiles.', 'Coordinates describe the different coordinate systems available to you.', 'The most used and default coordinate system is the Cartesian coordinate system.', 'Depending on the structure of the data you would like to plot, lesser used coordinate systems, such as the Polar coordinate system, might provide a better way of visualizing your data.', 'Finally, themes provide a variety of options to design all non-data elements of your plot, such as the legend, background, or annotations.', 'While there are many ways of visualizing the grammar of graphics, I particularly like the one I created above because it implies the additivity of these layers as well as the fact that they are building upon one another.', 'If you have ever used ggplot2, you are familiar with the ‘+’ in its syntax that symbolizes the same idea described above.', 'plotnine plotnine is a Python package allowing you to use ggplot2-like code that is implementing the grammar of graphics.', 'By doing so, just as in ggplot2, you are able to specifically map data to visual objects that make up the visualization.', 'This enables you to improve both the readability as well as the structure of your code.', 'While you could set matplotlib’s style to ggplot, you cannot implement the grammar of graphics in matplotlib the same way you can in ggplot2.', 'Installation Before getting started, you have to install plotnine.', 'As always, there are two main options for doing so: pip and conda.', 'Plotting Having installed plotnine, you can get started plotting using the grammar of graphics.', 'Let us begin by building a very simple plot only using the three requisite components: data, aesthetics, and geometric objects.', 'Building a plot using the grammar of graphics As you can see, the syntax is very similar to ggplot2.', 'First, we specify the data source.', 'In our case, the data we are using is the classic mpg data set.', 'Next, we define that the variable ‘class’ is going to be displayed on the x-axis.', 'Lastly, we say that we would like to use a bar plot with bars of size 20 to visualize our data.', 'Let us look at the complete code and the resulting plot: The code above will yield the following output: While this is a good start, it is not very nice to look at yet.', 'Let us use other components of the grammar of graphics to beautify our plot.', 'For instance, we could flip the axes using coord_flip() and customize the plot and axes titles with labs() to improve our plot.', 'Using the code chunk above, our plot would look like this: Plotting Multidimensional Data Besides basic plots, you can do almost everything you could otherwise do in ggplot2, such as plotting multidimensional data.', 'If you would like to visualize the relationships between three variables you could add aesthetics to an otherwise two-dimensional plot: Adding color to the aesthetics will prompt plotnine to display a two-dimensional plot using displ (engine displacement, in liters) on its x- and hwy (highway miles per gallon) on its y-axis and color the data according to the variable class.', 'We have also switched the geometric object to geom_point(), which will give us a scatter instead of a bar plot.', 'Let us take a look at what that would look like: Conclusion As you can see, plotnine provides you with the ability to utilize the grammar of graphics within Python.', 'This increases the readability of your code and allows you to specifically map parts of your data to visual objects.', 'If you are already familiar with ggplot2, then you won’t have to learn anything new to master plotnine.', 'If not, here is a link to the ggplot2 website on which you can find out plenty more about the grammar of graphics and all types of geometric objects available to you.']\n",
      "\n",
      "\n",
      "\n",
      "['By the end of the article you will be able to answer the following questions: What is machine learning?', 'Why is machine learning so hot right now?', 'What is driving performance?', 'What are the challenges?', 'What’s going on in machine learning and energy today?', 'What might the future of energy and machine learning hold?', 'Glossary Artificial intelligence = machines that perceive the environment and take actions to achieve goals.', 'Machine learning = a branch of AI, that gives computers the ability to learn high dimensional patterns from data.', 'Deep learning = a family of machine learning models, that use multi-layered neural networks to approximate functions.', 'What is machine learning?', 'The business plans of the next 10,000 startups are easy to forecast: Take X and add AI — Kevin Kelly Machine learning is the biggest advance in how we can do engineering since the scientific method — Steve Juvertson The hype has officially peaked — deep learning is right at the top of the peak of inflated expectations.', 'The 2018 Gartner Hype Cycle Deep learning is foundation of the hype in modern machine learning.', 'Deep learning uses complex, many layered neural networks to learn patterns from large datasets.', 'This is the primary intelligence of machine learning — pattern recognition.', 'Machine learning has blown past the previous state of the art across a wide range of difficult problems.', 'It is impacting every industry — this ability stems from the capability of neural networks to learn from the same raw high dimensional data that we use and learn from, such as images or text.', 'So where are we today?', 'Currently, machine learning forms a core part of the breakthrough performance in computer vision and language processing.', 'The state of the art on tasks object recognition, image classification, speech recognition and language translation are all powered by deep learning.', 'Google, Amazon and Facebook all have world class AI labs and much of their business has been transformed by machine learning.', 'The potential of machine learning is more latent in industries that are less digitized (such as healthcare, energy or education).', 'Machine learning versus artificial intelligence So far machine learning has provided narrow artificial intelligence (AI).', 'The power of these systems are often superhuman, and more than enough to justify the hype around machine learning.', 'Typically the task involves perception, using high dimensional data (i.e. images).', 'This is the main contribution of machine learning — being able to create business value from raw, high dimensional data.', 'This narrow intelligence stands in contrast to the goal of many AI researchers — general AI, where a machine can perform a single machine can variety of tasks.', 'While it is almost certain that machine learning will form part of a general artificial intelligence, much more is needed to provide an intelligent machine that can perform a variety of tasks.', 'Machine learning and artificial intelligence shouldn’t be used interchangeably.', 'Machine learning is only a subset of the broader field of AI.', 'AI encompasses multiple distinct approaches that are beyond both the scope of this article.', 'One reason machine learning is often confused with AI is how well modern machine learning is performing — some researchers even think it’s all we will need to solve the general intelligence problem.', 'What exactly is needed is unclear, but we are many breakthroughs away from providing general intelligence.', 'Narrow superhuman machine intelligence is already here.', 'Three branches of machine learning To learn these patterns, machine learning makes use of three distinct learning signals, which separates machine learning into three branches.', 'The first is supervised learning, where the machine used labelled training data to learn how to predict the labels of unseen data.', 'Examples include time series forecasting, computer vision and language translation.', 'Supervised learning is the reason why Facebook can tell which of your friends is in your photo, or why Google can translate text from on a photo on your smart phone.', 'Recent progress in computer vision on the ImageNet benchmark — The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation The second is unsupervised learning, where the machine is able to generate new data without the supervision of labels.', 'Examples include artistic style transfer and generating realistic faces.', 'Generative Adversarial Networks (GANs) learn to generate realistic images using two competing neural networks.', 'One network generates images (the generator) and a second network has to decide if the image is real or fake.', 'This kind of adversarial learning is can be effective.', 'All the images to the right are machine generated — Karras et.', 'al (2019) A Style-Based Generator Architecture for Generative Adversarial Networks (NVIDIA) Adversarial learning can also be used in our final branch of machine learning — reinforcement learning.', 'In reinforcement learning the machine learns to select actions with the supervision of a scalar reward signal.', 'Reinforcement learning is applicable to a wide range of decision making problems with a reward signal, such as cost or carbon emissions.', 'The crown jewel of modern reinforcement learning is the 2016 AlphaGo victory over Lee Sedol.', 'Go was the last great challenge for AI in board games — creating a superhuman Go computer was thought to be a decade away.', 'AlphaGo used deep neural networks to to map from the high dimensional board state to an optimal next move.', 'Lee Sedol in his eventual 4–1 loss to AlphaGo — AlphaGo film AlphaGo stands in contrast to Deep Blue, the computer that solved chess with a 1996 victory over Garay Kasparov.', 'All of Deep Blue’s intelligence originated from a team of programmers and chess Grandmasters, who handcrafted moves into the machine.', 'Contrast this to the supervision that AlphaGo used — learning by playing itself at the game of Go.', 'DeepMind (the London lab behind AlphaGo) then introduced AlphaZero, which beat the previous version 100–0.', 'This version never relied on a dataset of human expert moves, and learnt entirely from adversarial self-play.', 'Why is machine learning so hot right now?', 'Each of these three branches is undergoing a tremendous period of performance, research activity and hype.', 'Fundamental to all of this improvement is deep learning — the use of multiple layer neural networks as complex function approximators.', 'These artificial neural networks are inspired by the biological neural networks in our own brains.', 'The artificial neural networks used by machines are much simpler — yet they are powering the performance of modern machine learning.', 'A neural network is like any other function — it takes some input and produces an output.', 'Reducing a high dimensional sample of data to a lower dimension is the fundamental process in machine learning.', 'Examples include predicting solar power generation from satellite images, or dispatching a battery from grid data.', 'Neural networks are general purpose.', 'If neural networks were only applicable in computer vision, this would still be a huge deal.', 'Yet neural networks are pushing the boundaries in multiple directions.', 'This generalizability of neural networks has allowed them to be state of the art across a wide range of problems, and also allows machine learning to be applied to a wide range of industries.', 'The atomic unit of a neural network is the perceptron — a simple model that combines input from other perceptrons, squeezes it through a non-linear function (such as a sigmoid or rectifier) and sends output to child perceptrons.', 'The mathematics of this are beyond the scope of this article — the key takeaway is that stacking these perceptrons together into many layers allows neural networks to learn complex functions from large amounts of data.', 'There is more to machine learning than just deep neural networks — algorithms like logistic regression and random forests are suitable for many business problems.', 'The problem with classical machine learning is that it doesn’t benefit from massive amounts of data.', 'Because the capacity of a neural network can be increased by adding depth, neural networks are able to break through the limits of classical machine learning models.', 'Deep neural networks are able to learn from massive amounts of data — adapted from ‘AI is the New Electricity’ (Andrew Ng) Layers are selected to take advantage of the structure in raw data.', 'Three common layers are fully connected, convolution and recurrent.', 'Deep convolutional neural network used in the 2015 DeepMind Atari work The convolutional layer is inspired by our own visual cortex, and is what powers modern computer vision.', 'They allow machines to see.', 'They can be used to classify the contents of the image, recognize faces and create captions for images.', 'An unrolled recurrent neural network — colah’s blog A recurrent layer processes input and generates output as sequences, and powers modern natural language processing.', 'Recurrent networks allow machines to understand the temporal structure in data, such as words in a sentence.', 'The ability to see and understand language not only drives performance, it also allows machine learning to generalize.', 'Vision and language understanding are low level skills used in essentially every domain of human life.', 'Mastering these low level skills means that machines can be useful in a range of industries.', 'Energy is no different.', 'What’s driving the performance of modern machine learning?', 'The performance of modern deep learning is driven by the interaction of two processes — the increased availability of data and the ability to train large models with lots of data.', 'The rise of the internet and devices that generate raw data (sensors, images and text) has lead to the curation of massive datasets.', 'These massive datasets are the food of deep neural networks — without the data, the models can’t learn.', 'The ability to train large models rests upon the ability to access specialized hardware in the cloud.', 'In the 2000’s researchers repurposed hardware designed for video games (graphics processing units, or GPUs) to train neural networks.', 'This led to dramatic speedup in training times, which is important — all our understanding of machine learning is empirical (learned through experiment).', 'The second hardware trend is cloud computing.', 'The cloud gives access to computation on a fully variable cost basis.', 'Platforms such as Amazon Web Services allow on-demand access to a large amount of GPU-enabled computing power with cheap data storage alongside it.', 'This access to computing power works both vertically within large technology companies and for smaller companies.', 'It enables the balance sheet benefit of shifting a capital expense (building data centres) into an operating expense.', 'A final trend driving modern machine learning is access to algorithms and tools.', 'Almost all the relevant literature for machine learning is available for free on sites like arXiv.', 'It’s also possible to access high quality implementations of machine learning tools on GitHub.', 'This tendency for openness stands in stark contrast with the paywalls and licensed software of the energy industry.', 'Challenges There are a wide range of challenges in machine learning.', 'Examining them all is outside the scope of this article — issues such as interpretability, worker displacement and misuse of powerful narrow AI are significant issues and the focus of much research.', 'There is also much work to be done extending the powerful, narrow machine intelligence we have today into a general artificial intelligence.', 'Instead we will focus on challenges specific to using machine learning on energy problems.', 'The primary challenge is access to data.', 'The energy industry is still in the process of digitization — all my work in the energy has involved setting up the basic data collection infrastructure.', 'We’ve seen how important large amounts of data is to machine learning — a lack of historical data can be a show stopper for many energy and machine learning projects.', 'Forward thinking energy companies know that data can only be collected once.', 'It’s more than just a local historian recording data from the site control system.', 'The 21st century energy company has everything from sensor level data to accounting data available to the employees and machines that need it, worldwide and in near real time.', 'The curation of large and interesting datasets is one of the few defensible advantages an energy company can build (another is brand).', 'These datasets are valuable not only because of how we can use them today, but because of the insights that can be generated tomorrow.', 'When thinking about applying machine learning to an energy problem, the first and most important consideration is the dataset.', 'In fact, the first step in many machine learning projects is the same — start collecting data.', 'A dataset for supervised machine learning has two parts — the features (such as images or raw text) and the target (what you want to predict).', 'A dataset for reinforcement learning is a simulator — something that the learning algorithm can interact with.', 'Some potential applications of machine learning in energy include (but are not limited too): predictive maintenance customer segmentation churn prediction and minimization Now we will dive into the details of a few applications of machine learning in energy that are happening today.', 'Time series forecasting The economics and environmental impact of energy depends on time of use.', 'Forecasting has always been an important practice in energy — increased deployment of variable wind and solar makes forecasting more valuable.', 'DeepMind have claimed a 20 % improvement in the value of energy using a 36 hour ahead forecast.', 'Better forecasts can increase the value of renewables and reduce the requirement for backup fossil fuels.', 'Particularly exciting is the ability to forecast wind or solar using satellite images and deep convolutional neural nets — see the work of Jack Kelly and Dan Travers at Open Climate Fix.', 'Control Optimal control of complex energy systems is hard.', 'Reinforcement learning is a framework for decision making that can be applied to a number of energy control problems, availability of reward signals, simulators Better control of our energy systems will allow us to reduce cost, reduce environmental impact and improve safety.', 'DeepMind’s data centre optimization is the most famous example of energy and machine learning.', 'The algorithm makes decisions on a five minute basis using thousands of sensors.', 'These sensors feed data into deep neural networks, which predict how different combinations of actions will affect the future efficiency of cooling generation.', 'This sounds like a form of DQN — a reinforcement learning algorithm that predicts future reward for each possible action.', 'The neural networks perform computations on the cloud, with the suggested action sent back to the data centre before safety verification by the local control system.', 'Performance of the data centre measured using energy input per ton of cooling (kW/tonC), and improves with more data, from an initial 12% to 30% over nine months.', 'Key Takeaways We’ve just had a whirlwind introduction to machine learning.', 'Key takeaways are: machine learning is the subset of AI that is working machine learning has three broad branches — supervised, unsupervised and reinforcement learning deep learning is powering modern machine learning convolution for vision, recurrent for sequences performance is driven by the availability of data, cloud compute and algorithms For more technical and non-technical machine learning resources, check out ml-resources.', 'For reinforcement learning resources, check out rl-resources.', 'Thanks for reading!']\n",
      "\n",
      "\n",
      "\n",
      "['Meta Intelligence - Writing Programs That Write Programs (Part 1: Genetic Evolution) Metaprogramming with AI Greg Surma · Follow 5 min read · Jun 4, 2019 -- 3 Share In today’s article, we are going to learn how to write programs that write programs.', 'The notion of programs that can generate other programs is called metaprogramming and by the end of this article, you will be able to create your own code-generating system.', 'Take a look at the following example of a self-generated program that prints ‘HI’ to the console.', '++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++.+.', 'Brainf*ck If you are confused by the above code, don’t worry - you are not alone.', 'It’s written in an esoteric, though Turing complete programming language called Brainf*ck which is notorious for i’s unreadability.', 'However, while being extra hard to understand for humans, it’s very simple to understand for computers.', 'It has only 8 instructions “>” Increment the pointer. “', '<” Decrement the pointer. “', '+” Increment the byte at the pointer. “-”', 'Decrement the byte at the pointer. “.”', 'Output the byte at the pointer. “[“', 'Jump forward past the matching ] if the byte at the pointer is zero. “]”]', 'Jump backward to the matching [ unless the byte at the pointer is zero. “,”', 'Input a byte and store it in the byte at the pointer.', 'so it’s relatively easy to write an interpreter for it.', 'If you are curious, take a look at the Python interpreter I wrote for this project.', 'But why do we need this weird language in the first place?', 'Let’s define our project’s problem first.', 'We are striving to generate a program that does something simple, like for example printing some text on a screen.', 'Okay, but what is a ‘program’?', 'Without going too much into the technical details, we can define a program as a collection of instructions that are being executed by a computer.', 'With that being said, our goal is to find this specific collection of instructions that after being executed by a computer, will output a specific text on a screen.']\n",
      "\n",
      "\n",
      "\n",
      "['Definitive Guide to Build Don’t Sleep: Building your first Drowsiness Detection System A sleeping student in front of laptop — Extracted from Medical News Today Introduction It was a very important day, the test and project deadline were due next week but you hadn’t prepared much because of the new Halo release.', 'Out of anxiety, you rushed to library and opened your laptop.', 'However, as you lost your sleep leveling up your character, your mind quickly took you to dreamland.', 'Coffee did no avail as you slept soundly in front of your laptop.', 'Time was running short and You were desperate to stay awake.', 'What if I tell you… Your laptop could help you stay awake.', 'The Solution: Drowsiness Detection System (DDS) And an alarm… Imagine you were that pitiful guy, you could have activated DDS app installed in your laptop.', 'This would trigger your laptop webcam.', 'Every time you fell drowsy, your laptop would notice and ring an alarm to your headphone.', 'After you woke up,your laptop would notice and turned off the alarm.', 'You could then resume your work.', 'Cool use case..?', 'Now, let us figure out how to develop the DDS model.', 'Hope you are ready ☺ Drowsiness Detection System Coded in Matlab How do we know you are sleepy?', 'The Naive Analysis of Sleepiness Imagine your friend’s or loved one’s sleepy face, how did you know they were sleepy?', 'Well, the most obvious signs are: Eyes: eyes are open (awake), eyes are closed (sleepy) Mouth: Mouth is closing (awake), mouth is split/droopy (sleepy) Head Position: Head is at the same position(awake), head is bobbing (sleepy) How do we teach our computer to notice these signs?', 'The Computer Vision Analysis']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T20:05:02.362834Z",
     "start_time": "2024-05-06T20:05:02.360236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_sentences_in_chunk = 10\n",
    "CONTEXT_WINDOW = 384\n",
    "TOKEN_SIZE = 4\n",
    "\n",
    "\n",
    "def split_text_into_chunks(text: str, n_sentences_in_chunk: int, overlap=0) -> list:\n",
    "    doc = nlp(text)\n",
    "    sents = [sent.text for sent in doc.sents]\n",
    "    chunks = []\n",
    "    n_sentences_in_chunk = n_sentences_in_chunk - overlap\n",
    "    remainder = 0\n",
    "    for i in range(overlap, len(sents), n_sentences_in_chunk):\n",
    "        if remainder == n_sentences_in_chunk:\n",
    "            remainder = 0\n",
    "        chunk = sents[i-overlap-remainder:i + n_sentences_in_chunk-remainder]\n",
    "        remainder = 0\n",
    "        while len(\" \".join(chunk)) / TOKEN_SIZE > CONTEXT_WINDOW:\n",
    "            remainder += 1\n",
    "            chunk = chunk[:-remainder]\n",
    "            \n",
    "        chunk = \" \".join(chunk)\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ],
   "id": "977a964a77580255",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T20:05:05.297204Z",
     "start_time": "2024-05-06T20:05:02.363539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dict_data = data.to_dict(\"records\")  \n",
    "\n",
    "for elem in tqdm(dict_data):\n",
    "    elem[\"chunks\"] = split_text_into_chunks(elem[\"Text\"], n_sentences_in_chunk, overlap=1)"
   ],
   "id": "c6a107f76d07d0f9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1391/1391 [00:02<00:00, 474.82it/s]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Example of chunking"
   ],
   "id": "2b4f31665d15786b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T20:05:05.299868Z",
     "start_time": "2024-05-06T20:05:05.297999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for chunk in dict_data[0][\"chunks\"]:\n",
    "    print(len(chunk) / 4)\n",
    "    print(chunk)\n",
    "    print(\"\\n\\n\")"
   ],
   "id": "8434fbb942475dae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285.0\n",
      "1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.\n",
      "\n",
      "\n",
      "\n",
      "293.5\n",
      "For instance, it will have two vector representations for Apple, one for the company and another for the fruit. For more details about the word2vec algorithm, please check here. 2. Gensim Python Library Introduction Gensim is an open source python library for natural language processing and it was developed and is maintained by the Czech natural language processing researcher Radim Řehůřek. Gensim library will enable us to develop word embeddings by training our own word2vec models on a custom corpus either with CBOW of skip-grams algorithms. At first, we need to install the genism package. Gensim runs on Linux, Windows and Mac OS X, and should run on any other platform that supports Python 2.7+ and NumPy. Gensim depends on the following software: Python >= 2.7 (tested with versions 2.7, 3.5 and 3.6) >= 2.7 (tested with versions 2.7, 3.5 and 3.6) NumPy >= 1.11.3 >= 1.11.3 SciPy >= 0.18.1 >= 0.18.1 Six >= 1.5.0 >= 1.5.0 smart_open >= 1.2.1 There are two ways for installation. We could run the following code in our terminal to install genism package. pip install --upgrade gensim Or, alternatively for Conda environments: conda install -c conda-forge gensim 3.\n",
      "\n",
      "\n",
      "\n",
      "322.0\n",
      "pip install --upgrade gensim Or, alternatively for Conda environments: conda install -c conda-forge gensim 3. Implementation of word Embedding with Gensim Word2Vec Model In this tutorial, I will show how to generate word embedding with genism using a concrete example. The dataset I used for this tutorial is from Kaggle Dataset. This vehicle dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The full python tutorial can be found here. >>> df = pd.read_csv('data.csv') >>> df.head() 3.1 Data Preprocessing: Since the purpose of this tutorial is to learn how to generate word embeddings using genism library, we will not do the EDA and feature selection for the word2vec model for the sake of simplicity. Genism word2vec requires that a format of ‘list of lists’ for training where every document is contained in a list and every list contains lists of tokens of that document. At first, we need to generate a format of ‘list of lists’ for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains lists of features of that make model.\n",
      "\n",
      "\n",
      "\n",
      "341.0\n",
      "To be more specific, each make model is contained in a list and every list contains lists of features of that make model. To achieve this, we need to do the following things : a. Create a new column for Make Model >>> df['Maker_Model']= df['Make']+ \" \" + df['Model'] b. Generate a format of ‘ list of lists’ for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size, Vehicle Style. # Select features from original dataset to form a new dataframe >>> df1 = df[['Engine Fuel Type','Transmission Type','Driven_Wheels','Market Category','Vehicle Size', 'Vehicle Style', 'Maker_Model']] # For each row, combine all the columns into one column >>> df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1) # Store them in a pandas dataframe >>> df_clean = pd. DataFrame({'clean': df2}) # Create the list of list format of the custom corpus for gensim modeling >>> sent = [row.split(',') for row in df_clean['clean']] # show the example of list of list format of the custom corpus for gensim modeling >>> sent[:2] [['premium unleaded (required)', 'MANUAL', 'rear wheel drive', 'Factory Tuner', 'Luxury', 'High-Performance', 'Compact', 'Coupe', 'BMW 1 Series M'], ['premium unleaded (required)', 'MANUAL', 'rear wheel drive', 'Luxury', 'Performance', 'Compact', 'Convertible', 'BMW 1 Series']] 3.2.\n",
      "\n",
      "\n",
      "\n",
      "359.5\n",
      "window: The maximum distance between a target word and words around the target word. The default window is 5. min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5. workers: The number of partitions during training and the default workers is 3. sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW. After training the word2vec model, we can obtain the word embedding directly from the training model as following. >>> model['Toyota Camry'] array([-0.11884457, 0.03035539, -0.0248678 , -0.06297892, -0.01703234, -0.03832747, -0.0825972 , -0.00268112, -0.09192555, -0.08458661, -0.07199778, 0.05235871, 0.21303181, 0.15767808, -0.1883737 , 0.01938575, -0.24431638, 0.04261152, 0.11865819, 0.09881561, -0.04580643, -0.08342388, -0.01355413, -0.07892415, -0.08467747, -0.0040625 , 0.16796461, 0.14578669, 0.04187112, -0.01436194, -0.25554284, 0.25494182, 0.05522631, 0.19295982, 0.14461821, 0.14022525, -0.2065216 , -0.05020927, -0.08133671, 0.18031682, 0.35042757, 0.0245426 , 0.15938364, -0.05617865, 0.00297452, 0.15442047, -0.01286271, 0.13923576, 0.085941 , 0.18811756], dtype=float32) 4. Compute Similarities Now we could even use Word2vec to compute the similarity between two Make Models in the vocabulary by invoking the model.similarity( ) and passing in the relevant words.\n",
      "\n",
      "\n",
      "\n",
      "327.5\n",
      "We also can use the built-in function model.most_similar() to get a set of the most similar make models for a given make model based on the Euclidean distance. >>> model1.most_similar('Mercedes-Benz SLK-Class')[:5] [('BMW M4', 0.9959905743598938), ('Maserati Coupe', 0.9949707984924316), ('Porsche Cayman', 0.9945154190063477), ('Mercedes-Benz SLS AMG GT', 0.9944609999656677), ('Maserati Spyder', 0.9942780137062073)] However, Euclidian similarity cannot work well for the high-dimensional word vectors. This is because Euclidian similarity will increase as the number of dimensions increases, even if the word embedding stands for different meanings. Alternatively, we can use cosine similarity to measure the similarity between two vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at a 0-degree angle. The following function shows how can we generate the most similar make model based on cosine similarity. def cosine_distance (model, word,target_list , num) : cosine_dict ={} word_list = [] a = model[word] for item in target_list : if item !\n",
      "\n",
      "\n",
      "\n",
      "317.25\n",
      "def cosine_distance (model, word,target_list , num) : cosine_dict ={} word_list = [] a = model[word] for item in target_list : if item ! = word : b = model [item] cos_sim = dot(a, b)/(norm(a)*norm(b)) cosine_dict[item] = cos_sim dist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order for item in dist_sort: word_list.append((item[0], item[1])) return word_list[0:num] # only get the unique Maker_Model >>> Maker_Model = list(df. Maker_Model.unique()) # Show the most similar Mercedes-Benz SLK-Class by cosine distance >>> cosine_distance (model,'Mercedes-Benz SLK-Class',Maker_Model,5) [('Mercedes-Benz CLK-Class', 0.99737006), ('Aston Martin DB9', 0.99593246), ('Maserati Spyder', 0.99571854), ('Ferrari 458 Italia', 0.9952333), ('Maserati GranTurismo Convertible', 0.994994)] 5. T-SNE Visualizations It’s hard to visualize the word embedding directly, for they usually have more than 3 dimensions. T-SNE is a useful tool to visualize high-dimensional data by dimension reduction while keeping relative pairwise distance between points. It can be said that T-SNE looking for a new data representation where the neighborhood relations are preserved. The following code shows how to plot the word embedding with T-SNE plot.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Creating a dataset with chunks"
   ],
   "id": "9ce2507de8bc9739"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T20:05:09.513945Z",
     "start_time": "2024-05-06T20:05:05.302573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "chunks = pd.DataFrame(columns=[\"Title\", \"Chunk\", \"Chunk_length\", \"Chunk_words_num\", \"Chunk_sentences_num\", \"Token_count\"])\n",
    "\n",
    "for d in tqdm(dict_data):\n",
    "    for chunk in d[\"chunks\"]:\n",
    "        chunk = chunk.strip()\n",
    "        chunk = re.sub(r\"\\s+\", \" \", chunk)\n",
    "        row = pd.DataFrame.from_dict(\n",
    "            {\n",
    "                \"Title\": d[\"Title\"],\n",
    "                \"Chunk\": chunk,\n",
    "                \"Chunk_length\": len(chunk),\n",
    "                \"Chunk_words_num\": len(chunk.split()),\n",
    "                \"Chunk_sentences_num\": len(list(nlp(chunk).sents)),\n",
    "                \"Token_count\": len(chunk) / 4\n",
    "            },\n",
    "            orient=\"index\"\n",
    "        )\n",
    "        row = row.T\n",
    "        chunks = pd.concat([chunks, row], axis=0)\n",
    "        \n",
    "numeric = [\"Chunk_length\", \"Chunk_words_num\", \"Chunk_sentences_num\", \"Token_count\"]\n",
    "chunks[numeric] = chunks[numeric].astype(float)\n",
    "\n",
    "chunks.head()"
   ],
   "id": "cf491b4645bed9e8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1391/1391 [00:04<00:00, 331.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                               Title  \\\n0  A Beginner’s Guide to Word Embedding with Gens...   \n0  A Beginner’s Guide to Word Embedding with Gens...   \n0  A Beginner’s Guide to Word Embedding with Gens...   \n0  A Beginner’s Guide to Word Embedding with Gens...   \n0  A Beginner’s Guide to Word Embedding with Gens...   \n\n                                               Chunk  Chunk_length  \\\n0  1. Introduction of Word2vec Word2vec is one of...        1140.0   \n0  For instance, it will have two vector represen...        1174.0   \n0  pip install --upgrade gensim Or, alternatively...        1288.0   \n0  To be more specific, each make model is contai...        1364.0   \n0  window: The maximum distance between a target ...        1438.0   \n\n   Chunk_words_num  Chunk_sentences_num  Token_count  \n0            188.0                 10.0        285.0  \n0            194.0                 10.0        293.5  \n0            215.0                 10.0        322.0  \n0            204.0                  4.0        341.0  \n0            185.0                 10.0        359.5  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Chunk</th>\n      <th>Chunk_length</th>\n      <th>Chunk_words_num</th>\n      <th>Chunk_sentences_num</th>\n      <th>Token_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>1. Introduction of Word2vec Word2vec is one of...</td>\n      <td>1140.0</td>\n      <td>188.0</td>\n      <td>10.0</td>\n      <td>285.0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>For instance, it will have two vector represen...</td>\n      <td>1174.0</td>\n      <td>194.0</td>\n      <td>10.0</td>\n      <td>293.5</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>pip install --upgrade gensim Or, alternatively...</td>\n      <td>1288.0</td>\n      <td>215.0</td>\n      <td>10.0</td>\n      <td>322.0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>To be more specific, each make model is contai...</td>\n      <td>1364.0</td>\n      <td>204.0</td>\n      <td>4.0</td>\n      <td>341.0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>window: The maximum distance between a target ...</td>\n      <td>1438.0</td>\n      <td>185.0</td>\n      <td>10.0</td>\n      <td>359.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T20:05:09.518233Z",
     "start_time": "2024-05-06T20:05:09.514805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunks.sample(1)"
   ],
   "id": "3b943d25bf1b0980",
   "outputs": [
    {
     "data": {
      "text/plain": "                             Title  \\\n0  Telegramgate Analysis in Python   \n\n                                               Chunk  Chunk_length  \\\n0  These elements are ‘Fdo’ and ‘R Russello’. We ...         883.0   \n\n   Chunk_words_num  Chunk_sentences_num  Token_count  \n0            146.0                 10.0       220.75  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Chunk</th>\n      <th>Chunk_length</th>\n      <th>Chunk_words_num</th>\n      <th>Chunk_sentences_num</th>\n      <th>Token_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Telegramgate Analysis in Python</td>\n      <td>These elements are ‘Fdo’ and ‘R Russello’. We ...</td>\n      <td>883.0</td>\n      <td>146.0</td>\n      <td>10.0</td>\n      <td>220.75</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T20:05:09.525214Z",
     "start_time": "2024-05-06T20:05:09.518812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunks.describe().astype(float).round(2)"
   ],
   "id": "182c909727869b70",
   "outputs": [
    {
     "data": {
      "text/plain": "       Chunk_length  Chunk_words_num  Chunk_sentences_num  Token_count\ncount       7334.00          7334.00              7334.00      7334.00\nmean        1052.25           173.26                 8.80       263.06\nstd          331.35            54.58                 2.16        82.84\nmin            0.00             0.00                 0.00         0.00\n25%          865.00           144.00                 9.00       216.25\n50%         1100.00           182.00                10.00       275.00\n75%         1313.00           213.00                10.00       328.25\nmax         1536.00           291.00                10.00       384.00",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Chunk_length</th>\n      <th>Chunk_words_num</th>\n      <th>Chunk_sentences_num</th>\n      <th>Token_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>7334.00</td>\n      <td>7334.00</td>\n      <td>7334.00</td>\n      <td>7334.00</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1052.25</td>\n      <td>173.26</td>\n      <td>8.80</td>\n      <td>263.06</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>331.35</td>\n      <td>54.58</td>\n      <td>2.16</td>\n      <td>82.84</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>865.00</td>\n      <td>144.00</td>\n      <td>9.00</td>\n      <td>216.25</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1100.00</td>\n      <td>182.00</td>\n      <td>10.00</td>\n      <td>275.00</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1313.00</td>\n      <td>213.00</td>\n      <td>10.00</td>\n      <td>328.25</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1536.00</td>\n      <td>291.00</td>\n      <td>10.00</td>\n      <td>384.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T20:05:09.528554Z",
     "start_time": "2024-05-06T20:05:09.525818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunks = chunks[chunks[\"Token_count\"] > 0]"
   ],
   "id": "84904f60068b9204",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T20:05:09.535622Z",
     "start_time": "2024-05-06T20:05:09.529135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunks.describe()"
   ],
   "id": "a84e3540a747eec4",
   "outputs": [
    {
     "data": {
      "text/plain": "       Chunk_length  Chunk_words_num  Chunk_sentences_num  Token_count\ncount   7292.000000      7292.000000          7292.000000  7292.000000\nmean    1058.311437       174.255623             8.854361   264.577859\nstd      322.506653        53.123813             2.061306    80.626663\nmin        9.000000         2.000000             1.000000     2.250000\n25%      871.000000       145.000000             9.000000   217.750000\n50%     1101.000000       182.000000            10.000000   275.250000\n75%     1314.000000       213.250000            10.000000   328.500000\nmax     1536.000000       291.000000            10.000000   384.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Chunk_length</th>\n      <th>Chunk_words_num</th>\n      <th>Chunk_sentences_num</th>\n      <th>Token_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>7292.000000</td>\n      <td>7292.000000</td>\n      <td>7292.000000</td>\n      <td>7292.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1058.311437</td>\n      <td>174.255623</td>\n      <td>8.854361</td>\n      <td>264.577859</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>322.506653</td>\n      <td>53.123813</td>\n      <td>2.061306</td>\n      <td>80.626663</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>9.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>2.250000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>871.000000</td>\n      <td>145.000000</td>\n      <td>9.000000</td>\n      <td>217.750000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1101.000000</td>\n      <td>182.000000</td>\n      <td>10.000000</td>\n      <td>275.250000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1314.000000</td>\n      <td>213.250000</td>\n      <td>10.000000</td>\n      <td>328.500000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1536.000000</td>\n      <td>291.000000</td>\n      <td>10.000000</td>\n      <td>384.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Printing some chunks with low token count"
   ],
   "id": "789065d133527bc7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T20:05:09.538573Z",
     "start_time": "2024-05-06T20:05:09.536166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MIN_TOKENS = 50\n",
    "\n",
    "for ch in chunks[chunks[\"Token_count\"] < 50].sample(5)[\"Chunk\"].values:\n",
    "    print(ch)\n",
    "    print(\"\\n\\n\")\n",
    "\n"
   ],
   "id": "fcedfd78f12eea02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any doubts or suggestions feel free to ping me 😃. Also, find me on Twitter and Linkedin. Adios!!\n",
      "\n",
      "\n",
      "\n",
      "All code and plots are available on Github. Please let me know what insights you find! Check out this link for more Simpson analysis: https://www.youtube.com/watch?v=9D420SOmL6U.\n",
      "\n",
      "\n",
      "\n",
      "GitHub is an online platform based on Git. What do you need to get started with GitHub? Installation\n",
      "\n",
      "\n",
      "\n",
      "The previous article in this series can be found here. The next article in this series, “Knitting and Recommendations” is available here.\n",
      "\n",
      "\n",
      "\n",
      "Being a Data Scientist is a Lot Data and a Little Science You should also know what you are getting into before diving into data science. The daily work of a data scientist is not that…\n",
      "\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we can see most of these short sentences come from the end of the articles. We can safely remove them from the dataset as they bring little information to the table."
   ],
   "id": "d5a5f2674e24c2cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T20:05:09.541792Z",
     "start_time": "2024-05-06T20:05:09.539086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(chunks.shape)\n",
    "chunks = chunks[chunks[\"Token_count\"] > MIN_TOKENS]\n",
    "print(chunks.shape)"
   ],
   "id": "3849783fd100b09e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7292, 6)\n",
      "(7185, 6)\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Embedding the chunks"
   ],
   "id": "127c33dbe4ad5909"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d20d0d2861f3ef57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T20:05:23.103130Z",
     "start_time": "2024-05-06T20:05:09.542368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "embeddings_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "sentences = [\"This is a sample sentence\", \"I like to eat apples\"]\n",
    "embeddings = embeddings_model.encode(sentences)\n",
    "cos_sim = np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]))\n",
    "cos_sim"
   ],
   "id": "6ea949ed430b97dc",
   "outputs": [
    {
     "data": {
      "text/plain": "0.1260562"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Enabling CUDA if possible"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "276cd8e10b12b68f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:05:23.107318Z",
     "start_time": "2024-05-06T20:05:23.104821Z"
    }
   },
   "id": "54f65a30f42a10",
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T20:16:40.106649Z",
     "start_time": "2024-05-06T20:05:23.107974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embeddings_model.to(device)\n",
    "chunks_dict = chunks.to_dict(\"records\")\n",
    "\n",
    "for chunk in tqdm(chunks_dict):\n",
    "    embedding = embeddings_model.encode(chunk[\"Chunk\"])\n",
    "    chunk[\"Embedding\"] = embedding"
   ],
   "id": "7b63ce8600a0969",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7185/7185 [11:16<00:00, 10.61it/s] \n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  Title  \\\n0     A Beginner’s Guide to Word Embedding with Gens...   \n1     A Beginner’s Guide to Word Embedding with Gens...   \n2     A Beginner’s Guide to Word Embedding with Gens...   \n3     A Beginner’s Guide to Word Embedding with Gens...   \n4     A Beginner’s Guide to Word Embedding with Gens...   \n...                                                 ...   \n7180  Primer on The Importance of Mindful Data Colle...   \n7181  Primer on The Importance of Mindful Data Colle...   \n7182  Primer on The Importance of Mindful Data Colle...   \n7183  Primer on The Importance of Mindful Data Colle...   \n7184  Primer on The Importance of Mindful Data Colle...   \n\n                                                  Chunk  Chunk_length  \\\n0     1. Introduction of Word2vec Word2vec is one of...        1140.0   \n1     For instance, it will have two vector represen...        1174.0   \n2     pip install --upgrade gensim Or, alternatively...        1288.0   \n3     To be more specific, each make model is contai...        1364.0   \n4     window: The maximum distance between a target ...        1438.0   \n...                                                 ...           ...   \n7180  This is true of any research institution, even...        1387.0   \n7181  The issues of differing standards can ultimate...        1435.0   \n7182  The above excerpt comes from the abstract to t...         915.0   \n7183  Show me your final data. This is all extremely...         981.0   \n7184  Is the information that you have fully anonymi...         870.0   \n\n      Chunk_words_num  Chunk_sentences_num  Token_count  \\\n0               188.0                 10.0       285.00   \n1               194.0                 10.0       293.50   \n2               215.0                 10.0       322.00   \n3               204.0                  4.0       341.00   \n4               185.0                 10.0       359.50   \n...               ...                  ...          ...   \n7180            227.0                 10.0       346.75   \n7181            226.0                  9.0       358.75   \n7182            153.0                 10.0       228.75   \n7183            167.0                 10.0       245.25   \n7184            144.0                  8.0       217.50   \n\n                                              Embedding  \n0     [0.040338237, 0.012804469, -0.006745447, 0.042...  \n1     [0.05746932, 0.0059202667, -0.01848462, 0.0432...  \n2     [-0.008038099, 0.018741773, -0.010320838, 0.06...  \n3     [0.028384276, 0.01910937, -0.023285013, 0.0194...  \n4     [0.028748535, -0.047673915, 0.012459073, 0.064...  \n...                                                 ...  \n7180  [0.04206838, 0.033875473, -0.034986444, -0.038...  \n7181  [0.03998078, 0.11374257, -0.031096485, -0.0056...  \n7182  [0.04979147, 0.06545234, -0.033590827, -0.0121...  \n7183  [0.01828431, 0.087652445, -0.06963481, 0.00219...  \n7184  [-0.0077653565, 0.13505033, -0.03715556, 0.018...  \n\n[7185 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Chunk</th>\n      <th>Chunk_length</th>\n      <th>Chunk_words_num</th>\n      <th>Chunk_sentences_num</th>\n      <th>Token_count</th>\n      <th>Embedding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>1. Introduction of Word2vec Word2vec is one of...</td>\n      <td>1140.0</td>\n      <td>188.0</td>\n      <td>10.0</td>\n      <td>285.00</td>\n      <td>[0.040338237, 0.012804469, -0.006745447, 0.042...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>For instance, it will have two vector represen...</td>\n      <td>1174.0</td>\n      <td>194.0</td>\n      <td>10.0</td>\n      <td>293.50</td>\n      <td>[0.05746932, 0.0059202667, -0.01848462, 0.0432...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>pip install --upgrade gensim Or, alternatively...</td>\n      <td>1288.0</td>\n      <td>215.0</td>\n      <td>10.0</td>\n      <td>322.00</td>\n      <td>[-0.008038099, 0.018741773, -0.010320838, 0.06...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>To be more specific, each make model is contai...</td>\n      <td>1364.0</td>\n      <td>204.0</td>\n      <td>4.0</td>\n      <td>341.00</td>\n      <td>[0.028384276, 0.01910937, -0.023285013, 0.0194...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n      <td>window: The maximum distance between a target ...</td>\n      <td>1438.0</td>\n      <td>185.0</td>\n      <td>10.0</td>\n      <td>359.50</td>\n      <td>[0.028748535, -0.047673915, 0.012459073, 0.064...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7180</th>\n      <td>Primer on The Importance of Mindful Data Colle...</td>\n      <td>This is true of any research institution, even...</td>\n      <td>1387.0</td>\n      <td>227.0</td>\n      <td>10.0</td>\n      <td>346.75</td>\n      <td>[0.04206838, 0.033875473, -0.034986444, -0.038...</td>\n    </tr>\n    <tr>\n      <th>7181</th>\n      <td>Primer on The Importance of Mindful Data Colle...</td>\n      <td>The issues of differing standards can ultimate...</td>\n      <td>1435.0</td>\n      <td>226.0</td>\n      <td>9.0</td>\n      <td>358.75</td>\n      <td>[0.03998078, 0.11374257, -0.031096485, -0.0056...</td>\n    </tr>\n    <tr>\n      <th>7182</th>\n      <td>Primer on The Importance of Mindful Data Colle...</td>\n      <td>The above excerpt comes from the abstract to t...</td>\n      <td>915.0</td>\n      <td>153.0</td>\n      <td>10.0</td>\n      <td>228.75</td>\n      <td>[0.04979147, 0.06545234, -0.033590827, -0.0121...</td>\n    </tr>\n    <tr>\n      <th>7183</th>\n      <td>Primer on The Importance of Mindful Data Colle...</td>\n      <td>Show me your final data. This is all extremely...</td>\n      <td>981.0</td>\n      <td>167.0</td>\n      <td>10.0</td>\n      <td>245.25</td>\n      <td>[0.01828431, 0.087652445, -0.06963481, 0.00219...</td>\n    </tr>\n    <tr>\n      <th>7184</th>\n      <td>Primer on The Importance of Mindful Data Colle...</td>\n      <td>Is the information that you have fully anonymi...</td>\n      <td>870.0</td>\n      <td>144.0</td>\n      <td>8.0</td>\n      <td>217.50</td>\n      <td>[-0.0077653565, 0.13505033, -0.03715556, 0.018...</td>\n    </tr>\n  </tbody>\n</table>\n<p>7185 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = pd.DataFrame(chunks_dict)\n",
    "chunks"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:16:40.172336Z",
     "start_time": "2024-05-06T20:16:40.109092Z"
    }
   },
   "id": "f79874853b2ade9f",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chunks.to_csv(\"../data/chunks_embedded.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:16:52.691543Z",
     "start_time": "2024-05-06T20:16:40.173031Z"
    }
   },
   "id": "b35421e235108c3b",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.'"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks = [chunk[\"Chunk\"] for chunk in chunks_dict]\n",
    "text_chunks[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:16:52.695562Z",
     "start_time": "2024-05-06T20:16:52.693094Z"
    }
   },
   "id": "605663d25870fcda",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/225 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dba68fc44fa047a4a8b74b98b54f0988"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tensor([[ 0.0403,  0.0128, -0.0067,  ..., -0.0049, -0.0416, -0.0356],\n        [ 0.0575,  0.0059, -0.0185,  ..., -0.0489, -0.0511, -0.0437],\n        [-0.0080,  0.0187, -0.0103,  ..., -0.0336, -0.0822, -0.0343],\n        ...,\n        [ 0.0498,  0.0655, -0.0336,  ..., -0.0047, -0.0249, -0.0174],\n        [ 0.0183,  0.0877, -0.0696,  ...,  0.0583,  0.0100,  0.0260],\n        [-0.0078,  0.1351, -0.0372,  ...,  0.0125, -0.0042,  0.0256]],\n       device='mps:0')"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings = embeddings_model.encode(text_chunks,\n",
    "                                          batch_size=32,\n",
    "                                          show_progress_bar=True,\n",
    "                                          convert_to_tensor=True)\n",
    "text_embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:23:53.733989Z",
     "start_time": "2024-05-06T20:16:52.696293Z"
    }
   },
   "id": "bb442471f5c259c",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.save(text_embeddings, f\"../data/text_embeddings_{device}.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:23:53.805362Z",
     "start_time": "2024-05-06T20:23:53.740789Z"
    }
   },
   "id": "8df59afb868ff025",
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RAG pipeline \n",
    "(Checkpoint 1) - work above is saved in the data folder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34fec35887939725"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 4.0338e-02,  1.2804e-02, -6.7454e-03,  4.2980e-02, -4.3603e-02,\n         1.6660e-02, -3.8313e-02,  3.4725e-02, -4.9667e-03, -3.8526e-02,\n         4.6374e-02, -5.1384e-02,  4.2376e-02,  1.3069e-02,  4.0606e-02,\n        -1.1125e-02,  7.3396e-02,  3.7244e-02, -5.2825e-02, -3.8347e-03,\n         1.3996e-02,  1.2569e-02, -1.7039e-03,  4.6136e-02, -3.2836e-02,\n         1.1327e-02,  2.5366e-02, -2.0369e-02,  2.4939e-03, -1.9886e-03,\n         2.1593e-02, -2.6125e-03,  8.8577e-03,  7.1654e-03,  1.8332e-06,\n        -2.1929e-02, -2.9749e-02, -1.1231e-03,  4.3468e-02, -1.9777e-02,\n         6.6936e-02, -1.2584e-02,  4.0857e-03,  2.1768e-02, -3.9587e-02,\n        -3.9380e-02,  6.4695e-02,  8.3795e-02,  2.7839e-02,  2.4199e-02,\n        -2.0639e-02, -8.7842e-02,  7.2766e-05, -1.5067e-02, -8.0752e-03,\n         5.4452e-03,  4.4952e-02, -2.2253e-02, -5.6358e-03,  5.0621e-03,\n        -4.8909e-02,  1.9777e-02, -4.4482e-02,  5.6928e-03,  4.2703e-02,\n         7.4282e-02, -5.1439e-02, -3.6327e-02,  1.5568e-02,  4.5734e-02,\n        -6.9814e-02, -2.3512e-02, -8.2065e-03,  3.1195e-02, -4.2038e-02,\n         5.4484e-02, -2.1119e-02, -4.8982e-02,  1.2412e-02,  4.7215e-02,\n         1.9225e-02,  2.0540e-02, -1.6174e-02, -9.1987e-03,  3.9902e-03,\n         9.0083e-03,  1.1403e-02, -3.1551e-02,  2.1063e-02,  5.3906e-03,\n        -3.9024e-02, -5.7090e-03,  3.2892e-02,  5.5936e-03,  4.6279e-02,\n         3.0679e-02, -8.1164e-03, -4.0032e-03,  1.3855e-02, -1.1066e-01,\n         1.7350e-02,  5.8734e-03, -8.1978e-03,  2.3331e-02, -8.3980e-02,\n        -1.3682e-02,  6.9244e-03,  3.4408e-02, -4.8677e-02, -2.7210e-02,\n        -4.3091e-02, -6.8824e-03, -5.2639e-02,  6.9034e-03,  8.2994e-03,\n        -6.4403e-02, -4.6465e-02, -1.4128e-02, -1.5669e-02, -1.5759e-02,\n        -2.5019e-03, -3.7707e-03,  1.1415e-02,  3.6307e-02, -5.0931e-03,\n        -2.5787e-02, -7.4555e-02,  3.3685e-02, -5.3894e-02, -1.7967e-02,\n        -1.0666e-02,  1.0209e-02,  3.5566e-02,  1.9196e-03, -2.1287e-04,\n         1.7665e-02,  1.1624e-03, -4.4006e-02, -4.2662e-02,  2.1699e-02,\n         6.3293e-03, -1.9838e-02, -1.8612e-02, -6.3355e-02,  5.4747e-02,\n         1.2408e-03, -6.5764e-03,  9.8922e-03,  2.7518e-03,  1.4020e-02,\n        -1.0641e-01,  9.4920e-02,  7.1589e-04, -2.1373e-02,  6.3663e-02,\n         2.0496e-02,  6.3503e-02,  3.0006e-02, -2.9892e-03,  2.4640e-03,\n         3.2903e-02,  1.4926e-02, -1.6931e-02, -9.9224e-03, -1.6327e-03,\n        -2.6710e-02, -2.7259e-02, -9.4161e-03,  6.1306e-02,  5.8812e-02,\n         2.3428e-02,  6.4554e-02, -5.0515e-02, -1.3765e-02,  1.6717e-02,\n         1.1004e-01,  6.8854e-02,  2.5919e-02,  4.4721e-02,  5.6924e-03,\n         1.3686e-03,  6.5999e-03, -7.7021e-03, -5.2941e-03, -6.1323e-02,\n        -3.1052e-02,  2.9818e-03, -2.1445e-02, -3.5433e-02, -1.4948e-02,\n        -8.5475e-03, -1.8804e-02,  7.3164e-02, -8.2104e-02,  1.5273e-02,\n         1.9736e-02, -3.5481e-02,  4.1343e-02,  2.1724e-02, -6.4630e-03,\n        -3.2484e-02, -9.9847e-04,  2.0462e-02, -3.6495e-03,  4.7215e-02,\n         1.8474e-02,  3.3292e-02, -2.4360e-02, -3.3443e-02,  4.9137e-03,\n         2.0790e-02, -1.3813e-02, -2.7619e-02, -6.2033e-02,  5.0690e-03,\n         1.7652e-02, -1.1191e-02, -5.4068e-03,  2.4587e-04,  6.3875e-02,\n        -2.8096e-03,  2.0485e-02, -1.5413e-03,  5.6374e-02, -5.9243e-03,\n         1.7175e-02,  3.9663e-03,  2.3582e-03, -3.3076e-03,  2.3728e-02,\n         1.3045e-02,  8.6086e-03,  3.7945e-02, -4.6797e-02, -3.6649e-02,\n         1.0662e-02, -2.5261e-02, -2.2052e-02,  2.8243e-02, -5.6062e-02,\n         5.9129e-02,  4.2087e-02, -4.1109e-03, -1.2835e-02, -4.0066e-02,\n        -2.9585e-02,  6.9311e-02, -4.5173e-02,  2.9595e-02, -5.0514e-02,\n         5.5798e-02,  9.6746e-03,  4.6358e-02, -4.4633e-02,  2.4983e-02,\n        -6.2620e-02,  3.3865e-02,  2.4810e-03,  1.0979e-02, -2.5550e-02,\n         3.8334e-02,  1.5955e-02, -1.1015e-02, -6.4810e-03,  1.3049e-02,\n        -2.0583e-02, -1.7488e-02, -9.3122e-02, -1.1887e-02,  1.0688e-02,\n        -2.5635e-02, -3.6331e-02, -2.1370e-02,  9.0411e-03,  9.8020e-03,\n        -5.8344e-02,  9.4708e-02, -3.5288e-02,  6.7093e-03,  4.8990e-02,\n        -1.0660e-02, -4.9656e-02, -4.5616e-02, -3.7784e-02,  2.0120e-03,\n         6.2116e-02,  2.1148e-03, -4.7143e-02, -4.3755e-02,  1.8829e-02,\n        -2.5856e-02,  4.2640e-02, -1.2108e-03, -3.7812e-02, -3.2396e-02,\n        -1.0772e-03,  4.8470e-02,  2.9009e-02,  5.4998e-02,  5.4786e-02,\n         1.8730e-02,  4.2511e-02, -7.0360e-03, -2.7515e-02, -2.8157e-02,\n         5.6851e-02, -1.3369e-02,  4.0362e-03, -4.3507e-02,  6.4669e-02,\n         2.3271e-02, -5.8750e-03, -2.6182e-02,  3.1748e-02, -2.2170e-02,\n        -4.4358e-03, -2.1300e-02, -4.7316e-02, -2.1868e-02, -2.3009e-02,\n        -1.0518e-02, -2.7838e-03,  3.1461e-02, -2.1896e-02,  5.1027e-03,\n        -8.2296e-02, -4.1595e-02, -2.2327e-02,  1.5725e-02,  1.0463e-02,\n         2.4343e-02, -5.0474e-02, -9.3056e-03, -9.5722e-03,  1.8149e-02,\n         1.0542e-02, -4.4574e-02,  2.3499e-03,  1.9129e-02,  1.6178e-02,\n        -4.7368e-03, -7.4853e-03, -1.9592e-02, -6.1684e-02, -1.9216e-02,\n         2.6107e-02,  5.3410e-02, -2.9589e-04, -3.0168e-02,  3.5141e-02,\n        -4.0609e-02,  4.6494e-04, -6.5181e-03, -1.5895e-02, -8.9241e-03,\n         6.6092e-02,  5.9327e-02,  2.5177e-02, -3.0538e-03,  3.9438e-02,\n        -2.4695e-03,  3.5744e-02, -2.3450e-02,  1.6282e-02, -2.2283e-02,\n         2.0444e-02,  5.2639e-02, -4.8475e-02, -1.1420e-02, -2.5014e-02,\n         2.3278e-02,  2.0238e-02, -2.1236e-02,  6.0703e-02,  2.3050e-02,\n        -7.1616e-02,  2.1884e-02, -3.8773e-02, -4.5307e-02,  1.7414e-02,\n         2.9120e-02, -3.2075e-02, -8.0505e-03, -2.8858e-02, -1.8512e-02,\n         7.4910e-03, -3.1102e-02, -1.2657e-02, -9.3850e-02,  4.0955e-03,\n         1.8655e-02, -4.4176e-02, -2.0912e-02, -5.6691e-02, -2.4886e-02,\n        -2.6517e-03, -2.8342e-02, -2.1722e-02, -1.1166e-02, -4.3291e-02,\n        -4.2569e-04, -3.2671e-02, -1.1506e-02,  4.4697e-02, -6.3844e-02,\n         3.8272e-02,  6.5488e-02,  7.0109e-02, -1.7519e-02, -3.2642e-02,\n         2.5020e-02, -5.7008e-02, -1.2018e-02, -4.4883e-02,  3.5135e-02,\n        -5.0799e-02,  2.7771e-02, -9.5076e-03,  2.9873e-02,  3.9807e-02,\n        -5.0510e-03, -3.4743e-02,  5.5519e-02,  1.5044e-02, -5.9464e-02,\n        -5.9287e-02,  3.5313e-02,  1.3216e-02,  4.9230e-02,  3.0191e-02,\n        -4.9923e-02, -3.7641e-03,  2.5596e-02,  3.1593e-02,  8.4745e-03,\n        -3.3374e-02, -4.2287e-03, -4.6147e-04, -1.8654e-02, -5.2096e-02,\n        -3.8613e-02,  2.7336e-02,  3.7965e-02,  7.4905e-03,  6.5040e-02,\n         1.8984e-03, -1.7864e-02, -8.2078e-02, -3.8695e-02,  5.1317e-02,\n         6.3565e-02,  2.5538e-02, -2.0416e-02,  4.0569e-02,  1.2866e-02,\n        -4.3217e-02, -2.1023e-02, -5.3889e-02, -3.9480e-02, -3.4362e-03,\n         2.4566e-02,  1.2069e-02,  6.7633e-02, -3.7755e-02, -8.5897e-02,\n         5.1692e-02,  5.9214e-03, -3.9576e-02, -4.4924e-03, -1.3299e-02,\n         3.2938e-02, -6.9333e-04,  2.9899e-02, -6.0269e-03,  4.1086e-02,\n        -2.9213e-02,  3.0973e-02, -1.5868e-02, -2.0589e-02, -1.0789e-02,\n        -5.4202e-02,  6.6151e-02, -1.7258e-02,  1.7344e-02, -5.5295e-02,\n        -1.7145e-02, -1.0538e-02,  1.1231e-02, -7.6914e-03,  9.5006e-03,\n         3.6312e-02, -1.7368e-02, -3.4962e-02, -4.0799e-02, -7.4599e-03,\n         4.8584e-03,  2.4638e-02,  6.7507e-02,  4.1681e-02,  5.9626e-02,\n        -2.4109e-02,  3.9863e-02, -2.9409e-02, -3.6364e-02, -1.6249e-02,\n         1.8472e-02, -5.7591e-02,  3.4990e-02,  1.7438e-02,  1.1545e-02,\n        -5.1641e-02,  5.9184e-02, -1.7433e-02,  2.8969e-02, -2.5939e-02,\n         3.6833e-02,  2.2220e-02, -7.3983e-02,  3.6730e-03, -4.9616e-03,\n        -2.8638e-02,  7.0342e-03,  5.8020e-03, -5.7426e-02,  1.3324e-02,\n         4.3954e-02,  6.3868e-03, -1.6699e-02, -9.8630e-03,  2.1701e-02,\n        -8.9306e-03, -2.3797e-02, -2.4204e-02, -2.1439e-02, -2.4742e-02,\n         5.7075e-02,  7.0567e-02,  6.7286e-02, -1.1287e-02,  2.9831e-02,\n        -6.4173e-02, -1.7028e-02,  4.7620e-02, -1.4078e-03, -1.0523e-02,\n         4.4901e-02,  3.0617e-02,  4.6855e-02,  5.3896e-02,  2.7912e-02,\n        -4.2356e-02,  1.7155e-02, -4.3507e-02, -6.2437e-02, -9.3900e-04,\n        -6.0955e-33, -1.5531e-02, -4.8107e-02, -1.6007e-02,  7.0986e-03,\n        -5.3599e-02,  1.9782e-02, -3.6739e-03, -2.1252e-02, -3.5942e-02,\n         2.6183e-02, -1.1784e-02,  1.6808e-02,  7.3935e-03, -4.6172e-02,\n        -1.5122e-02,  9.2817e-03,  5.5605e-03,  1.4320e-02,  5.1121e-03,\n        -1.6208e-02,  1.5799e-02,  8.4198e-02,  2.7278e-02, -5.8789e-02,\n         3.5549e-02, -1.7686e-02,  5.3469e-02,  6.5103e-04,  4.9715e-02,\n         4.4775e-02, -5.2611e-02,  2.0857e-02,  3.1678e-02,  7.8672e-03,\n         8.9479e-03,  6.9453e-02, -9.4216e-02, -6.4373e-02, -2.7900e-03,\n         6.2007e-03, -9.0077e-02,  1.1519e-02,  4.2121e-02,  2.2113e-02,\n        -3.7287e-02,  2.3732e-02,  5.9268e-02, -2.2784e-02, -7.4168e-02,\n         1.5361e-02, -5.4825e-02,  1.2803e-02,  7.7091e-03,  2.9509e-02,\n         4.4752e-02, -1.4630e-02,  4.3444e-02, -1.1163e-02, -6.8349e-02,\n        -6.2489e-04, -6.0438e-02,  1.7778e-02,  5.5640e-03,  3.1206e-03,\n         3.5786e-03,  7.6922e-02,  5.1393e-02, -7.0269e-02, -7.2741e-03,\n         2.6484e-02,  2.8018e-02,  3.2627e-02,  1.9111e-02,  2.2364e-02,\n         7.4360e-02, -7.8762e-02, -1.9037e-02,  3.8081e-02,  1.2337e-02,\n         2.3304e-02,  1.2089e-03, -1.9533e-02, -5.7168e-02, -2.0120e-02,\n        -6.0326e-04, -5.6932e-02, -4.0208e-02, -4.2724e-02,  2.0878e-02,\n         1.8570e-02, -2.6654e-02,  2.2706e-02, -4.7357e-03, -3.5769e-02,\n         1.1706e-03,  2.8404e-02,  5.5882e-03, -8.2944e-03, -4.1969e-03,\n         2.5875e-02, -1.0121e-02, -6.1092e-02, -4.2737e-02,  1.1433e-03,\n         2.2780e-02, -8.6300e-03,  7.6811e-03,  2.2248e-03, -3.9198e-02,\n        -9.6497e-03,  4.4523e-04, -7.1237e-02, -3.6763e-02,  3.0291e-03,\n         8.8223e-04,  1.3320e-02,  2.5637e-03,  3.5796e-02,  4.5423e-02,\n         6.1223e-03, -2.0381e-03, -6.5276e-02,  2.8891e-02,  3.4197e-02,\n        -3.6006e-02, -5.6297e-03, -3.0483e-02,  5.2160e-02,  4.4743e-02,\n        -8.2420e-02,  1.8474e-03,  3.6759e-02,  2.5770e-07,  4.3233e-02,\n         5.2532e-02,  1.0703e-02, -4.6429e-02, -2.0519e-03, -1.6382e-02,\n         1.1641e-02,  4.6093e-02, -5.2596e-02,  1.5429e-02,  6.2698e-03,\n         2.6984e-02,  1.6455e-02,  1.1468e-02, -5.3987e-02,  3.3069e-02,\n        -6.5980e-02,  2.7588e-02, -4.0315e-02,  1.3747e-02,  7.9731e-02,\n         1.0426e-01,  5.3706e-02, -4.6293e-03, -4.1634e-02, -2.3175e-02,\n         1.9650e-02, -1.1191e-02,  2.7770e-02,  5.6879e-05, -9.5837e-03,\n         2.1413e-02,  1.5023e-02, -6.5604e-03, -2.3111e-02,  8.4525e-02,\n         6.2702e-02,  7.1242e-03, -4.5455e-02,  2.8458e-02,  1.4833e-02,\n        -1.3782e-02, -2.2681e-02, -1.6218e-02,  7.6284e-02, -1.8914e-02,\n        -3.2173e-02, -4.2537e-02,  4.3697e-02,  3.7832e-02,  6.2611e-02,\n         6.6882e-03, -4.0621e-02, -8.5162e-03,  3.5008e-02, -1.7046e-02,\n         2.2553e-03, -8.1312e-02,  2.1368e-02,  1.9106e-02,  1.2892e-03,\n         4.5419e-04,  6.9134e-04,  8.9144e-03,  1.0071e-01,  3.0770e-02,\n        -1.0060e-02,  1.9801e-34,  1.2513e-02,  5.7043e-03,  6.4225e-03,\n         3.8398e-02,  2.3528e-03, -2.6968e-02,  2.7612e-02,  2.5686e-02,\n        -4.9020e-03, -4.1622e-02, -3.5611e-02], device='mps:0')"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "text_chunks_df = pd.read_csv(\"../data/chunks_embedded.csv\")\n",
    "text_chunks_dict = text_chunks_df.to_dict(\"records\")\n",
    "\n",
    "text_embeddings = torch.load(f\"../data/text_embeddings_{device}.pt\")\n",
    "text_embeddings[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:23:55.396393Z",
     "start_time": "2024-05-06T20:23:53.808325Z"
    }
   },
   "id": "ccd3b8add3a7af14",
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Query embedding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4c55077e9568d22"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n  (2): Normalize()\n)"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\", device=device)\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:23:59.765731Z",
     "start_time": "2024-05-06T20:23:55.397621Z"
    }
   },
   "id": "6ae2eca028b177da",
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Retrieval indexing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccde52af445ad454"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.return_types.topk(\nvalues=tensor([[0.7937, 0.7876, 0.7609, 0.7162, 0.6967]], device='mps:0'),\nindices=tensor([[1032,  393, 1982, 6746, 3390]], device='mps:0'))"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query \n",
    "query = \"What is machine learning?\"\n",
    "\n",
    "# embedding\n",
    "query_embedding = model.encode(query, convert_to_tensor=True, device=device)\n",
    "\n",
    "# similarity - dot because of a Normalize layer in the model\n",
    "dot_products = util.dot_score(query_embedding, text_embeddings)\n",
    "\n",
    "# best results\n",
    "torch.topk(dot_products, k=5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:24:04.146833Z",
     "start_time": "2024-05-06T20:23:59.767834Z"
    }
   },
   "id": "6f1f4cafd03a8e3b",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                        Title  \\\n1032  On the Journey to Machine Learning / AI   \n393     Microsoft Introduction to AI — Part 1   \n1982             So what is Machine Learning?   \n6746                    Why Machine Learning?   \n3390               Machine Learning in Energy   \n\n                                                  Chunk  Chunk_length  \\\n1032  What is Machine Learning? There are millions o...         953.0   \n393   Learning from this data it can understand our ...         792.0   \n1982  Photo by fabio on Unsplash I am sure by now yo...        1194.0   \n6746  Image by the author In my previous post I talk...        1104.0   \n3390  What is machine learning? The business plans o...        1206.0   \n\n      Chunk_words_num  Chunk_sentences_num  Token_count  \\\n1032            159.0                 10.0       238.25   \n393             131.0                 10.0       198.00   \n1982            215.0                 10.0       298.50   \n6746            181.0                 10.0       276.00   \n3390            200.0                 10.0       301.50   \n\n                                              Embedding  \n1032  [ 2.16269530e-02 -2.55265869e-02 -6.68139756e-...  \n393   [ 2.41031330e-02 -5.80267282e-03 -4.96849753e-...  \n1982  [ 3.63377593e-02  2.96800584e-02 -4.95566875e-...  \n6746  [ 3.04092318e-02  5.43406904e-02 -4.28669490e-...  \n3390  [ 6.98239403e-03  6.78827018e-02 -6.71653450e-...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Chunk</th>\n      <th>Chunk_length</th>\n      <th>Chunk_words_num</th>\n      <th>Chunk_sentences_num</th>\n      <th>Token_count</th>\n      <th>Embedding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1032</th>\n      <td>On the Journey to Machine Learning / AI</td>\n      <td>What is Machine Learning? There are millions o...</td>\n      <td>953.0</td>\n      <td>159.0</td>\n      <td>10.0</td>\n      <td>238.25</td>\n      <td>[ 2.16269530e-02 -2.55265869e-02 -6.68139756e-...</td>\n    </tr>\n    <tr>\n      <th>393</th>\n      <td>Microsoft Introduction to AI — Part 1</td>\n      <td>Learning from this data it can understand our ...</td>\n      <td>792.0</td>\n      <td>131.0</td>\n      <td>10.0</td>\n      <td>198.00</td>\n      <td>[ 2.41031330e-02 -5.80267282e-03 -4.96849753e-...</td>\n    </tr>\n    <tr>\n      <th>1982</th>\n      <td>So what is Machine Learning?</td>\n      <td>Photo by fabio on Unsplash I am sure by now yo...</td>\n      <td>1194.0</td>\n      <td>215.0</td>\n      <td>10.0</td>\n      <td>298.50</td>\n      <td>[ 3.63377593e-02  2.96800584e-02 -4.95566875e-...</td>\n    </tr>\n    <tr>\n      <th>6746</th>\n      <td>Why Machine Learning?</td>\n      <td>Image by the author In my previous post I talk...</td>\n      <td>1104.0</td>\n      <td>181.0</td>\n      <td>10.0</td>\n      <td>276.00</td>\n      <td>[ 3.04092318e-02  5.43406904e-02 -4.28669490e-...</td>\n    </tr>\n    <tr>\n      <th>3390</th>\n      <td>Machine Learning in Energy</td>\n      <td>What is machine learning? The business plans o...</td>\n      <td>1206.0</td>\n      <td>200.0</td>\n      <td>10.0</td>\n      <td>301.50</td>\n      <td>[ 6.98239403e-03  6.78827018e-02 -6.71653450e-...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks_df.iloc[torch.topk(dot_products, k=5).indices.cpu().numpy().ravel()]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:24:04.176668Z",
     "start_time": "2024-05-06T20:24:04.160948Z"
    }
   },
   "id": "762b1e5df6960cf9",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([7185, 768])"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:24:04.180168Z",
     "start_time": "2024-05-06T20:24:04.177621Z"
    }
   },
   "id": "1954f10d380749a8",
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Helper functions for retrieval and printing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24388e1261b968be"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is a convolutional neural net?\n",
      "=======\n",
      "Article title: A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model || Score: 2903\n",
      "\n",
      "1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings\n",
      "using a two-layer neural network. Its input is a text corpus and its output is a set of vectors.\n",
      "Word embedding via word2vec can make natural language computer-readable, then further implementation\n",
      "of mathematical operations on words can be used to detect their similarities. A well-trained set of\n",
      "word vectors will place similar words close to each other in that space. For instance, the words\n",
      "women, men, and human might cluster in one corner, while yellow, red and blue cluster together in\n",
      "another. There are two main training algorithms for word2vec, one is the continuous bag of\n",
      "words(CBOW), another is called skip-gram. The major difference between these two methods is that\n",
      "CBOW is using context to predict a target word while skip-gram is using a word to predict a target\n",
      "context. Generally, the skip-gram method can have a better performance compared with CBOW method,\n",
      "for it can capture two semantics for a single word. For instance, it will have two vector\n",
      "representations for Apple, one for the company and another for the fruit.\n",
      "\n",
      "\n",
      "=======\n",
      "Article title: A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model || Score: 6250\n",
      "\n",
      "1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings\n",
      "using a two-layer neural network. Its input is a text corpus and its output is a set of vectors.\n",
      "Word embedding via word2vec can make natural language computer-readable, then further implementation\n",
      "of mathematical operations on words can be used to detect their similarities. A well-trained set of\n",
      "word vectors will place similar words close to each other in that space. For instance, the words\n",
      "women, men, and human might cluster in one corner, while yellow, red and blue cluster together in\n",
      "another. There are two main training algorithms for word2vec, one is the continuous bag of\n",
      "words(CBOW), another is called skip-gram. The major difference between these two methods is that\n",
      "CBOW is using context to predict a target word while skip-gram is using a word to predict a target\n",
      "context. Generally, the skip-gram method can have a better performance compared with CBOW method,\n",
      "for it can capture two semantics for a single word. For instance, it will have two vector\n",
      "representations for Apple, one for the company and another for the fruit.\n",
      "\n",
      "\n",
      "=======\n",
      "Article title: A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model || Score: 7111\n",
      "\n",
      "1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings\n",
      "using a two-layer neural network. Its input is a text corpus and its output is a set of vectors.\n",
      "Word embedding via word2vec can make natural language computer-readable, then further implementation\n",
      "of mathematical operations on words can be used to detect their similarities. A well-trained set of\n",
      "word vectors will place similar words close to each other in that space. For instance, the words\n",
      "women, men, and human might cluster in one corner, while yellow, red and blue cluster together in\n",
      "another. There are two main training algorithms for word2vec, one is the continuous bag of\n",
      "words(CBOW), another is called skip-gram. The major difference between these two methods is that\n",
      "CBOW is using context to predict a target word while skip-gram is using a word to predict a target\n",
      "context. Generally, the skip-gram method can have a better performance compared with CBOW method,\n",
      "for it can capture two semantics for a single word. For instance, it will have two vector\n",
      "representations for Apple, one for the company and another for the fruit.\n",
      "\n",
      "\n",
      "=======\n",
      "Article title: A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model || Score: 2904\n",
      "\n",
      "1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings\n",
      "using a two-layer neural network. Its input is a text corpus and its output is a set of vectors.\n",
      "Word embedding via word2vec can make natural language computer-readable, then further implementation\n",
      "of mathematical operations on words can be used to detect their similarities. A well-trained set of\n",
      "word vectors will place similar words close to each other in that space. For instance, the words\n",
      "women, men, and human might cluster in one corner, while yellow, red and blue cluster together in\n",
      "another. There are two main training algorithms for word2vec, one is the continuous bag of\n",
      "words(CBOW), another is called skip-gram. The major difference between these two methods is that\n",
      "CBOW is using context to predict a target word while skip-gram is using a word to predict a target\n",
      "context. Generally, the skip-gram method can have a better performance compared with CBOW method,\n",
      "for it can capture two semantics for a single word. For instance, it will have two vector\n",
      "representations for Apple, one for the company and another for the fruit.\n",
      "\n",
      "\n",
      "=======\n",
      "Article title: A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model || Score: 691\n",
      "\n",
      "1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings\n",
      "using a two-layer neural network. Its input is a text corpus and its output is a set of vectors.\n",
      "Word embedding via word2vec can make natural language computer-readable, then further implementation\n",
      "of mathematical operations on words can be used to detect their similarities. A well-trained set of\n",
      "word vectors will place similar words close to each other in that space. For instance, the words\n",
      "women, men, and human might cluster in one corner, while yellow, red and blue cluster together in\n",
      "another. There are two main training algorithms for word2vec, one is the continuous bag of\n",
      "words(CBOW), another is called skip-gram. The major difference between these two methods is that\n",
      "CBOW is using context to predict a target word while skip-gram is using a word to predict a target\n",
      "context. Generally, the skip-gram method can have a better performance compared with CBOW method,\n",
      "for it can capture two semantics for a single word. For instance, it will have two vector\n",
      "representations for Apple, one for the company and another for the fruit.\n",
      "\n",
      "\n",
      "=======\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "\n",
    "def retrieve_simillar_embeddings(query, embeddings, model=model, device=device, n=5):\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True, device=device)\n",
    "    dot_products = util.dot_score(query_embedding, embeddings)\n",
    "    top_results = torch.topk(dot_products, k=n)\n",
    "    \n",
    "    indices, scores = top_results\n",
    "    \n",
    "    return indices, scores\n",
    "\n",
    "\n",
    "def search_text(query, embeddings, model=model, device=device, n=5):\n",
    "    indices, scores = retrieve_simillar_embeddings(query, embeddings, model, device, n)\n",
    "    indices = indices.cpu().numpy().ravel()\n",
    "    scores = scores.cpu().numpy().ravel()\n",
    "    \n",
    "    results = text_chunks_df.iloc[indices]\n",
    "    chunks = results[\"Chunk\"].values\n",
    "    titles = results[\"Title\"].values\n",
    "    \n",
    "    return chunks, titles, scores\n",
    "    \n",
    "    \n",
    "def print_text(chunks, titles, scores, query, width=100):\n",
    "    print(\"Query:\", query)\n",
    "    print(\"=======\")\n",
    "    wrapper = textwrap.TextWrapper(width=width)\n",
    "\n",
    "    for chunk, title, score in zip(chunks, titles, scores):\n",
    "        print(f\"Article title: {title} || Score: {score}\\n\")\n",
    "        word_list = wrapper.wrap(text=chunk)\n",
    "        for element in word_list:\n",
    "            print(element)\n",
    "        print(\"\\n\")\n",
    "        print(\"=======\")\n",
    "        \n",
    "query = \"What is a convolutional neural net?\"\n",
    "\n",
    "chunks, titles, scores = search_text(query, text_embeddings)\n",
    "\n",
    "print_text(chunks, titles, scores, query)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:24:04.280337Z",
     "start_time": "2024-05-06T20:24:04.180950Z"
    }
   },
   "id": "75b99730ad03b1c3",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0.7434, 0.6638, 0.6285, 0.6077, 0.6074]], device='mps:0'),\n tensor([[2903, 6250, 7111, 2904,  691]], device='mps:0'))"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_simillar_embeddings(\"What is a convolutional neural net?\", text_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:24:04.315041Z",
     "start_time": "2024-05-06T20:24:04.281698Z"
    }
   },
   "id": "4290a1eaf0428353",
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Local LLM generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15f036e540957a45"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# model https://huggingface.co/google/gemma-2b-it\n",
    "# huggingface-cli https://huggingface.co/docs/huggingface_hub/main/en/guides/cli"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:24:04.317332Z",
     "start_time": "2024-05-06T20:24:04.315881Z"
    }
   },
   "id": "b56442b116af7616",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "330c75de6fef40d3a7e0f94fea18547a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "GemmaForCausalLM(\n  (model): GemmaModel(\n    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n    (layers): ModuleList(\n      (0-17): 18 x GemmaDecoderLayer(\n        (self_attn): GemmaSdpaAttention(\n          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): GemmaRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n    )\n    (norm): GemmaRMSNorm()\n  )\n  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n)"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# using 7b is possible if 24gb of VRAM on a GPU is available\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\",\n",
    "                                           attn_implementation=\"sdpa\",\n",
    "                                           torch_dtype=torch.float16,\n",
    "                                           low_cpu_mem_usage=False)\n",
    "\n",
    "llm.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:24:26.364264Z",
     "start_time": "2024-05-06T20:24:04.318204Z"
    }
   },
   "id": "93fe66bd957e8a5e",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used memory (MPS GPU): 11.95 GB\n",
      "model size: 2.51B\n",
      "Model memory: 5.55 GB\n",
      "Model parameters: 5549215744 bytes\n"
     ]
    }
   ],
   "source": [
    "def print_model_data(model, device):\n",
    "    if device == torch.device(\"mps\"):\n",
    "        print(f\"Used memory (MPS GPU): {(torch.mps.current_allocated_memory() / 1024 ** 3):.2f} GB\", )\n",
    "    print(f\"model size: {(sum([p.numel() for p in model.parameters()]) / 1e9):.2f}B\")\n",
    "    \n",
    "    mem_params = sum([p.nelement() * p.element_size() for p in model.parameters()])\n",
    "    mem_buffers = sum([b.nelement() * b.element_size() for b in model.buffers()])\n",
    "    \n",
    "    print(f\"Model memory: {(mem_params + mem_buffers) / 1e9:.2f} GB\")\n",
    "    print(f\"Model parameters: {mem_params + mem_buffers} bytes\")\n",
    "\n",
    "\n",
    "print_model_data(llm, device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:24:26.380716Z",
     "start_time": "2024-05-06T20:24:26.368765Z"
    }
   },
   "id": "a46ab5207c2dee2b",
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccb42699499bf0d5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a convolutional neural net?\n"
     ]
    },
    {
     "data": {
      "text/plain": "'<bos><start_of_turn>user\\nWhat is a convolutional neural net?<end_of_turn>\\n<start_of_turn>model\\n'"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"What is a convolutional neural net?\"\n",
    "print(input_text)\n",
    "\n",
    "llm_prompt_template = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": input_text,\n",
    "}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(llm_prompt_template, tokenize=False, add_generation_prompt=True)\n",
    "prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:24:26.400625Z",
     "start_time": "2024-05-06T20:24:26.381574Z"
    }
   },
   "id": "da71c2eccd74d6b2",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "What is a convolutional neural net?\n",
      "model\n",
      "A convolutional neural network (CNN) is a type of artificial neural network (ANN) used for image recognition and classification. It is a specialized type of neural network that is designed to process and analyze visual information.\n",
      "\n",
      "**Key features of CNNs:**\n",
      "\n",
      "* **Layers:** CNNs consist of multiple layers, each containing a set of interconnected neurons.\n",
      "* **Convolutional layers:** These layers perform a convolution operation on the input image, extracting features and identifying patterns.\n",
      "* **Pooling layers:** After the convolution layer, the feature maps are reduced in size, reducing computation and allowing for efficient processing.\n",
      "* **Max-pooling:** This operation takes the maximum value from each feature map cell in a given region.\n",
      "* **Activation function:** After the convolution and pooling operations, an activation function is applied to each neuron, introducing non-linearity into the model.\n",
      "* **Fully connected layers:** These layers connect the output of the convolutional and pooling layers to the output layer, which makes the final classification decision.\n",
      "\n",
      "**How CNNs work:**\n",
      "\n",
      "1. **Input:** The input image is fed into the network.\n",
      "2. **Convolution:** The input image is convolved with a set of filters, extracting features from the image.\n",
      "3. **Pooling\n"
     ]
    }
   ],
   "source": [
    "tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "output = llm.generate(**tokenized_prompt, max_new_tokens=256)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:25:03.038353Z",
     "start_time": "2024-05-06T20:24:26.401862Z"
    }
   },
   "id": "b49af16688a41acb",
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output from the model looks fine, let's prompt engineer it"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b99e6fa5356c37b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is the difference between supervised and unsupervised learning in machine learning?\",\n",
    "    \"Can you explain the bias-variance tradeoff in machine learning?\",\n",
    "    \"What are some common activation functions used in neural networks?\",\n",
    "    \"What is the purpose of regularization in machine learning models?\",\n",
    "    \"How does gradient descent optimize the parameters of a machine learning model?\",\n",
    "    \"What is cross-validation and why is it used in machine learning?\",\n",
    "    \"Explain the concept of feature engineering in machine learning.\",\n",
    "    \"What is the role of hyperparameters in machine learning algorithms?\"\n",
    "]\n",
    "\n",
    "\n",
    "def create_prompt(query, context_chunks):\n",
    "    query_start = \"Answer the question: \" + query\n",
    "    answer_requirements = \"\"\"\n",
    "Give yourself room to think by extracting relevant passages from the context before answering.\n",
    "Return just the answer to the question.\n",
    "Make sure the answer is as explanatory as possible.\n",
    "Use the following reference questions and answers as a style guideline but answer only to the question above:\n",
    "1. What is overfitting in machine learning?\n",
    "   - Overfitting occurs when a model learns to memorize the training data instead of capturing the underlying patterns, leading to poor generalization on unseen data.\n",
    "\n",
    "2. What is the purpose of a validation set in machine learning?\n",
    "   - The validation set is used to evaluate the performance of a model during training and to tune hyperparameters to prevent overfitting.\n",
    "\n",
    "3. What is the difference between precision and recall in binary classification?\n",
    "   - Precision measures the proportion of true positives among all predicted positives, while recall measures the proportion of true positives among all actual positives.\n",
    "\n",
    "4. What is the softmax function used for in neural networks?\n",
    "   - The softmax function is used to convert the raw output of a neural network into probabilities, enabling it to make multi-class predictions.\n",
    "\n",
    "5. What is transfer learning in deep learning?\n",
    "   - Transfer learning involves using pre-trained neural network models as a starting point for training on a new task, often resulting in faster convergence and better performance with less data.\n",
    "\n",
    "6. What is batch normalization in neural networks?\n",
    "   - Batch normalization is a technique used to normalize the inputs of each layer in a neural network, stabilizing training and accelerating convergence.\n",
    "\n",
    "7. What is the purpose of the Adam optimizer in deep learning?\n",
    "   - The Adam optimizer is an adaptive learning rate optimization algorithm that combines the advantages of both AdaGrad and RMSProp, making it widely used in training deep neural networks.\n",
    "\n",
    "8. What is the curse of dimensionality in machine learning?\n",
    "   - The curse of dimensionality refers to the increased difficulty of learning and generalizing from data in high-dimensional spaces, leading to sparsity and increased computational complexity.\n",
    "    \"\"\"\n",
    "\n",
    "    query_end = \"Based on the following context.\"\n",
    "    context = \"- \" + \"\\n- \".join([chunk for chunk in context_chunks])\n",
    "\n",
    "    prompt = \"\\n\".join([query_start, answer_requirements, context, query_end])\n",
    "\n",
    "    llm_prompt_template = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": prompt,\n",
    "    }]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(llm_prompt_template,\n",
    "                                           tokenize=False,\n",
    "                                           add_generation_prompt=True)\n",
    "\n",
    "    return prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:25:03.049476Z",
     "start_time": "2024-05-06T20:25:03.041265Z"
    }
   },
   "id": "a8910b4c0cbe1d30",
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Test the prompt generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11b11e72398568db"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Answer the question: What is cross-validation and why is it used in machine learning?\n",
      "\n",
      "Give yourself room to think by extracting relevant passages from the context before answering.\n",
      "Return just the answer to the question.\n",
      "Make sure the answer is as explanatory as possible.\n",
      "Use the following reference questions and answers as a style guideline but answer only to the question above:\n",
      "1. What is overfitting in machine learning?\n",
      "   - Overfitting occurs when a model learns to memorize the training data instead of capturing the underlying patterns, leading to poor generalization on unseen data.\n",
      "\n",
      "2. What is the purpose of a validation set in machine learning?\n",
      "   - The validation set is used to evaluate the performance of a model during training and to tune hyperparameters to prevent overfitting.\n",
      "\n",
      "3. What is the difference between precision and recall in binary classification?\n",
      "   - Precision measures the proportion of true positives among all predicted positives, while recall measures the proportion of true positives among all actual positives.\n",
      "\n",
      "4. What is the softmax function used for in neural networks?\n",
      "   - The softmax function is used to convert the raw output of a neural network into probabilities, enabling it to make multi-class predictions.\n",
      "\n",
      "5. What is transfer learning in deep learning?\n",
      "   - Transfer learning involves using pre-trained neural network models as a starting point for training on a new task, often resulting in faster convergence and better performance with less data.\n",
      "\n",
      "6. What is batch normalization in neural networks?\n",
      "   - Batch normalization is a technique used to normalize the inputs of each layer in a neural network, stabilizing training and accelerating convergence.\n",
      "\n",
      "7. What is the purpose of the Adam optimizer in deep learning?\n",
      "   - The Adam optimizer is an adaptive learning rate optimization algorithm that combines the advantages of both AdaGrad and RMSProp, making it widely used in training deep neural networks.\n",
      "\n",
      "8. What is the curse of dimensionality in machine learning?\n",
      "   - The curse of dimensionality refers to the increased difficulty of learning and generalizing from data in high-dimensional spaces, leading to sparsity and increased computational complexity.\n",
      "    \n",
      "- 1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.\n",
      "- 1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.\n",
      "- 1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.\n",
      "- 1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.\n",
      "- 1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.\n",
      "Based on the following context.<end_of_turn>\n",
      "<start_of_turn>model\n"
     ]
    }
   ],
   "source": [
    "query_idx = np.random.randint(0, len(questions))\n",
    "query = questions[query_idx]\n",
    "\n",
    "chunks, titles, scores = search_text(query, text_embeddings)\n",
    "prompt = create_prompt(query, chunks)\n",
    "\n",
    "print(prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:25:03.342253Z",
     "start_time": "2024-05-06T20:25:03.050750Z"
    }
   },
   "id": "aa62017c1f887974",
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Answer the question: What is cross-validation and why is it used in machine learning?\n",
      "\n",
      "Give yourself room to think by extracting relevant passages from the context before answering.\n",
      "Return just the answer to the question.\n",
      "Make sure the answer is as explanatory as possible.\n",
      "Use the following reference questions and answers as a style guideline but answer only to the question above:\n",
      "1. What is overfitting in machine learning?\n",
      "   - Overfitting occurs when a model learns to memorize the training data instead of capturing the underlying patterns, leading to poor generalization on unseen data.\n",
      "\n",
      "2. What is the purpose of a validation set in machine learning?\n",
      "   - The validation set is used to evaluate the performance of a model during training and to tune hyperparameters to prevent overfitting.\n",
      "\n",
      "3. What is the difference between precision and recall in binary classification?\n",
      "   - Precision measures the proportion of true positives among all predicted positives, while recall measures the proportion of true positives among all actual positives.\n",
      "\n",
      "4. What is the softmax function used for in neural networks?\n",
      "   - The softmax function is used to convert the raw output of a neural network into probabilities, enabling it to make multi-class predictions.\n",
      "\n",
      "5. What is transfer learning in deep learning?\n",
      "   - Transfer learning involves using pre-trained neural network models as a starting point for training on a new task, often resulting in faster convergence and better performance with less data.\n",
      "\n",
      "6. What is batch normalization in neural networks?\n",
      "   - Batch normalization is a technique used to normalize the inputs of each layer in a neural network, stabilizing training and accelerating convergence.\n",
      "\n",
      "7. What is the purpose of the Adam optimizer in deep learning?\n",
      "   - The Adam optimizer is an adaptive learning rate optimization algorithm that combines the advantages of both AdaGrad and RMSProp, making it widely used in training deep neural networks.\n",
      "\n",
      "8. What is the curse of dimensionality in machine learning?\n",
      "   - The curse of dimensionality refers to the increased difficulty of learning and generalizing from data in high-dimensional spaces, leading to sparsity and increased computational complexity.\n",
      "    \n",
      "- 1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.\n",
      "- 1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.\n",
      "- 1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.\n",
      "- 1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.\n",
      "- 1. Introduction of Word2vec Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.\n",
      "Based on the following context.\n",
      "model\n",
      "Sure, here's the answer to the question:\n",
      "\n",
      "Cross-validation is a technique used in machine learning to evaluate the performance of a model during training and to prevent overfitting. It involves dividing the data into multiple folds, training the model on each fold while testing it on the remaining folds. This process helps to ensure that the model is not memorizing the training data and is able to generalize well on unseen data.\n",
      "\n",
      "Cross-validation is often used with k-fold cross-validation, where k is the number of folds in the data set. In this approach, the data is divided into k folds, and the model is trained on k-1 folds while testing it on the remaining one fold. This process is repeated k times, with each fold being tested once and the other folds being held out as a validation set.\n",
      "\n",
      "Cross-validation is a powerful technique that can help to improve the performance of machine learning models. By training a model on a subset of the data and testing it on a different subset, cross-validation helps to prevent overfitting and ensure that the model is able to generalize well on unseen data.\n"
     ]
    }
   ],
   "source": [
    "llm_prompt_template = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": prompt,\n",
    "}]\n",
    "\n",
    "tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "output = llm.generate(**tokenized_prompt, max_new_tokens=256, temperature=0.7, do_sample=True)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T20:25:45.493393Z",
     "start_time": "2024-05-06T20:25:03.342883Z"
    }
   },
   "id": "c219a190a8e22692",
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "39f6d056c668e2f6",
   "execution_count": 53
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
